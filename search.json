[
  {
    "objectID": "posts/2024-01-04-YaoMillionaireProblemPart1/MillionaireGame.html",
    "href": "posts/2024-01-04-YaoMillionaireProblemPart1/MillionaireGame.html",
    "title": "On Yao’s Millionaire Problem. Part 1.",
    "section": "",
    "text": "[Originally written February 2022 … –JHM ]\nThe purpose of this article is to investigate whether there is strategy or skill possible in the following variation of Yao’s “Millionaire Problem”.\nHere is the game. We have a huge grid \\(\\mathbf{R}^2\\). Now let two players \\(A,B\\) have secret locations \\(s_A=(x_A, y_A)\\) and \\(s_B=(x_B, y_B)\\). These secrets are points in the euclidean plane \\(\\mathbf{R}^2\\).\nNow the players \\(A, B\\) are going to take turns guessing affine functions (or affine lines in \\(\\mathbf{R}^2\\)) and the first player to guess an affine function which separates the secrets wins!\nThe gameplay is something like this: The players \\(A,B\\) take turns. If player \\(A\\) goes first, then player \\(A\\) chooses an affine function \\(\\ell\\) on \\(\\mathbf{R}^2\\), and asks player \\(B\\) to reply with the sign of \\(\\ell(s_B)\\). We require that \\(B\\) replies honestly with \\(sgn(\\ell(s_B))\\). This is the end of player \\(A\\)’s turn. If \\(\\ell\\) separates the secrets, then player A wins. Otherwise it’s player B’s turn. Next player \\(B\\) chooses an affine function \\(\\ell'\\), and asks player \\(A\\) to reply with the sign of \\(\\ell'(s_A)\\). Once player \\(A\\) replies, then this is the end of player \\(B\\)’s turn. Again, if \\(\\ell'\\) separates the secrets, then player B wins. Otherwise it’s player A’s turn.\nThe object of the game is to determine an affine function \\(\\ell\\) which separates the secrets, i.e. for which $ sgn((s_A)) sgn((s_B)).$ The first player to demonstrate an affine function which separates the secrets wins!\nOur interest is to find optimal strategies for this game. Firstly, we have to consider whether any strategy is even possible. For example, can player \\(A\\) use the cumulative history of both player \\(A\\) and \\(B\\)’s affine guesses to better inform their next guess? For example, if player \\(A\\) guesses an affine function \\(\\ell\\) which does not separate, then player \\(B\\) can use the knowledge of their own private secret to determine which halfspace contains \\(s_A\\). And indeed, by the same reasoning player \\(A\\) can use their knowledge of \\(s_A\\) to likewise determine which halfspace contains \\(s_B\\). So obviously the initial distribution \\(d\\lambda\\) is updated to the restricted distribution \\(d\\lambda\\cdot 1_H\\), where \\(H\\) is the halfspace defined by \\(\\ell\\) and containing \\(s_A, s_B\\). With successive guesses, the distribution becomes a descending chain of closed convex sets, namely the intersection of successive halfspaces, having the form \\[d\\lambda \\leadsto d\\lambda \\cdot 1_H \\leadsto d\\lambda \\cdot 1_H 1_{H'} \\leadsto d\\lambda \\cdot 1_H 1_{H'} 1_{H''} \\leadsto \\cdots. \\]\nThe notation is somewhat strange, but simply expresses that we remain uncertain of the specific location of the secrets \\(s_A, s_B\\), except we know the possibly location is becoming more restricted.\nIn the millionaire game, the players \\(A,B\\) have an interest in privacy. Their secrets \\(s_A, s_B\\) are intended to be secret. This means the players \\(A,B\\) might not choose affine functions which potentially reveal information about their own secrets. In practice this means players determined to maintain their privacy will always choose affine functions which do not bound compact convex sets. Similarly, an opponent will not readily choose affine functions which separates the domain into a bounded component, since the probability that the opponent’s secret lies in the bounded component is relatively small, while the probability of its lying in the unbounded component is much greater.\nThe subject of so-called zero knowledge proofs in cryptography is related to the millionaires problem. Here we try to find a balance where the players can choose to reveal as much as they wish of their own balances, while their own guesses are signals/indications in-themselves of the secret balance.\nOur question is whether there is any strategy or skill in this game. What is the optimal strategy? Can the player use the knowledge of the opponent’s affine functions to improve their own selection of affine function??\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Now we simulate the millionaire problem on the euclidean two-dimensional plane.\n# For convenience we rename the players $A,B$ as players $+1, -1$, respectively.\n# for testing purposes we suppose the players A,B have secrets below:\n\n#s_A=input(\"What is player A's secret position?\")\ns_A=[16,0]\ns_A=np.array(s_A)\n\n#s_B=input(\"What is player B's secret position?\")\ns_B=[0, 0.2]\ns_B=np.array(s_B)\n\n\n# now we define some basic functions, i.e. to compute affine functions based on\n# their normal n and height b.\ndef affine(n,x,b):\n    n=np.array(n)\n    x=np.array(x)\n   # return n.dot(x)+b\n    return n[0]*x[0]+n[1]*x[1] + b\n\n\n# to protect the secret we really only need the sign of the affine function.\ndef sign(x_Real):\n    if x_Real&lt;0:\n        return -1\n    else:\n        return +1\n\n# here t defines the test function, which returns True iff the affine function\n# separates the secrets. True is returned if the signs of the affine function\n# evaluated on the secrets are not equal.\ndef t(n,b):\n    n=np.array(n)\n    if sign(affine(n,s_A,b)) != sign(affine(n,s_B, b)):\n         return True\n    else:\n         return False\n\n\n# Now we setup the basic routine, i.e. sequence of gameplay.\n\n#initial conditions.\noutcome=False\nhistory=[]\nvector_history=[]\nplayer=+1\ni=0\ncolor=[]\n\n\nwhile outcome == False:\n    print(\"\\n Player \" + str(player) + \"'s turn to play:\" )\n    print(\"Given the history \" + str(history) + \" choose your affine function:\")\n\n    n0 = float(input())\n    n1 = float(input())\n    b = float(input())\n    history = history + [[n0, n1, b]]\n    vector_history=vector_history + [[n0, n1]]\n    i=i+1\n\n\n    if t([n0, n1], b) == True:\n        outcome = True\n        print(\"Winner! Player \" + str(player)+ \" has separated the secrets with \" + str([n0, n1, b]) + \". End of Game!\")\n    else:\n        print(\"Fail! Player \" + str(player) + \" has failed to separate the secrets... End of turn.\")\n        player=player*(-1)\n\n\n\n# the following plots the various normals chosen by the players, but we would\n# prefer to have the half spaces.\n    V=np.array(vector_history)\n    origin=np.array([[0]*i, [0]*i])\n    plt.quiver(*origin, V[:,0], V[:,1], scale=21)\n    plt.show()\n\n\n Player 1's turn to play:\nGiven the history [] choose your affine function:\n1\n1\n0\nFail! Player 1 has failed to separate the secrets... End of turn.\n\n Player -1's turn to play:\nGiven the history [[1.0, 1.0, 0.0]] choose your affine function:\n3\n1\n0\nFail! Player -1 has failed to separate the secrets... End of turn.\n\n Player 1's turn to play:\nGiven the history [[1.0, 1.0, 0.0], [3.0, 1.0, 0.0]] choose your affine function:\n1\n4\n0\nFail! Player 1 has failed to separate the secrets... End of turn.\n\n Player -1's turn to play:\nGiven the history [[1.0, 1.0, 0.0], [3.0, 1.0, 0.0], [1.0, 4.0, 0.0]] choose your affine function:\n-3\n0\n-5\nFail! Player -1 has failed to separate the secrets... End of turn.\n\n Player 1's turn to play:\nGiven the history [[1.0, 1.0, 0.0], [3.0, 1.0, 0.0], [1.0, 4.0, 0.0], [-3.0, 0.0, -5.0]] choose your affine function:\n-1\n-4\n2\nWinner! Player 1 has separated the secrets with [-1.0, -4.0, 2.0]. End of Game!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe above is a very simple gameplay, where it happens by chance that the two points can be separated by a flat strip, namely the space between two parallel halfspaces. We included the above simply as an example.\n[To Do:] 1. Use matplotlib to plot the halfspaces, and not simply the normal vector, which is what we have above.\n\nDetermine some automatic routine to compete with a human opponent.\nThe millionaire’s problem implicitly assumes the players \\(A,B\\) have large funds, i.e. enough to pay for a dinner! Therefore we might need assume our secrets \\(s_A, s_B\\) are sufficiently far from the origin. (?)\nIf the domain is essentially infinite, then a certain amount of privacy will always be maintained, because it’s better to bisect the unknown into two halfspaces of equal (possibly infinite) area. If the affine function indeed separates the secrets, then the position of that secret is only known to occupy an infinite area domain, and thus essentially remains private in a restricted sense. Although of course the direction of the secret, and not necessarily its magnitude will be better known to the opponent, i.e. there will be a definite reduction of uncertainty in the direction of the opponents secret, but not necessarily a reduction in uncertainty in its magnitude.\n\nIf an opponent proposes an affine function which separates the domain into a bounded and unbounded component, then that is huge risk for the player, i.e. it’s unlikely that the small bounded domain (chosen at random) will contain the secret as opposed to the infinite domain. At the risk of belabouring the point: a random infinite domain is more likely to contain an unknown secret than a compact domain. We find this an interesting point…\n\nfrom scipy.spatial import HalfspaceIntersection\n\nprehistory = history[:-1]\nsigns=[]\nsph=[]\n\nfor x in prehistory:\n    epsilon=sign(affine([x[0], x[1]], s_A, x[2]))\n    signs=signs+[epsilon]\n    sph=sph+[epsilon*np.array(x)]\n\nsph=np.array(sph)\n\n# for illustration we have the secret s_A as feasible_point.\n# its interesting question to select a feasible point which\n# does not reveal too much information about the secrets...\n# but obviously any point on the convex hull formed by the secrets s_A, s_B\n# will be a feasible point. But there are many more choices, so which choice reveals\n# the least information about the secrets s_A, s_B ? I.e. which feasible point can be chosen\n# which reveals the least information about s_A, s_B?\n\nfeasible_point = np.array([16.0, 0.0])\n\nhalfspaces = sph*(-1)\n\n\n# we need reverse-signs to align with the convention in qhull that\n# the halfspaces are defined by the inequality Ax+b &lt;= 0.\nhs = HalfspaceIntersection(halfspaces, feasible_point)\n\nimport matplotlib.pyplot as plt\nfig = plt.figure()\nax = fig.add_subplot('111', aspect='equal')\nxlim, ylim = (-100, 100), (-100, 100)\nax.set_xlim(xlim)\nax.set_ylim(ylim)\nx = np.linspace(-100, 100, 1000)\nsymbols = ['-', '+', 'x', '*']\nsigns = [0, 0, -1, -1]\nfmt = {\"color\": None, \"edgecolor\": \"b\", \"alpha\": 0.5}\nfor h, sym, sign in zip(halfspaces, symbols, signs):\n    hlist = h.tolist()\n    fmt[\"hatch\"] = sym\n    if h[1]== 0:\n        ax.axvline(-h[2]/h[0], label='{}x+{}y+{}=0'.format(*hlist))\n        xi = np.linspace(xlim[sign], -h[2]/h[0], 1000)\n        ax.fill_between(xi, ylim[0], ylim[1], **fmt)\n    else:\n        ax.plot(x, (-h[2]-h[0]*x)/h[1], label='{}x+{}y+{}=0'.format(*hlist))\n        ax.fill_between(x, (-h[2]-h[0]*x)/h[1], ylim[sign], **fmt)\nx, y = zip(*hs.intersections)\nax.plot(x, y, 'o', markersize=8)\n\n[[ 1.  1.  0.]\n [ 3.  1.  0.]\n [ 1.  4.  0.]\n [ 3. -0.  5.]]\n&lt;class 'numpy.ndarray'&gt;\n\n\n\n\n\nThe above intersection of halfspaces isn’t what i expected. The complete intersection is the sector in the upper right hand corner.\nNow if we are truly taking secret points \\(s_A, s_B\\) at random in \\(\\mathbf{R}^2\\), then almost all random choices of affine functions will not separate the secrets. For example, given the homogeneity of \\(\\mathbf{R}^2\\), we can consider the secrets \\(s_A, s_B\\) as being extremely close such that they are basically coincident, or at least as seen from a far distance. But then a random choice of affine function is extremely unlikely to contain the two points. Thus it appears that truly random choices of affine functions have essentially zero probability of separating the secrets.\nThis leads to the next step in our study of the Millionaire Problem, namely where the initial distribution on \\(\\mathbf{R}^2\\) is not necessarily uniform. E.g., perhaps we know that the secrets are distributed within a given large radius ball. If we have no other information about the secrets except that it lies somewhere on the large ball \\(D\\), then one probabilistic strategy is to bisect the ball with affine functions, i.e. randomly guess an affine \\(\\ell\\) such that $ 1_D 1_{&gt;0}$ and \\(1_D 1_{\\ell &gt; 0}\\) have equal area.\nBut what about privacy? In the previous case where the distribution was uniform on its support on \\(\\mathbf{R}^2\\), the privacy of the secrets was maintained so long as the affine functions were unbounded (from above and below). And the opponents would always guess such affine functions because there odds of correctly separating the secrets is significantly increased. But now its possible that the distribution will not be uniform on its support, and therefore the secrets can be learned with a reduction in uncertainty, i.e. perhaps we know that the opponents secret lies in a bounded set, or that 90% percent of the time the opponents secret lies in a given domain.\nWe remark that there is something like a “maximum likelihood” principle being used here. Now regarding privacy: if the domain \\(D\\) is bounded, then depending on the distribution, the secret might have diminished privacy. This leads to Part 2 of our study, where the secrets are distibuted according to nonuniform distributions \\(\\mu_A\\), \\(\\mu_B\\) on \\(\\mathbf{R}^2\\).\n\nWhat is the optimal strategy for nonuniform initial distributions \\(\\mu_A, \\mu_B\\) ?\nCan we quantify the “loss” of privacy when the distributions are supported on unbounded domains versus bounded domains?\n\n(To be continued…)"
  },
  {
    "objectID": "posts/2024-01-04-EconomicsMovingAndDelivery/index.html",
    "href": "posts/2024-01-04-EconomicsMovingAndDelivery/index.html",
    "title": "Economics of Moving and Delivery.",
    "section": "",
    "text": "[Originally written April 2022 -JHM]\nIt’s commonly reported that “moving” or “la demenagement” is among the most stressful events for consumers, c.f. “Americans find moving more stressful than divorce”.\nWhat are the essential difficulties involved in moving? Here we assume we are moving a residential home.\n\nMoving is labour intensive, involving hands-on moving of numerous boxes, furniture (tables, couches, bedsets, dressers), and fragile items (TVs, mirrors, lamps).\nMoving items is highly constrained, often involving heavy items being securely extracted from dwellings, and all this with zero damage to walls or floors.\nAnother difficulty, perhaps unappreciated by the clients, is the necessity of packing the objects into the moving truck.\n\nThis is a type of entropy problem, since the compressed volume of the moving truck restricts the possible range of motions of the objects. Moving requires alot of work and foresight to efficiently pack all the items securely in a truck.\nAnother view of the entropy difficulty is this: a house has many rooms, with the objects distributed sparsely throughout the space. However in a moving truck, all the objects need to be compressed into a single room (namely the box of the truck).\n\nCan the clients estimate the volume of all their objects?\nIs it possible for them to imagine all the objects to be relocated into a single room?\n\nThere is considerable stress involved in the action of, say, extracting heavy expensive “precious” furniture through various stairwells, corners, basements, etc.. The business of last-minute kijiji moving is even more stressful, for the clients are typically totally unprepared. For example, they might be selling a freezer located in the basement of a townhouse, with a very tight spiral staircase, and the client has arranged for its delivery to another basement appartment. The client might be moving their entire household, or only moving this single item. Or the client has received a new treadmill, and require its transport into the basement.\nBuilding codes and standard construction methods make the extraction and deliveries somewhat easier. However extreme furniture pieces often push the movers ingenuity to the extreme, and requires alot of experience to immediately know which precise “furniture ballet” is required. As a rule of thumb : if an object makes three points of contact with the wall/floor/ceiling, then the object cannot be pushed any further without causing damage to the surroudings.\nSo how can the consumer save money when moving?\nThe answer is basically preparation.\nTo save money on moving: order a very large truck to make the loading and offloading easier, and pack as much as possible in regular cardboard boxes. This cannot be overremphasized: as much as possible, all the irregular objects should be packed into boxes, and into as many boxes as necessary.\nWhy?\nBecause it’s expensive for the movers to waste their time in arranging and sometimes rearranging irregular shapes objects into the truck.\nThe client should dissessemble the furniture as much as possible beforehand. Otherwise the movers need to spend working hours on the assembly/disassembly of furniture, and/or the preparation of more fragile objects.\nAnother difficulty is the patience and time required to safely move objects in tight spaces. For example, let us consider IKEA items which are very popular. One of the keys to IKEA’s business model is that their furniture is transported and sold totally disassembled, and neatly packing into boxes. I think it’s evident that tables and dressers dissassembled in boxes is more convenient for transport than fully assembled! In IKEA the consumer is required to read the instructions to assemble their items.\nAnother important constraint in domestic moves is: what is the pathway from the pickup to the truck, then from the truck to the dropoff. These are the environmental factors. Is the client moving in/out of a 20 storey building? Are we moving everything through elevators and long hallways? Is the client moving in/out of a basement? How many stairs? How close can the truck get to the unit?\nNow IKEA and movers are not compatible. For the movers might not be able to move IKEA items in their assembled state. Why? Because the IKEA items are fragile, and not designed to be moved. However IKEA items are also not easily disassembled without causing damage (typically cosmetic) to the items. Therefore movers are typically required to transport IKEA items “as is”, and this is a challenge. For assembled IKEA items have no strength, and are not at all designed for “strongman” transport.\nNow for all the complicated parameters that exist in moving, in this article our goal is to reduce everything to the simplest variables. Basically, if a client calls and wants to move their entire household, and wants to have an estimate (or the moving manager wants an estimate for their own schedule), then we ask the following questions:\n\nWhen is the last time the client has moved their household?\nIf appplicable, ask how long it took and how many “human labour hours” were required for their last move?\nHow many “bedrooms” are now being moved?\n(Basic logistics: pickup and dropoff addresses).\nWhat kind of heavy items? (Fridges, couches, exercises equipment, etc.).\n\nWe make some comments: If the client is only moving a select number of items, then ask client “how did the item get here, how many persons were involved, how long did it take”.\nQuestion 1. gives a lower bound (“a floor”) for the moving manager. Our experience is that people only accumulate items, even more items, after they move. When persons are settled in a location, then they collect more and more diverse items. This always adds to the time required and increases the complexity. Question 2. gives some idea, for example was it a team of four movers or two? Was it a big truck, or a smaller cube truck? Were there any incidents during that last move, particular events or damages to the items?\nQuestion 2. is applicable only if the client can remember the last time the item was moved. However it does help manage the clients expectations.\nQuestion 3. is important, especially for single persons who have recently moved themselves. For every single person, there is required approximately 6 to 10 total labour hours required. I.e. two movers require approximately 3 to 5 hours to move a single person (bachelor). This is large interval, which really determines on the particular circumstances. Heavy objects can take 15 – 30 minutes per item to move per team of two.\nQuestions 4, 5 are rather standard. Some estimate of the travel time and circumstances is necessary. For example, if the clients are located in an appartment building, then there is frequently a large walking distance required, and if there is an elevator involved, then the time can be much longer.\n[End –JHM]"
  },
  {
    "objectID": "posts/2024-01-03-SpineForTeichmuellerSpace/A Spine for Teichmueller Space of Closed Hyperbolic Surfaces (Draft).html",
    "href": "posts/2024-01-03-SpineForTeichmuellerSpace/A Spine for Teichmueller Space of Closed Hyperbolic Surfaces (Draft).html",
    "title": "Spine for Teichmueller Space of Closed Hyperbolic Surfaces",
    "section": "",
    "text": "The main purpose of this article is to establish the following:\n\n** Proposition:** We claim a minimal dimension spine \\(W\\) of Teichmueller space \\(Teich(S)\\) consists of surfaces which are filled by their shortest essential nonseparating geodesics.\n\nThis has been recently announced by [ref] on arxiv. Our work is independant of this paper, which upon quick review, is based on tedious cellular arguments which we avoid.\nA complete proof of this fact requires: - (i) that we construct continuous equivariant deformation retract \\(\\text{Teich} \\leadsto W\\); - (ii) that we prove \\(\\dim W = \\dim Teich - (2g-1)\\)\nOur retraction is based on constructing harmonic one forms on the surface which are “adapted” to the short curves on \\(S\\). The key fact is that we can simultaneously increase the lengths of these short curves by flowing along a Teichmueller deformation in the direction of the above harmonic one form."
  },
  {
    "objectID": "posts/2024-01-03-SpineForTeichmuellerSpace/A Spine for Teichmueller Space of Closed Hyperbolic Surfaces (Draft).html#introduction",
    "href": "posts/2024-01-03-SpineForTeichmuellerSpace/A Spine for Teichmueller Space of Closed Hyperbolic Surfaces (Draft).html#introduction",
    "title": "Spine for Teichmueller Space of Closed Hyperbolic Surfaces",
    "section": "",
    "text": "The main purpose of this article is to establish the following:\n\n** Proposition:** We claim a minimal dimension spine \\(W\\) of Teichmueller space \\(Teich(S)\\) consists of surfaces which are filled by their shortest essential nonseparating geodesics.\n\nThis has been recently announced by [ref] on arxiv. Our work is independant of this paper, which upon quick review, is based on tedious cellular arguments which we avoid.\nA complete proof of this fact requires: - (i) that we construct continuous equivariant deformation retract \\(\\text{Teich} \\leadsto W\\); - (ii) that we prove \\(\\dim W = \\dim Teich - (2g-1)\\)\nOur retraction is based on constructing harmonic one forms on the surface which are “adapted” to the short curves on \\(S\\). The key fact is that we can simultaneously increase the lengths of these short curves by flowing along a Teichmueller deformation in the direction of the above harmonic one form."
  },
  {
    "objectID": "posts/2024-01-03-SpineForTeichmuellerSpace/A Spine for Teichmueller Space of Closed Hyperbolic Surfaces (Draft).html#c-systoles-homological-rank-and-filling.",
    "href": "posts/2024-01-03-SpineForTeichmuellerSpace/A Spine for Teichmueller Space of Closed Hyperbolic Surfaces (Draft).html#c-systoles-homological-rank-and-filling.",
    "title": "Spine for Teichmueller Space of Closed Hyperbolic Surfaces",
    "section": "C-Systoles, Homological Rank, and Filling.",
    "text": "C-Systoles, Homological Rank, and Filling.\nWe let \\((S,g)\\) be a closed hyperbolic surface with constant Gauss curvature \\(K=-1\\). We begin with some definitions and notation.\nDefinitions: A collection \\(C\\) of curves is essential if the curves are homotopically nontrivial. The collection is nonseparating if the curves are nonzero \\([\\alpha]\\neq 0\\) in \\(H_1(S, \\bf{Z})\\) for \\(\\alpha \\in C\\). Let \\(C=C(g)\\) be the set of geodesic nonseparating essential curves on \\((S,g)\\).\nDefinition: Let \\(C'\\) be a subset of \\(C\\). The complexity of \\(C'\\) is defined as the rank of the homological image of \\(C'\\), namely \\[\\xi(C'):=\\dim \\text{span} (H_1(C')).\\]\nWe observe \\(\\xi=\\xi(C')\\) is an integer taking every integral value between \\(1 \\leq \\xi \\leq 2g\\) where \\(g\\) is the genus of the surface \\(S\\).\nDefinition: The \\(C\\)-systole of \\((S,g)\\) consists of the shortest curves in \\(C\\) relative to \\(g\\)-length. We denote the \\(C\\)-systoles of a given metric \\(g\\) by \\(C'=C'(g)\\).\nDefinition: A collection of curves \\(C_0\\) fills the surface \\(S\\) if the complement \\(S-C_0\\) is a disjoint union of topological disks.\nLemma: A subset \\(C_0 \\subset C\\) is filling if and only if \\(\\xi(C_0)=2g\\).\nProof: [Incomplete]"
  },
  {
    "objectID": "posts/2024-01-03-SpineForTeichmuellerSpace/A Spine for Teichmueller Space of Closed Hyperbolic Surfaces (Draft).html#belt-tightening-lemma",
    "href": "posts/2024-01-03-SpineForTeichmuellerSpace/A Spine for Teichmueller Space of Closed Hyperbolic Surfaces (Draft).html#belt-tightening-lemma",
    "title": "Spine for Teichmueller Space of Closed Hyperbolic Surfaces",
    "section": "Belt Tightening Lemma",
    "text": "Belt Tightening Lemma\nThe following Lemma is our main observation:\nBelt Tightening Lemma: Let \\((S,g)\\) be hyperbolic surface with \\(C\\)-systoles \\(C'\\). There exists a one-parameter deformation \\(\\{g_t\\}\\) in \\(Teich(S)\\) such that:\n\nthe metric \\(g_t\\) is hyperbolic for all \\(t\\geq 0\\) and \\(g_0=g\\);\nthe curve lengths \\(\\ell(\\gamma, g_t)\\) are simultaneously increasing for all \\(t \\geq 0\\) and all \\(\\gamma \\in C'\\).\n\nThe proof depends on the construction of an explicit harmonic function on the surface-with-corners \\(S - C'\\). This construction is provided in Lemma [ref].\nProof: Let \\(\\phi=d\\hat{u}\\) be the unique harmonic one-form constructed in Lemma [ref]. Let $ q_t:= (e^t + i e^{-t} ^{*})^2$ for \\(t\\geq 0\\). Define \\(g_t\\) to be the resultant hyperbolic structure \\((q_t)\\# g\\). [Incomplete]\nClaim: The lengths of the curves in \\(C'\\) are provided by [formula] and the lengths are simultaneously increasing with respect to the deformation. QED."
  },
  {
    "objectID": "posts/2024-01-03-SpineForTeichmuellerSpace/A Spine for Teichmueller Space of Closed Hyperbolic Surfaces (Draft).html#harmonic-functions-on-surfaces-with-corners",
    "href": "posts/2024-01-03-SpineForTeichmuellerSpace/A Spine for Teichmueller Space of Closed Hyperbolic Surfaces (Draft).html#harmonic-functions-on-surfaces-with-corners",
    "title": "Spine for Teichmueller Space of Closed Hyperbolic Surfaces",
    "section": "Harmonic Functions on Surfaces-With-Corners",
    "text": "Harmonic Functions on Surfaces-With-Corners\nOn a Riemann surface \\((S,g)\\) we let \\(*\\) (“star”) denote the Hodge star operator. The Cauchy-Riemann theorem says harmonic one-forms on Riemann surfaces are precisely the real parts of analytic complex differentials. In Lemma [ref] we construct a holomorphic one-form \\(\\phi + i \\phi^*\\) which is canonically defined by the \\(C\\)-systoles of a Riemann surface \\(S\\). The complex square \\(q:=(\\phi+i \\phi^*)^2\\) defines a holomorphic quadratic differential on the surface. The transverse invariant foliations of this quadratic differential are the real and imaginary parts of \\(\\sqrt{q}\\), and indeed defined by \\(\\phi\\) and \\(\\phi^*\\). The retraction constructed in Lemma [ref] depends on repeated application of a flow \\(t\\mapsto e^{-t} \\phi + i e^t \\phi^*\\) constructed from the harmonic one-form \\(\\phi\\).\nIn general the systoles \\(C'=C'(g)\\) define a link in the surface. This means \\(C'\\) has an almost everywhere uniquely defined normal vector except at the geometric intersections of certain curves \\(\\gamma \\cap \\gamma'\\). But these intersections are finite. Therefore we consider the normal derivative \\(n\\) as defined everywhere except some isolated finite number of points on \\(C\\) in \\(S\\).\nLemma: (Unique Harmonic Extensions) Let \\((S,g)\\) be a hyperbolic surface with \\(C\\)-systoles \\(C'\\). There exists harmonic extensions \\(\\hat{u}\\) on \\(S\\) satisfying the measurable Neumann boundary condition \\[\\frac{\\partial u}{\\partial n}=1~~\\text{almost everywhere on}~~C'.\\] The harmonic extension \\(\\hat{u}\\) is unique up to additive constant.\nProof: The curves \\(C'\\) are geodesics in the surface \\(S\\), and \\(C'\\hookrightarrow S\\) is a link. We cut the surface along this link to obtain the hyperbolic surface with corners \\(S-C'\\). Poisson’s fundamental theorem [insert ref] says there exists a unique harmonic extension \\(\\hat{u}\\) onto \\(S-C'\\) with prescribed normal derivative on the boundary.\nClaim: The extension \\(\\hat{u}\\) is nonconstant since \\(n\\cdot \\nabla u =1 \\neq 0\\) by hypothesis and is unique up to additive constant. Therefore the harmonic one-form \\(\\phi:=d\\hat{u}\\) is uniquely defined on \\(S\\).\nRemark: The purpose of the harmonic one form \\(\\phi\\) is to define the quadratic holomorphic differential \\(q=(\\phi+i \\phi^{*})^2\\) whose measured foliations are precisely given by \\(\\phi, \\phi^*\\). The key point is that Teichmueller type deformations $ q_t:= (e^t + e^{-t} ^*) $ for \\(t \\geq 0\\) are well-defined analytic deformations on Teichmueller space. In otherwords \\(\\{~ q_t~|~t\\geq 0 ~ \\}\\) is an explicit variation of hyperbolic structure, and there is a pushforward of hyperbolic structures \\(g_t:=q_t\\#g.\\)\nRemark: We want to relate \\(\\phi=d\\hat{u}\\) to the hypothesis that \\(\\xi(C') &lt;2g\\). In otherwords, we need a comment on cohomology and Hodge theorem to say that \\(\\phi\\) is nonzero when \\(\\xi &lt; 2g\\). This could be established by period arguments, since by construction it’s evident that \\(\\int_\\gamma \\phi \\neq 0\\) (?) [Error Careful]\nLemma:(?) If \\(C'\\) does not fill \\(S\\), then the harmonic extension \\(\\bar{u}\\) constructed in Lemma [ref] is nonconstant on \\(S\\).\nProof. [Incomplete]"
  },
  {
    "objectID": "posts/2024-01-03-SpineForTeichmuellerSpace/A Spine for Teichmueller Space of Closed Hyperbolic Surfaces (Draft).html#well-rounded-retract-of-teichmueller-space",
    "href": "posts/2024-01-03-SpineForTeichmuellerSpace/A Spine for Teichmueller Space of Closed Hyperbolic Surfaces (Draft).html#well-rounded-retract-of-teichmueller-space",
    "title": "Spine for Teichmueller Space of Closed Hyperbolic Surfaces",
    "section": "Well-Rounded Retract of Teichmueller Space",
    "text": "Well-Rounded Retract of Teichmueller Space\nHere is our proposal for constructing well-rounded retracts of \\(Teich(S)\\).\nRemark: We observe that if \\(C'\\) does not fill \\(S\\), then the harmonic extensions \\(\\hat{u}\\) are everywhere nonconstant by Lemma [ref]. This implies the Teichmueller deformations \\(q_t\\) are nontrivial for \\(t \\geq 0\\).\nDefinition: For every index \\(1 \\leq j \\leq 2g\\), let \\(W_j\\) be the subvariety of \\(Teich(S)\\) consisting of hyperbolic metrics whose \\(C\\)-systoles satisfy \\(\\xi(C) \\geq j\\).\nTheorem: The Teichmueller space \\(Teich\\) continuously and equivariantly retracts onto \\(W=W_{2g}\\). Moreover \\(W\\) has codimension \\(2g-1\\) in \\(Teich\\) and therefore is a minimal dimension spine of \\(Teich\\).\nProof: The retract \\(Teich \\to W\\) is defined as a composition of retracts \\(W_1 \\to W_2 \\to \\cdots \\to W_{2g}.\\) The general retract \\(W_{j} \\to W_{j+1}\\) is defined as follows:\nLet \\((S,g)\\) be a hyperbolic surface in \\(W_j\\) with \\(\\xi(C(g))=j &lt; 2g\\). Let \\(\\{g_t\\}\\) be the unique one-parameter deformation of hyperbolic metrics constructed in Belt Tightening Lemma which simultaneously expands the lengths of \\(C'\\).\nClaim: There exists a minimal stopping time \\(\\tau=\\tau(g)\\) which depends continuously on \\(g\\) such that \\(g_\\tau \\in W_{j+1}\\). Equivalently \\(\\tau\\) is the unique minimal time such that a new independant \\(C\\)-systole appears and which strictly increases the complexity \\(\\xi\\). Analytically \\(\\tau\\) is defined as the least time \\(t\\) such that \\[\\xi(S_0(g_t)) &gt; \\xi(S_0(g)).\\]\nClaim: The one-parameter deformation defines a continuously well-posed global retraction \\(g\\mapsto g_\\tau\\) from \\(W_j\\) to \\(W_{j+1}\\).\nClaim: The subvariety \\(W_{j+1}\\) is a codimension one subvariety of \\(W_j\\). Therefore \\(W_{2g}\\) is a codimension \\(2g-1\\) subvariety of \\(Teich\\). This is the minimal possible dimension according to Bieri-Eckmann homological duality. QED.\nRemark: Geometric minimality requires a further homological duality argument a la [Souto-Pettet]."
  },
  {
    "objectID": "posts/2024-01-03-SpineForTeichmuellerSpace/A Spine for Teichmueller Space of Closed Hyperbolic Surfaces (Draft).html#conclusion",
    "href": "posts/2024-01-03-SpineForTeichmuellerSpace/A Spine for Teichmueller Space of Closed Hyperbolic Surfaces (Draft).html#conclusion",
    "title": "Spine for Teichmueller Space of Closed Hyperbolic Surfaces",
    "section": "Conclusion",
    "text": "Conclusion\nWe still need to complete the above proofs. But the idea of the construction is clear, with the critical point being the construction of harmonic one forms, and the possibility of simultaneously increasing lengths of non filling curves along Teichmueller deformations.\n[To be continued – JHM]"
  },
  {
    "objectID": "posts/2024-01-02-Vintage/index.html",
    "href": "posts/2024-01-02-Vintage/index.html",
    "title": "Mathematics of Vintage Selling",
    "section": "",
    "text": "Vintage selling is risk and reward. Here’s our fundamental premise about selling: “People only buy what they see.” They have to see it to believe it, and then they buy.\nHere’s some questions to introduce everything:\n\nPick a random item in the store. “How many people on average have to walk into the store until the item is sold? How many people until the item has a \\(50 \\%\\) chance of selling?”\n“How many customers per week are entering the store? What’s the value of increasing the foot traffic by 15 percent into the store?”\n“Is it better to sell a given item for twenty dollars today, or thirty dollars next month?”\n“How many customers have to see an item and say no until the item is purged from inventory or the selling price updated for clearance?”"
  },
  {
    "objectID": "posts/2024-01-02-Vintage/index.html#risk-reward-and-people-see-to-believe-then-buy.",
    "href": "posts/2024-01-02-Vintage/index.html#risk-reward-and-people-see-to-believe-then-buy.",
    "title": "Mathematics of Vintage Selling",
    "section": "",
    "text": "Vintage selling is risk and reward. Here’s our fundamental premise about selling: “People only buy what they see.” They have to see it to believe it, and then they buy.\nHere’s some questions to introduce everything:\n\nPick a random item in the store. “How many people on average have to walk into the store until the item is sold? How many people until the item has a \\(50 \\%\\) chance of selling?”\n“How many customers per week are entering the store? What’s the value of increasing the foot traffic by 15 percent into the store?”\n“Is it better to sell a given item for twenty dollars today, or thirty dollars next month?”\n“How many customers have to see an item and say no until the item is purged from inventory or the selling price updated for clearance?”"
  },
  {
    "objectID": "posts/2024-01-02-Vintage/index.html#formal-model-time-to-sale-holding-cost-surplus.",
    "href": "posts/2024-01-02-Vintage/index.html#formal-model-time-to-sale-holding-cost-surplus.",
    "title": "Mathematics of Vintage Selling",
    "section": "Formal Model: Time to Sale, Holding Cost, Surplus.",
    "text": "Formal Model: Time to Sale, Holding Cost, Surplus.\nFormally we assume there is a collection \\(X\\) of clothing items available to the vintage seller. The seller chooses to buy items \\(x\\) at price \\(-\\psi(x)\\) measured in units of dollars.\nThe item \\(x\\) is in inventory for a random period until “time to sale” which we denote by \\(\\tau=\\tau(x)\\). The time to sale occurs when some customer eventually purchases \\(x\\) at sale price \\(+ \\phi(x)\\). The time to sale \\(\\tau\\) is a random variable \\(\\tau: X \\to R_{&gt; 0}\\). The law of \\(\\tau\\) is unspecified at this stage.\nWe assume there is a holding cost \\(\\alpha: R_{&gt;0} \\to R_{&gt; 0}\\) representing the cost of holding an item in inventory over a period of time \\(t\\). We assume the holding cost \\(\\alpha\\) is independant of \\(x\\) and depends only on time. The inventory cost \\(\\alpha\\) depends on operating hours, rent, basically the expenses of running the business from day to day. Ideally these are fixed costs and \\(\\alpha\\) becomes linear function of \\(\\tau\\).\nDefinition:(surplus beta) If item \\(x\\) is sold at future time \\(\\tau\\) and price \\(\\phi(x)\\), the seller’s surplus \\(\\beta\\) on item \\(x\\) is represented as \\[\\beta(x) :=  -\\psi(x) -\\alpha. \\tau(x) + \\phi(x).\\]\nThe vintage seller’s goal is to find inventory and customers with prices \\(\\psi, \\phi\\) which maximize surplus beta \\(\\beta\\).\nMaximizing beta is art and science. The supply of clothing \\(X\\) is huge, and the seller puts capital at risk when purchasing the item (represented by \\(-\\psi\\)) and commiting capital to future holding costs (represented by the random cost \\(-\\alpha.\\tau\\)). The art is finding high beta inventory, where maximizing beta \\(\\beta\\) means optimizing the market conditions where supply of items \\(x\\) meets customers in timely manner (\\(\\tau(x)&gt;0\\) not too large) and at good price (\\(-\\psi+\\phi\\) positive nominal profit).\nBeta \\(\\beta\\) represents net gain when \\(\\beta=\\beta(x)\\) is positive \\(&gt; 0\\) for a given item \\(x\\). On average the seller is profitable if the average of \\(\\beta(x)\\) over the entire supply \\(x\\in X\\) is positive, but ideally we want a positive surplus for every inventory item. This is the simplest way to ensure the average is positive as desired.\nRemark: The time to sale \\(\\tau=\\tau(x)\\), the holding cost \\(\\alpha\\), and the surplus \\(\\beta\\) are random variables with unknown probability laws at this stage. To statistically sample we need apply a “Rule of Five”. This means taking a random sample of five customers, and noting their transaction and experience in detail. The averages of these five random customers will give useful information."
  },
  {
    "objectID": "posts/2024-01-02-Vintage/index.html#how-long-until",
    "href": "posts/2024-01-02-Vintage/index.html#how-long-until",
    "title": "Mathematics of Vintage Selling",
    "section": "How Long Until…?",
    "text": "How Long Until…?\nNaively one looks to maximize the price difference \\(-\\psi(x)+\\phi(x)\\). But realistically the item \\(x\\) might never sell at the price \\(\\phi(x)\\). The item will therefore sit in inventory for long time \\(T\\) and represent a persistent negative cost and liability represented in \\(-\\alpha.T\\). Sellers might consider defining a baseline to purge items from their inventory. This is to avoid the sunken cost fallacy, but also because unsold items can interfere with sale of other items. Unused items lock up inventory, waste the client’s time, and basically the seller has better things to sell. So there’s better use of the space. A rule of thumb for purging, for example, might be “never buy the same item twice” represented by the rule of purging \\(x\\) after time \\(t\\) if \\(\\psi(x) &lt; \\alpha(t)\\)."
  },
  {
    "objectID": "posts/2024-01-02-Vintage/index.html#where-is-the-lowest-risk-return-in-vintage-market",
    "href": "posts/2024-01-02-Vintage/index.html#where-is-the-lowest-risk-return-in-vintage-market",
    "title": "Mathematics of Vintage Selling",
    "section": "Where is the lowest risk return in vintage market?",
    "text": "Where is the lowest risk return in vintage market?\nWherever there is risk there needs be reward. The vintage sellers have capital at risk, as represented by the costs \\(-\\phi, \\alpha\\). So what is their fair reward? The standard economics textbook answer says the interest earned from government bonds, for example, is a risk free return. Therefore every vintage seller could buy bonds instead of clothes, but this is obviously unrealistic. Yet the environment of vintage selling somewhere has it’s own form of risk free return, and this lowest risk return needs be identified. It could be in the form of charitable donations and tax receipts.\n[To be continued - JHM]"
  },
  {
    "objectID": "posts/2024-01-02-FaradayCage/Mathematical Review of Faraday Cages - A Gap In the Literature.html",
    "href": "posts/2024-01-02-FaradayCage/Mathematical Review of Faraday Cages - A Gap In the Literature.html",
    "title": "Mathematical Review of Faraday Cages. A Gap In the Literature.",
    "section": "",
    "text": "In this article we review the basic properties of Faraday cages.\nWe were brought to this subject reviewing Professor A.K.T. Assis’ book with J.A. Hernandes “The Electric Force of a Current”. Assis remarks that Faraday cages have surprising properties which are elementary, but not well known. Moreover the authors of [ “Mathematics of the Faraday cage.” Siam Review 57.3 (2015): 398-417] were surprised to find no readily available references or mathematical explanations of Faraday’s cage. This was the beginning of our investigation.\n\nHollow Conductors\nIn this article \\(C\\) denotes a conductive body in \\(R^3\\). This means \\(C\\) has a boundary surface \\(S = \\partial C\\), and has a complement \\(R^3 - C\\). We say \\(C\\) is hollow if \\(C\\) divides \\(R^3\\) into “inside” and “outside” volumes, i.e. if \\(R^3-C\\) consists of two connected components. In our examples \\(C\\) will be a hollow spherical shell. The width of the shell is thin. By contrast we might otherwise have a solid spherical ball, and in this case the conductor is solid.\nTo say the body \\(C\\) is a conductor means the shell consists of metal or aluminum, something with abundance of free mobile electrons. By contrast an insulator is made of glass, or ceramic porcelain, or rubber, or oil and plastics.\nIn the setting of Faraday cages, we have a hollow conducting body \\(C\\). Topologically this means \\(C\\) divides the ambient space \\({\\bf{R}}^3\\) into two volumes \\({\\bf{R}}^3 - C = V_i \\coprod V_o\\). We consider \\(V_i, V_o\\) as the inner and outer volumes bounded by \\(C\\). The coproduct symbol “\\(\\coprod\\)” emphasizes that the volumes \\(V_i\\), \\(V_o\\) are disjoint. For a hollow conductor \\(C\\), the boundary intersections \\(\\partial V_i\\) and \\(\\partial V_o\\) are disjoint and also partition the topological boundary \\(S=\\partial C\\) of the conductor. For example a hollow sphere has two boundary components, namely the inner and outer surfaces, which are disjoint and bound the inner and outer volumes.\nWith Faraday cages we consider a hollow conductor \\(C\\) and study the presence or absence of electrical forces in the interior volume. The basic question motivating Faraday cages is this:\n\n“If \\(C\\) is a hollow conductor and \\(F_{ext}\\) is an external electric force originating from the exterior volume, then what is the net electric force on the interior volume \\(V_i\\)?”\n\nActually we will see many Faraday cages are bird cages, being not hollow and having no well-defined topological interior or exterior, i.e. the bird cage does not divide \\(R^3\\) into two volumes! In these cases the Faraday shielding effect is informally(!) phrased as:\n\n“If \\(C\\) is a bird cage conductor (not hollow) and \\(F_{ext}\\) is an electric force originating from an external source, then what is the net electric force inside the bird cage?”\n\nWe should remark that bird cages have no interor or exterior, unless you’re a bird who can’t fit through the grates. From the bird’s point of view, the cage divides space into two volumes which are disjoint and inaccesible.\n\nTherefore we do not deny the apparent shielding effects. But we think there is yet no satisfactory derivation. Historically there are two opposing viewpoints. First we have the standard Faraday-Maxwell viewpoint that conductors have a “shielding effect” and the electric field vanishes within the interior of \\(V_i\\) in the above setting. Second we have the nonstandard viewpoint of Weber-Kirchoff that the electric field is nonzero on the interior of \\(V_i\\). Both claims cannot be correct.\n\n\nPoisson’s Theorem and Surface Charge Distributions:\nWe begin with Poisson’s fundamental theorem on electrostatics (1812).\n\n“When arbitrary electric forces act upon a conductor of arbitrary form from the outside, a distribution of free electricity on the surface of the conductor is always possible – but only one of them – for which the electric forces that originate from that distribution of free electricity will likewise be in equilibrium with the electric forces at all points of the interior of the conductor that act from the outside.” [Quoted from Weber, s 13.29].\n\nNotice the topological expressions e.g. “from the outside”, “on the surface”, “of the interior”. In practice the “inside of \\(C\\)” and the “interior volume bounded by \\(C\\)” are frequently confused, especially in the case when the conductor \\(C\\) is a thin volume, e.g. thin hollow spherical shell \\(\"O\"\\).\nHere’s the formal statement of Poisson’s theorem:\n\n“Given a conducting body \\(C\\), and a net external force \\(F_{ext}\\) acting on \\(C\\), then there exists a unique surface charge distribution \\(\\sigma\\) on the boundary surface \\(\\partial C\\) such that \\[F_{ext}(x)+F_\\sigma(x) =0~~\\text{for every}~~x\\in C.\"\\]\n\nThe key point is that the force throughout the volume of the body is balanced by a charge distribution supported only on the boundary surface \\(S=\\partial C\\).\n[Comment about surface charge distributions and normal derivative] The surface charge \\(\\sigma\\) is obtained via the normal derivative of the potential \\(\\phi\\) along the boundary, namely \\(\\frac{\\partial \\phi}{\\partial n}\\), which is by definition equal to \\(\\nabla \\phi \\cdot {\\bf{n}}~ dS\\) where \\(\\bf{n}\\) is the “outer” normal of the surface.\nPoisson claims the net force \\(F_{net}(x)=0\\) vanishes for \\(x\\in C\\) everywhere inside the conductor. However the Faraday-Maxwell shielding effect asserts that \\(F_{net}(x)=0\\) everywhere in the inner volume \\(x\\in V_i\\). We argue with Weber. et al, that indeed \\(F_{net}\\) is nonzero in \\(V_i\\)!\nProof of Poisson’s Theorem: [Incomplete] [Green-Thompson Energy Argument?]\n\n\nMethod of Images \nWhat are we saying, that Faraday cages don’t work? That you shouldn’t put your face next to the microwave? No, well maybe.\n[Method of images: Review!] Here is the basic example: suppose the conductor \\(C\\) is the lower halfspace \\(\\{(x,y)~|~y\\leq 0\\}\\). Suppose an electric point charge \\(+q\\) is placed external to \\(C\\), i.e. somewhere on the vertical half line \\(y&gt;0\\). Then Poisson’s theorem says there exists a surface distibution on \\(C\\) which balances the net force of the charge \\(+q\\). Here we argue by the method of images. The test charge \\(+q\\) induces a charge in the conductor equivalent to the field generated by an image charge \\(-q_i\\). The net electric potential is therefore the sum of the potentials generated by these point sources \\(+q\\) and \\(-q_i\\), and the electric field is the sum of the electric fields generated by the sources \\(+q, -q_i\\). In this example we see the boundary surface \\(\\partial C\\) is an equipotential surface.\nRemark. The argument is usually erroneously made via Gauss’ law. The error is supposing that a vanishing charge density implies a vanishing electric field. This is false! Gauss’ law relates the divergence of the electric field to the charge density \\(\\rho\\). But vanishing divergence \\(div(\\nabla \\phi)\\)=0$ of course does not imply a vanishing gradient \\(\\nabla \\phi\\). [See links below].\n\n\n“But What About Experiments?”\nBut what about all the experiments? A google search gives twenty links on how to make your own Faraday cage in five minutes for five dollars. We argue that there is an apparent shielding effect, yet this shielding effect is unexplained hitherto.\nFor example it’s recorded that Faraday built a room in 1836 coated with metal foil (specific metal unknown) and could not detect any electrical forces inside the room neither near it’s walls. But what is the explanation?\nIt’s interesting that many applications of Faraday cages involve bird cage designs, where the conductor \\(C\\) actually does not topologically divide the space into inner and outer volumes. In applications the following heuristic is introduced: that the cage only shields radiation which is too large to impinge and pass through the “grates”. This does not really follow from the usual arguments and involves new implicit hypotheses.\n\n\n\nScreenshot from “Prelude to Power”\n\n\n\n\n\nibid\n\n\n\n\n\nibid\n\n\n\nKatie Loves Physics on history of Faraday Cage.\n“How to derive Faraday cage properties from Maxwell equations?”.\n“Why is charge distribution on outer surface zero?”\n“How is electric field inside hollow conducting sphere zero?”\n“What is electric field inside hollow conducting sphere?’\n“What is electric field inside Faraday cage?”\n“What is electric field inside hollow conducting body?”\nEMP Misconceptions.\n\nThere’s alot of different answers, but always ending with a nonexplanation, simply the claim that the resultant electric fields cancel to zero. Similarly there is repeated erroneous argument based on Gauss’ law. But again we must emphasize that the vanishing charge density does not imply a vanishing electric field! This is elementary mistake repeated again and again.\n\n\nProf. Lewin’s Experiment\nProf. Lewin’s experiment does not seem convincing. He has a conductive ping pong. Are we expected to believe that if his pingpong touches the positively charged surface, then this positive charge will be imparted to the pingpong rod, and eventually ? This is only if the positive charges are mobile and attracted to the rod.\nHe touches one side of the box and it’s negatively charged. He touches the other side of the box, and he sees that it’s positively charged. (He’s collecting these charges using the condutive ping pong, then discharging at an electrometer to see the polarity, positive or negative charge). But then Prof. Lewin places the pingpong in the volume bounded by the conductor can, and he moves the ping pong around and seems to indicate that he collects no free electric charges. The absence of charges is shown by the electrometer registering no charge. But what does this show? Not that there is zero net force in the interior, but rather that there are no free electrons!\nIs this fair critique? There are no mobile positive ions which are collected by the pingpong and then deposited. Rather there’s only mobile electrons. On the negative side, there is an excess of electrons which are possibly attracted to the pingpong. On the positive side there is an absence of electrons. When the pingpong conductor is placed in physical contact with the box, there is a flow of electrons from the pingpong conductor to the box."
  },
  {
    "objectID": "posts/2023-03-13-DoldThom_Part3/index.html",
    "href": "posts/2023-03-13-DoldThom_Part3/index.html",
    "title": "Dold Thom Theorem. Configuration Spaces and Homology. (3/3)",
    "section": "",
    "text": "\\(\\newcommand{\\sub}{\\partial^c \\psi(y)}\\)\nThis will be final post on Dold-Thom theorem for a while. It’s a strange result which gives a reconceptualization of homology. The key is the introduction of the topological group Dold Thom group \\(A(X)\\) defined as the kernel of the augmentation map \\(\\epsilon: G(X) \\to G\\).\nWith these posts we have presented the main statement of a relative Dold-Thom theorem which allows us to further prove the natural equivalence between the long exact sequence in homotopy with the long exact sequence in reduced and relative homology, as arising from the short exact sequence and quasifibration \\[0 \\to A(Y) \\to A(X) \\to A(X/ (Y \\cup \\emptyset)) \\to 0.\\]\nWe were initially motivated by electric ideas, how charges behave and neutralize on a conductive plate. They either cancel/neutralize in the interior, or they escape to the boundary. That is the same idea in the short exact sequence above. Cycles are either cycles in the interior, or they are cycles modulo \\(Y\\) by their escaping to \\(Y\\) as a reservoir.\n\nSweepouts and Optimal Transport\nFrom Dold-Thom we moved to sweepouts and optimal transportation (OT). The goal is to construct interesting sweepouts of a given source Riemannian manifold \\((X,\\sigma)\\) using optimal transportation methods and variational data. If \\((Y, \\tau)\\) is a target space satisfying \\(\\dim(X) \\geq \\dim(Y)\\) and $_X _Y , $ then costs \\(c: X\\times Y \\to \\bf{R}\\) which satisfy assumptions labelled (A0123) admit unique \\(c\\)-optimal semicouplings and potentials solving Kantorovich’s dual max program, namely \\(c\\)-concave potentials \\(\\psi\\) on the target \\(Y\\). The \\(c\\)-concave potential, or specifically it’s \\(c\\)-subdifferential, generates a \\(Y\\)-parameter family of subsets \\(Z(y):=\\sub\\). We study \\(y\\mapsto Z(y)\\) as a topological sweepout of \\(X\\) in the sense of [Almgren, Guth]. This requires hypotheses on \\(c\\), \\(\\tau\\), \\(\\psi\\), such that:\n\n\nthe cells \\(Z(y) \\hookrightarrow X\\) are closed cycles on \\(X\\), i.e. \\(Z(y)\\) has vanishing boundary;\n\n\nand the \\(Y\\)-parameter sweepout \\(y\\mapsto Z(y)\\) of \\(X\\) is continuous in the flat Almgren topology: namely if \\(y_0, y_1\\in Y\\) are sufficiently close, then \\(Z(y_0)\\), \\(Z(y_1)\\) bound a Lipschitz chain \\(C\\) of arbitrarily small area with \\(\\partial C= Z(y_1)-Z(y_0)\\).\n\n\nThe Almgren continuity follows from the \\(c\\)-concave potential \\(\\psi\\) and its \\(c\\)-convex transform \\(\\phi:=\\psi^c\\) being locally Lipschitz on its domain. In otherwords, the source potential \\(\\phi\\) has locally bounded gradient everywhere and this keeps the area of bounding chains small.\nN.B. Without \\(\\phi, \\psi\\) being locally Lipschitz, it would be difficult to control the cell in the limit \\(\\lim_{y\\to y_1} Z(y)\\).\n\n\nProblem of Constructing Costs on Dold-Thom Groups\nWe need emphasize that geometry in OT begins with the construction of geometric costs \\(c: X\\times Y \\to \\bf{R}\\). Applications with DT would require costs defined say by \\(c: X \\times A(S^q) \\to \\bf{R}\\), where \\(S^q\\) is the topological \\(q\\)-sphere and \\(A(S^q)\\) is the DT group on the sphere.\nConstructing continuous maps \\(S^d \\to A(S^q)\\) involves different ideas from the construction of continuous maps \\(S^d \\to S^q\\). It’s an interesting problem.\nNaive question: consider the Hopf mapping \\(S^3 \\to S^2\\). Is there a natural cost \\(c: S^3\\times S^2 \\to \\bf{R}\\) for which the Hopf map arises as \\(c\\)-optimal transport between the canonical measures on \\(S^3, S^2\\), respectively?\n\n\nTwo sweepouts on \\(S^2\\)\nTo illustrate the basic idea, we consider two different sweepouts of \\(S^2\\) into \\(1\\)-cycles. In both cases, we see an explosion of (+), (-) charges emerging from the equator.\nIn the first case, the (+), (-) charges accumulate at the north, south poles, respectively, and then travelling in a group annihilate each other along some longitudinal line.\nIn the second case, the (+), (-) charges emerge from the equator and pass through the north, south poles, respectively, and without stopping continue through the poles eventually annihilating again along the equator lines.\nWhich of these two sweepouts is canonical?\nIn our view, the second sweepout, wherein momentum is preserved, is the more canonical. But it’s another interesting question."
  },
  {
    "objectID": "posts/2023-03-10-DoldThom_Part2/index.html",
    "href": "posts/2023-03-10-DoldThom_Part2/index.html",
    "title": "Dold Thom Theorem. Configuration Spaces and Homology. (2/3)",
    "section": "",
    "text": "\\(\\newcommand{\\sub}{\\partial^c \\psi(y)}\\) \\(\\newcommand{\\del}{\\partial}\\)\nIn post we introduced the Dold-Thom group \\(A(X)\\) as the kernel of the augmentation map \\(\\epsilon\\), and the relative Dold-Thom theorems. Ultimately we look to construct topological sweepouts via OT.\nFor convenience we recall the basic definition. - (Definition Dold Thom Group \\(A(X)\\)) If \\(X\\) is topological space, and \\(G\\) is finitely-generated abelian group, then the Dold-Thom group is the kernel \\(A(X;G):=\\ker(\\epsilon)\\) of the augmentation map \\(\\epsilon: G(X) \\to G,\\) defined by \\(\\epsilon(\\sum' g_x .x)=\\sum' g_x\\).\nTherefore \\(A(X;G)\\) consists of all finitely-supported \\(G\\)-valued distributions on \\(X\\) with zero net sum. Here \\(\\emptyset\\) represents the constant \\(0\\)-valued distribution on \\(X\\). The vacuum state \\(\\emptyset\\) serves as canonical basepoint on \\(A(X)\\).\n\n(Dold Thom Theorem) The singular reduced homology functor \\(X\\mapsto \\tilde{H}_*(X;G)\\) is naturally equivalent to the functor of \\(\\emptyset\\)-pointed homotopy groups \\(X\\mapsto \\pi_*(A(X;G), \\emptyset)\\).\n\nFor applications we choose \\(G=\\mathbb{Z}\\) or \\(\\mathbb{Z}/2\\), and will suppress ‘\\(G\\)’ for the remainder of this article. As basic corollary we find:\n\n(Corollary) If \\(Y\\) is a Moore space, e.g. \\(Y=\\mathbb{S}^q\\) is a \\(q\\)-sphere, then \\(A(Y;G)\\) is a model of an Eilenberg-Maclane classifying space \\(K(G,q)\\).\n\n\nHow to Represent Homology Cycles: Dold-Thom and Steenrod’s Problem\nThus DT provides an apparently new source of classifying spaces for abelian coefficient groups \\(G\\). According to the obstruction methods of Steenrod, Eilenberg, Maclane, it follows that there is a natural equivalence between free homotopy classes \\[[X, A_0(Y;G)]\\] and singular cohomology groups \\[H^q(X;G)\\]. Thus Dold-Thom allows the construction of cohomology cycles via the construction of topologically nontrivial maps \\(X\\to A_0(Y;G)\\) whenever \\(H^q(X;G)\\neq 0\\). This leads us to a general topological problem:\n\n(Problem 1) Construct and classify homotopically nontrivial continuous maps \\[f: X \\to A_0(Y)\\] for given topological space \\(X\\) and \\(q\\)-sphere \\(Y=\\mathbb{S}^q\\) whenever \\(H^q(X)\\neq 0\\).\nE.g., If \\(X\\) is compact oriented without boundary, then \\(H^n(X)\\) is nontrivial cyclic group when \\(n=\\dim(X)\\). Can we construct explicit maps \\(f: X \\to A_0(S^n)\\) realizing the orientation class in \\(H^n\\)?\nIf \\(X=S_g\\) is a hyperbolic Riemann surface, can we construct maps \\(f: S_g \\to A_0(\\mathbb{S}^1)\\)?\n\nWe remark that maps \\(f\\) solving Problem 1 generate homological cycles on \\(X\\). The argument is described in R. Kirby’s book (Kirby 2006). If \\(f\\) is a continuous map solving Problem 1, then regular fibres \\(f^{-1}(pt)\\), where \\(pt\\) represents a distribution on \\(Y\\), are cycles in \\(X\\). These cycles are nontrivial whenever \\(f\\) is homotopically nontrivial, and even Poincare dual to the cocycle generated by \\(f\\) when \\(X\\) is compact oriented and \\(Y\\) is a Moore space). A similar idea is found in (Thom 1954).\nBut what are the applications and corollaries of DT?\nWe consider DT as providing a constructive method for building homology cycles, e.g. Steenrod’s problem [ref].\nLet \\((X,d,\\sigma)\\) be a source space, say an oriented complete compact Riemannian manifold with \\(\\sigma=vol_X\\). The topologist faces several questions:\n\nHow to generate nontrivial homological \\(k\\) cycles in \\(X\\)?\nHow to generate nontrivial \\(k\\) dimensional sweepouts of \\(X\\)?\n\nOur goal is to generate smooth maps from the regularity theory of optimal transport.\n\\(\\newcommand{\\sub}{\\partial^c \\psi(y)}\\) \\(\\newcommand{\\del}{\\partial}\\)\nIn post we introduced the Dold-Thom group \\(A(X)\\) as the kernel of the augmentation map \\(\\epsilon\\), and the relative Dold-Thom theorems. Ultimately we look to construct topological sweepouts via OT.\nFor convenience we recall the basic definition. - (Definition Dold Thom Group \\(A(X)\\)) If \\(X\\) is topological space, and \\(G\\) is finitely-generated abelian group, then the Dold-Thom group is the kernel \\(A(X;G):=\\ker(\\epsilon)\\) of the augmentation map \\(\\epsilon: G(X) \\to G,\\) defined by \\(\\epsilon(\\sum' g_x .x)=\\sum' g_x\\).\nTherefore \\(A(X;G)\\) consists of all finitely-supported \\(G\\)-valued distributions on \\(X\\) with zero net sum. Here \\(\\emptyset\\) represents the constant \\(0\\)-valued distribution on \\(X\\). The vacuum state \\(\\emptyset\\) serves as canonical basepoint on \\(A(X)\\).\n\n(Dold Thom Theorem) The singular reduced homology functor \\(X\\mapsto \\tilde{H}_*(X;G)\\) is naturally equivalent to the functor of \\(\\emptyset\\)-pointed homotopy groups \\(X\\mapsto \\pi_*(A(X;G), \\emptyset)\\).\n\nFor applications we choose \\(G=\\mathbb{Z}\\) or \\(\\mathbb{Z}/2\\), and will suppress ‘\\(G\\)’ for the remainder of this article. As basic corollary we find:\n\n(Corollary) If \\(Y\\) is a Moore space, e.g. \\(Y=\\mathbb{S}^q\\) is a \\(q\\)-sphere, then \\(A(Y;G)\\) is a model of an Eilenberg-Maclane classifying space \\(K(G,q)\\).\n\n\n\nHow to Represent Homology Cycles: Dold-Thom and Steenrod’s Problem\nThus DT provides an apparently new source of classifying spaces for abelian coefficient groups \\(G\\). According to the obstruction methods of Steenrod, Eilenberg, Maclane, it follows that there is a natural equivalence between free homotopy classes \\[[X, A_0(Y;G)]\\] and singular cohomology groups \\[H^q(X;G)\\]. Thus Dold-Thom allows the construction of cohomology cycles via the construction of topologically nontrivial maps \\(X\\to A_0(Y;G)\\) whenever \\(H^q(X;G)\\neq 0\\). This leads us to a general topological problem:\n\n(Problem 1) Construct and classify homotopically nontrivial continuous maps \\[f: X \\to A_0(Y)\\] for given topological space \\(X\\) and \\(q\\)-sphere \\(Y=\\mathbb{S}^q\\) whenever \\(H^q(X)\\neq 0\\).\nE.g., If \\(X\\) is compact oriented without boundary, then \\(H^n(X)\\) is nontrivial cyclic group when \\(n=\\dim(X)\\). Can we construct explicit maps \\(f: X \\to A_0(S^n)\\) realizing the orientation class in \\(H^n\\)?\nIf \\(X=S_g\\) is a hyperbolic Riemann surface, can we construct maps \\(f: S_g \\to A_0(\\mathbb{S}^1)\\)?\n\nWe remark that maps \\(f\\) solving Problem 1 generate homological cycles on \\(X\\). The argument is described in R. Kirby’s book (Kirby 2006). If \\(f\\) is a continuous map solving Problem 1, then regular fibres \\(f^{-1}(pt)\\), where \\(pt\\) represents a distribution on \\(Y\\), are cycles in \\(X\\). These cycles are nontrivial whenever \\(f\\) is homotopically nontrivial, and even Poincare dual to the cocycle generated by \\(f\\) when \\(X\\) is compact oriented and \\(Y\\) is a Moore space). A similar idea is found in (Thom 1954).\nBut what are the applications and corollaries of DT?\nWe consider DT as providing a constructive method for building homology cycles, e.g. Steenrod’s problem [ref].\nLet \\((X,d,\\sigma)\\) be a source space, say an oriented complete compact Riemannian manifold with \\(\\sigma=vol_X\\). The topologist faces several questions:\n\nHow to generate nontrivial homological \\(k\\) cycles in \\(X\\)?\nHow to generate nontrivial \\(k\\) dimensional sweepouts of \\(X\\)?\n\nOur goal is to generate smooth maps from the regularity theory of optimal transport.\n\n\nDold-Thom and Optimal Transport\nIn (Martel) and (Martel 2022) we introduced a method for applying optimal transportation to algebraic topology, where the main idea was that singularities of optimal transports are well-defined geometric subvarieties whose geometry is described by the differential geometry of the cost function \\(c\\). Formally given a source \\((X, \\sigma)\\), we introduce target spaces \\((Y, \\tau)\\) and costs \\(c: X\\times Y \\to \\bf{R}\\).\nWhen the cost \\(c\\) satisfies certain assumptions (A0123) we find optimal transport solutions and the solutions to Kantorovich’s dual program are unique and have sufficiently regular topology. In the dual program, the key definition is the \\(c\\)-subdifferential \\(\\sub \\hookrightarrow X\\) defined for \\(y\\in Y\\). Using the \\(c\\)-subdifferential we define Kantorovich’s contravariant functor \\[Z: 2^Y \\to 2^X\\] by \\[Z(Y_I)=\\cap_{y\\in Y_I}\\sub\\] for every closed subset \\(Y_I \\hookrightarrow Y\\).\nWe view \\(Z\\) as defining a \\((Y, \\tau)\\)-parameter family of cells \\(\\sub\\) in \\(X\\). The cells need not be disjoint. There are standard assumptions on the cost \\(c\\) such that almost every cell \\(Z(y)=\\sub\\) is an \\((n-k)\\)-dimensional submanifold of \\(X\\), and otherwise singular subvarieties. Our main idea is that every pair of target \\((Y,\\tau)\\) and cost \\(c\\) determines a unique \\(Y\\)-parameter family of subsets \\(Z(y)=\\sub\\) of \\(X\\).\nThese definitions lead to the following problem:\n\n(Problem 1) Find general conditions on \\(Y, \\tau, c\\) such that \\(c\\)-optimal transports \\(\\pi\\) generate continuous maps \\(f=f_\\pi: X \\to A_0(Y)\\), and such that \\(c\\)-optimal transports \\(\\pi\\) general nontrivial sweepouts of \\(X\\).\n(Problem 2) Find general conditions such that \\(y\\mapsto Z(y)\\) is continuous in the Almgren flat-chain topology.\n\nIn otherwords \\(Z(y)=\\sub\\) defines a \\(Y\\)-parameter family of subvarieties of \\(X\\). Regularity of the optimal transport corresponding to \\(Z=\\sub\\) controls the regularity of the sweepout. Problem 2 appears to be satisfied by the basic cost assumptions (A0)–(A4) in the notation used below.\nFor the purposes of generating sweepouts, the geometric homology spheres \\(S^d\\) form a natural class of target spaces. But given a source \\(X\\), can we readily generate costs \\(c: X\\times S^d \\to \\bf{R}\\)? Already the case of constructing interesting costs \\(S \\times S^1 \\to \\bf{R}\\), or better yet \\(S \\times A_0(S^1) \\to \\bf{R}\\), appears nontrivial problem.\n\\(\\newcommand{\\del}{\\partial}\\)\n\n\nRegularity and Sweepouts\nThe regularity of optimal semicouplings is studied in [McCann-Pass] and [McCann-Pass-Chiappori]. We do not enter into the details in this post. We look to construct interesting topological sweepouts of \\(X\\). The idea is that optimal semicouplings from \\(X\\) to \\(k\\)-dimensional target spaces \\((Y, \\tau)\\) with \\(k &lt; &lt; \\dim(X)\\) provide interesting sweepouts if the cost \\(c: X \\times Y \\to \\bf{R}\\) satisfies various conditions, to be labelled (A0) – (A3) below.\nThe key assumptions require the cost \\(c\\) to be \\(C^2\\) on \\(X\\times Y\\), proper, and satisfying a (Twist) condition. The assumptions imply every \\(c\\)-concave function \\(\\psi: Y\\to {\\bf{R}}\\cup\\{+\\infty\\}\\) is Lipschitz and semiconcave, and differentiable \\(\\tau\\)-almost everywhere on \\(Y\\). Therefore \\(dom(\\nabla_x \\phi)\\) is a full measure subset of the activated domain \\(A \\hookrightarrow\\). The active domain \\(A\\) is naturally defined as \\(dom(\\phi)\\) and with a closure equal to \\(dom(D\\phi)\\). Furthermore, \\(D^2\\psi\\) is defined \\(\\tau\\)-almost everywhere on \\(dom(D\\psi)\\).\nIf \\(\\psi\\) is a \\(c\\)-concave function on the target \\(Y\\) which solves the dual Kantorovich program for the \\(c\\)-optimal semicoupling from source to target, then we obtain the Kantorovich functor \\(Z: 2^Y \\to 2^X\\) as defined previously. For our study of sweepouts, it’s convenient to study semicoupling programs where \\(Z\\) is supported on the singletons \\(Y \\hookrightarrow 2^Y\\). This allows us to represent \\(Z\\) as a function \\(Z: Y \\to 2^X\\), with \\(y\\mapsto Z(y)=\\del^c \\psi(y)\\). Thus our study of sweepouts is complementary to our study of singularities. The regularity of the above map \\(y\\mapsto Z(y)\\) is an important question. Under mild assumptions (given below) we easily find \\(Z(y)\\) are closed subsets of the source for every \\(y\\in Y\\). Moreover an application of Sard’s theorem implies \\(Z(y)\\) are smooth submanifolds for almost every \\(y\\in Y\\). For application to sweepouts, we do not expect all \\(Z(y)\\) to be smooth, for indeed there must exist some singular values. However we do seek sweepouts where \\(Z(y)\\) varies continuously with respect to the \\(y\\) parameter. In the language of sweepouts [Guth], we find it necessary to establish the following elementary result.\n\n(Almgren Continuity) If \\(y_0, y_1\\) are sufficiently close in \\(Y\\), then the \\((n-k)\\)-cycles \\(Z(y_0)\\) and \\(Z(y_1)\\) bound a Lipschitz \\((n-k+1)\\)-chain of small area, i.e. there exists a chain \\(C\\) such that \\(\\del C = Z(y_1)-Z(y_0)\\) and \\(C\\) has arbitrarily small area.\n\nWe assume that \\(Z\\) is Almgren continuous when the cost satisfies assumptions (A0123). We reason that the functions are proper Lipschitz and the gradients, and therefore the unit normal areas are bounded from above, implying that the bounding surface is not too large. [Insert details]\n\n\nTechnical Assumptions on the Cost\nTo begin the study of optimal transport from source \\((X,\\sigma)\\) to targets \\((Y,\\tau)\\), we require a choice of cost function \\(c\\) which satisfies some geometric assumptions.\n\n[(A0)] The cost \\(c\\) is twice-continuously differentiable throughout its domain, jointly in the source and target variables \\((x,y)\\), and nonnegative, and proper. Thus \\(c(x,y)\\geq 0\\) and all proper closed sublevels are compact.\n[(A1)] For every \\(y\\in Y\\), we assume \\(x\\mapsto c(x,y)\\) is nonconstant on every open subset of \\(X\\).\n[(A2)] The cost satisfies (Twist) condition with respect to the source variable throughout \\(dom(c)\\): for every \\(x'\\in X\\) the rule \\(y\\mapsto \\nabla_x c(x',y)\\) defines an injective mapping \\(dom(c_{x'}) \\to T_{x'} X\\).\n\nIt is extremely useful to allow the cost \\(c\\) to have poles, for example \\(c(x,y)=+\\infty\\) whenever \\(x\\in Y\\subset X\\).\nThe (Twist) condition (A2) is properly understood through Kantorovich duality, as we will discuss below. It essentially guarantees the uniqueness a.e. of \\(c\\)-optimal transports. It moreover gives important equation describing the fibres of the optimal transport mapping.\nNow suppose we have a source \\(\\sigma\\), target \\(\\tau\\), and cost \\(c\\) satisfying the assumptions (A012). If the \\(c\\)-optimal transport is regular and continuously single-valued, then the functor \\(Z\\) reduces to a map \\(Y\\to 2^X\\) defined by \\(Z(y)=\\sub\\). As we vary \\(y\\) over the target \\((Y, \\tau)\\) we obtain \\(Y\\)-parameter family of closed subsets \\(Z(y)\\) on \\(X\\). Now we need assumptions on the cost to guarantee the cells \\(Z(y)\\) are closed \\((n-k)\\)-dimensional Lipschitz subvarieties of \\(X\\), and even smooth \\((n-k)\\)-submanifolds for a.e. every \\(y\\in Y\\). The Assumptions (A012) do not imply the cells \\(Z(y)\\) have empty boundary. This requires a further hypothesis depending on the \\(c\\)-convex potential, namely:\n\n[(A3)] If \\(\\phi\\) is a \\(c\\)-convex potential on \\(X\\), then for every \\(y\\in Y\\), we require the gradient \\(\\nabla_x (c(x,y)+\\phi(x))\\) be uniformly bounded away from zero for all \\(x\\in \\sub\\).\n\nAgain these are technical conditions which we do not expect the reader to appreciate at first glance. However they arise naturally from the analysis. For example, (A3) is simply a strong form of Clarke’s nonsmooth implicit function theorem.\nSo where do we go from here?\nTo summarize:\n\nWe want to construct sweepouts from optimal transport, and specifically from optimal semicouplings. Given a source \\((X, \\sigma)\\), we require targets \\((Y, \\tau)\\) and costs \\(c\\) on \\(X\\times Y\\) under various conditions. Generally \\(\\dim(Y) &lt;&lt; \\dim(X)\\). Here the assumptions (A0)–(A2) are relevant.\nRegularity of the sweepouts is attained via regularity of the \\(c\\)-subdifferentials \\(Z(y)=\\sub\\) where \\(\\psi=\\psi^{cc}\\) is the \\(c\\)-concave potential solving Kantorovich’s dual max program. Here the assumption (A3) enters, which is dependant on the gradient of \\(\\psi\\).\n\nThere are various problems which one might consider.\n\n\nPreliminary Remarks on Gromov Guth Width Inequality\nWe conclude this post with a question relating to Gromov-Guth’s width inequalities, and a certain line of enquiry which we could not complete. The previous section introduced the possibility of constructing topological sweepouts via solutions of OT programs. Now we study the possibility of representing minimal sweepouts by such solutions motivated by Guth-Gromov’s width inequality (Guth 2007) and (Guth 2009).\n\nIf \\(X=(X,g)\\) is a closed \\(n\\)-dimensional Riemannian manifold, then there exists a universal constant \\(C(n)\\) depending only on the dimension such that \\(width_k(X,g)^{1/k}\\leq C(n) vol(X,g)^{1/n}.\\)\n\nRecall that the \\(k\\)-width of \\((X,g)\\) is defined by a min-max problem, namely \\[width_k(X,g):=\\min_{\\{z_t\\}} \\max_{t} vol_k(z_t),\\] where the minimum ranges over all \\(k\\)-parameter sweepouts \\(z\\) of \\(X\\), and the volume is with respect to the metric \\(g\\). Estimates on \\(width_k\\) imply every \\(k\\)-sweepout of \\(X\\) contains at least one cycle of large volume.\nOur goal is to interpret \\(width_k\\) in terms of optimal transportation. As we described above, the \\(c\\)-optimal transport from source \\(X\\) to target \\(Y\\) yields an \\((n-k)\\)-dimensional sweepout \\(y\\mapsto Z(y)\\). This assuming the cost \\(c\\) satisfies Assumptions (A0123). A priori it is possible, even likely, that not every \\((n-k)\\)-sweepout \\(\\{z_t\\}\\) is representable by a \\(c\\)-optimal transport \\(Z(y)\\). All the same, the optimal transport datum yields much more geometric information on the sweepout. Let \\(\\sigma, \\tau, c\\) be as above, with \\(Z(y):=\\sub\\). The assumptions (A0123) imply the existence of a measurable map \\(T: X\\to Y\\) defined \\(\\sigma\\)-a.e. satisfying \\(T\\# \\sigma = \\tau\\), and such that \\[g(y)=\\int_{T^{-1}(y)} \\frac{1}{|JT|} f(x) d\\mathscr{H}^{n-k}(x) \\] for \\(\\tau\\)-a.e. \\(y\\in Y\\). Here \\(|JT|\\) denotes Jacobian of \\(T\\) defined by \\(|JT|=\\sqrt{DT . {}^t DT}\\). Recall the Cauchy-Binet formula which expresses \\(JF\\) as the euclidean norm of all \\(k\\times k\\)-square subdeterminants of \\(DF\\).\nTo compare \\(g(y)\\) with the volume of the fibres, we write \\[vol_{n-k}[T^{-1}(y)]=vol_{n-k} [\\sub]=\\int_{T^{-1}(y)} 1 . f(x) d\\mathscr{H}^{n-k}(x).\\] If the Jacobian \\(JT\\) has constant magnitude along the fibre \\(T^{-1}(y)\\), then we can immediately compare the density \\(g(y)=\\frac{d\\tau}{d\\mathscr{H}^{k}}(y)\\) with the \\((n-k)\\)-volume of the fibre \\(T^{-1}(y)\\), namely \\(g(y) . |JT| = vol[T^{-1}(y)]\\). But in general, we cannot expect \\(|JT|\\) to be constant along fibres. Furthermore the width of the sweepout depends on the fibre of maximal volume. So the question arises: how can we use OT to identify the maximal fibres and their respective volumes? How can we generate the extremal width volume inequalities using OT?\n\n(Problem:) Under the above hypotheses, determine method such that the volume of the fibres \\(T^{-1}(y)=\\sub\\) can be estimated and bounded by \\(g(y)\\). That is, approximate the \\((n-k)\\)-width of the sweepout \\(y\\mapsto Z(y)\\) in terms of the optimal transport data \\(\\sigma\\), \\(\\tau\\), \\(c\\), \\(\\psi^{cc}=\\psi\\).\n\n\n\nConclusion\nThis post has more questions than answers. But we trying to indicate something of how OT applies to geometric topology. Naturally we continue studying the Kantorovich functor \\(Z\\). Here we seek basic regularity in \\(Z(y)=\\sub\\) that we might obtain regular sweepouts.\n-JHM.\n\n\n\n\n\nReferences\n\nGuth, Larry. 2007. “The Width-Volume Inequality.” Geometric and Functional Analysis 17 (4): 1139–79.\n\n\n———. 2009. “Minimax Problems Related to Cup Powers and Steenrod Squares.” Geometric And Functional Analysis 18 (6): 1917–87.\n\n\nKirby, Robion C. 2006. The Topology of 4-Manifolds. Vol. 1374. Springer.\n\n\nMartel, J. H. “Applications of Optimal Transport to Algebraic Topology: How to Build Spines from Singularity.” PhD thesis, University of Toronto. https://github.com/jhmartel/Thesis2019.\n\n\n———. 2022. “Topology of Singularities of Optimal Semicouplings.” arXiv Preprint arXiv:2201.12817.\n\n\nThom, René. 1954. “Quelques Propriétés Globales Des Variétés Différentiables.” Commentarii Mathematici Helvetici 28 (1): 17–86."
  },
  {
    "objectID": "posts/2023-03-08-FizeauSansbury/index.html",
    "href": "posts/2023-03-08-FizeauSansbury/index.html",
    "title": "Modified Fizeau Sansbury Spinning Wheel Experiment",
    "section": "",
    "text": "In this post we discuss our modified Fizeau-Sansbury spinning sawtooth experiment. To begin we review Fizeau’s original 1849 experiment and introduce our modification which allows for testing Ralph Sansbury’s apparatus as outlined in (Sansbury, n.d., 2012). While Fizeau studies a saw tooth wheel in uniform motion on the order of twenty rotations per second, Sansbury’s setup can be interpreted as either the same wheel in nonuniform motion, or a wheel with variable saw tooth length in uniform motion. We argue that Sansbury’s predictions could be tested if Fizeau’s wheel could increase it’s angular velocity from 20 revolutions per second, to 80 revolutions or 160 revolutions per second. This is not easily realized.\nAs described in previous posts, our critical essay on foundations in SR is basically stable and readable. Recently this document has been rewritten into four posts, of which this post is the fifth post, which is centred on Sansbury’s experiment.\nSansbury’s experiment is this. We take \\(c\\) the speed of light as equal to \\(1\\) foot per nanosecond \\([\\mu s]=10^{-8}\\) seconds.\n\nSansbury’s Experiment\nWe quote from (Sansbury, n.d.) and his book (Sansbury 2012).\n(Case 1) A \\(15\\) nanosecond light pulse from a laser was sent to a light detector, \\(30\\) feet away. When the light pulse was blocked at the photodiode during the time of emission, but unblocked at the expected time of arrival, \\(31.2\\) nanoseconds after the beginning of the time of emission, for \\(15\\) nanosecond duration, little light was received. (A little more than the \\(4mV\\) noise on the oscilloscope). This process was repeated thousands of times per second.\n(Case 2) When the light was unblocked at the photodiode during the time of emission (\\(15\\) nanoseconds) but blocked after the beginning of the time of emission, during the expected time of arrival for \\(15\\) nanoseconds, twice as much light was received (\\(8mV\\)). This process was repeated thousands of times per second.\nSansbury’s conclusion? That this indicated that light is not a moving wave or photon, but rather the cumulative effect of instantaneous forces at a distance. That is, undetectable oscillations of charge can occur in the atomic nuclei of the photodiode that spill over as detectable oscillations of electrons after a delay.\nSansbury found the equipment necessary for the experiment too expensive to rent for an extended period of time, and he was possibly not a sufficient expert in calibrating the equipment. So Sansbury’s experiment appears to have not been sufficiently investigated, and we would argue that the experiment has neither been reproduced nor properly reviewed. Thus we turn to the classical Fizeau experiment, and consider its similarities to Sansbury’s setup.\n\n\nFizeau’s Saw Tooth Experiment (1849)\nNow Sansbury’s apparatus has some similarity with Fizeau’s sawtooth apparatus, as used circa 1849 to prove some of the first sensible measurements of luminal velocity.\nHere is a blog which inlcudes a transcription of Fizeau’s original 1849 paper in Comptes Rendus.\nHere is an interesting youtube video en francais sur la mesure de Fizeau. Around the 4-5 minute mark is the most interesting. While the speed of the wheel is increased, the received light signal becomes increasingly erratic and intermittent, until a sufficiently high speed of rotation is achieved and the received light signal becomes eventually null and no light is received.\nHere is a useful physics stackexchange answer.\n\n\nSansbury vs. Fizeau\nNow Fizeau’s original setup is a type of Sansbury test where the wheel is in uniform motion. Sansbury’s setup involves either a wheel in nonuniform motion, or equivalently a wheel in uniform motion with a nonuniform sawtooth distribution.\nWe remark that Fizeau was historically looking to estimate the luminal velocity \\(c\\). Sansbury’s experiment assumes \\(c\\) as given, estimated at \\(1\\) foot per nanosecond. Sansbury’s goal is to distinguish light propagation from particle model, and his setup is meant to test whether light is even something that travels at all.\nLet’s review the basic math of Fizeau’s apparatus, it’s very simple. No matter how we setup the mirrors, we suppose light has some total travel time. In Fizeau’s original setup, light travelled a total path of \\(\\approx 16 km\\). With an expected speed of light \\(c=3\\times 10^8 km\\), then the expected travel time is\n\\[\\begin{equation}\n\\frac{16 \\times 10^3 [m]}{3 \\times 10^8 [m]/[sec]}=5.333\\ldots \\times 10^{-5} [sec].\\end{equation}\\]\nNow consider the wheel with angular velocity \\(\\omega\\) having units of \\([degrees]/[sec]\\) and having a toothlength equal to \\(1/720\\) degrees. The time required to turn one toothlength is therefore \\(\\frac{1/720}{\\omega}=\\frac{1}{720 \\omega} [sec]\\). Thus we find that the expected travel time is equal to the time to rotate one toothlength if the following equality holds \\[\\frac{16 \\times 10^3 }{3 \\times 10^8} = \\frac{1}{720 \\omega}, \\] which implies \\[\\omega \\approx 26 ~~~\\frac{[rotations]}{[sec]}.\\]\nSansbury’s (Case 1) could be realized if the wheel was allowed to rotate nonuniformly, i.e. if the wheel could be accelerated in “impulses” something closer to the actual discrete motions of a clock. For example, if the wheel is initially opened at the time of emission, then immediately rotated one saw tooth length (to the closed position) during the expected time of travel, and just prior to the expected time of arrival is rotated another tooth length (to the open position), then Fizeau would predict that the receiver would observe a strong light source. However Sansbury predicts that the receiver would observe rather a very weak signal. Notice here we require the wheel to move twice as fast as Fizeau’s angular velocity. In otherwords the wheel must rotate two complete tooth lengths before the estimated arrival time.\nSansbury’s (Case 2) could be realized if the wheel was rotating nonuniformly. For example, if we keep the wheel fixed during the expected time of flight of the light particle, and turn the wheel one complete toothlength at the expected time of arrival, then Sansbury would predict a relatively strong signal would be received. Fizeau and the particle model would however predict no light would be received, since in the model it would be blocked by the sawtooth at the expected time of arrival.\nN.B. Fizeau’s conception of the sawtooth wheel is classical. But what happens if we retrospectively apply the SR methodology to the experiment, what results are obtained? It appears that SR has a null effect on the entire experiment, i.e. returns the same results as the classical case. While the sawteeth lengths are contracting in SR, this effects the circumference of the wheel but not the angular velocity. Thus Fizeau experiment appears insensitive to any Lorentz SR effects and an experiment which cannot prove SR in contrast to the classical mechanics.\nWe do not require mathematics at this stage, but rather to perform an experiment. However there is an intersting math aspect to the question, “How to keep a nonuniform wheel in uniform motion?”.\nFizeau’s original wheel was materially balanced: the distribution of teeth was equidistant and regular. The centre of mass corresponded with the axis of rotation.\nBut if we begin to study nonuniform wheels, then the behaviour becomes more difficult depending on, say, whether the centre of mass coincides with the axis of rotation.\nSome comparisons between Fizeau and Sansbury:\n\nFizeau’s involves several reflecting mirrors (beam splitters). Therefore there is more interaction involved in Fizeau’s setup than with Sansbury’s. For Fizeau’s is a type of two-way trip of light, where the source and receiver are space-coincident. But Sansbury’s is a one-way trip, requiring some electronics at the receiver namely a photodiode, to measure the amount of electrons released by the light emission.\nIf the phase of Fizeau’s wheel could be controlled, then we could compare the behaviour of the experiment when the wheels differ by one saw tooth length. The trouble in Fizeau is that, because the source and receiver coincide, it’s evident that no light is emitted when the phase is shifted one tooth length. Sansbury’s experiment however does emphatically require the apparatus to be open at the moment of emission.\n\n\n\nFaster Fizeau Wheels\nWe could test some of Sansbury’s ideas if we could increase the angular velocity of the Fizeau wheel by factor of \\(4\\), i.e. we need a wheel of roughly \\(100\\) revolutions per second instead of \\(20\\) revolutions per second.\nGiven such a revolution speed, then we could change the sawtooth pattern of the wheels, having some that are \\(1/4\\) closed, \\(1/2\\) closed, and \\(3/4\\) closed wheels. For example, we could have the alternating sawtooth \\[\\ldots 0101010101 \\ldots\\] or we could have \\[\\ldots 001100110011 \\ldots\\] both of which are \\(1/2\\) closed but having different patterns. And these patterns would have different predictions depending on the photon model or Sansbury’s cumulative action-at-a-distance. Likewise it would be interesting to compare the predictions given a wheel having a \\(1/4\\)-closed sawtooth pattern \\[\\ldots 0001000100010001 \\ldots\\] versus a \\(3/4\\)-closed pattern \\[ \\ldots 0111011101110111\\ldots.\\]\nIf we could get the Fizeau wheel to spin \\(200\\) revolutions per second, then we could test the theories according to \\(8\\)-periodic patterns, i.e. with sawtooth patterns being \\(1/8, 2/8\\), \\(\\ldots\\), \\(7/8\\)ths closed.\nIf we could build a larger wheel with more teeth, say, \\(1440\\) teeth, then \\(2880\\) teeth, then basic gear ratio would increase the speed of the initial pinion wheel by factor of \\(\\times 2\\), \\(\\times 4\\), etc..\nThe heuristics by which we can determine reasonable revolutions per second depends probably on some energy estimates and would require smaller and smaller radii.\nTo be continued. -JHM\n\n\n\n\n\nReferences\n\nSansbury, Ralph. 2012. The Speed of Light: Cumulative Instantaneous Forces at a Distance.\n\n\n———. n.d. “Light Speed Measurements from Roemer and Bradley to the GPS System.” http://www.naturalphilosophy.org/pdf/abstracts/abstracts_5955.pdf."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "jhm labs posts on various topics in mathematics, physics, and applications. Our special interest is electrodynamics via Ampere, Weber, Sansbury, et. al., planetary atomic models. Otherwise we work on geometry, topology, optimal transport, etc…"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "jhm labs",
    "section": "",
    "text": "Economics of Moving and Delivery.\n\n\n\n\n\n\n\nmoving\n\n\n\n\nHow to save money on moving and delivery? Basically prepare and pack as much as possible into as many closed cardboard boxes as necessary, and order a large truck for transport. We also discuss the basic questions which are most useful for estimating the total cost of moving a household. This manages the client’s expectations, but also allows the moving manager to maximize their profit and schedule themselves accordingly\n\n\n\n\n\n\nJan 4, 2024\n\n\nJHM\n\n\n\n\n\n\n  \n\n\n\n\nComputational Complexity of Fibonacci Sequences.\n\n\n\n\n\n\n\ncomplexity\n\n\n\n\nWe review the complexity of Fibonacci’s sequence \\(1, 1, 2, 3, 5, 8, 13,\\) etc., and its relation to S. Wolfram’s informal definition of computational irreducibility. We consider whether topological irreducibility has analogy to computation, and this is somewhat speculative, as we are looking for strategies to prove that \\(O(log_2~N)\\) is the minimal complexity of computing the \\(N\\)th Fibonacci element \\(f(N)\\).\n\n\n\n\n\n\nJan 4, 2024\n\n\nJHM\n\n\n\n\n\n\n  \n\n\n\n\nSpine for Teichmueller Space of Closed Hyperbolic Surfaces\n\n\n\n\n\n\n\nTeichmueller\n\n\nhyperbolic\n\n\n\n\nWe provide a self-contained construction of a minimal dimension equivariant spine for Teichmueller space of hyperbolic surfaces. The spine is essentially Thurston’s, and the result is becoming well known in the literature, and here we present our own proof.\n\n\n\n\n\n\nJan 3, 2024\n\n\nJHM\n\n\n\n\n\n\n  \n\n\n\n\nWeber Hamiltonian Gibbs Liouville Measure on State Space.\n\n\n\n\n\n\n\nweber\n\n\natom\n\n\n\n\nWe examine the Hamilton equations for Weber’s potential in an isolated two-body system. We construct an invariant measure on the xv state space which is distinct from the conventional Gibbs Liouville measure. The invariant measure is necessary background measure from which we can define entropy of distributions.\n\n\n\n\n\n\nJan 3, 2024\n\n\nJHM\n\n\n\n\n\n\n  \n\n\n\n\nOn Yao’s Millionaire Problem. Part 1.\n\n\n\n\n\n\n\nsecret\n\n\n\n\nWe begin the study of Yao’s Millionaire Problem, approaching via convex analysis. Two players have secret points, and the first player to guess an affine function separating the secrets wins. The question is whether the optimal strategy is uniform on the domain, or whether there is some variation in the density. This is elaborated below using both python and some elementary convex analysis\n\n\n\n\n\n\nJan 3, 2024\n\n\nJHM\n\n\n\n\n\n\n  \n\n\n\n\nBookstores and Image to ISBN Problem\n\n\n\n\n\n\n\nbookstore\n\n\nimage_to_isbn\n\n\n\n\nPeople don’t buy what they don’t see. Take pictures of bookshelves, get an ordered index of ISBN numbers. Get books seen and sold.\n\n\n\n\n\n\nJan 2, 2024\n\n\nJHM\n\n\n\n\n\n\n  \n\n\n\n\nMathematical Review of Faraday Cages. A Gap In the Literature.\n\n\n\n\n\n\n\nFaraday\n\n\nharmonic functions\n\n\n\n\nPreliminary article. We review the basic properties of Faraday cages and a curious absence of any mathematical proof of their supposed properties from the Maxwell field equations. This leads to some basic questions.\n\n\n\n\n\n\nJan 2, 2024\n\n\nJHM\n\n\n\n\n\n\n  \n\n\n\n\nMeasuring Tapes and Physics\n\n\n\n\n\n\n\nmeasuring tape\n\n\n\n\nWe present our preliminary thoughts on a new digital measuring tape. This is our answer to the question of dude would you hold my tape.\n\n\n\n\n\n\nJan 2, 2024\n\n\nJHM\n\n\n\n\n\n\n  \n\n\n\n\nMathematics of Vintage Selling\n\n\n\n\n\n\n\nvintage\n\n\nrisk\n\n\n\n\nThis is our introductory pitch on math and vintage selling.\n\n\n\n\n\n\nJan 2, 2024\n\n\nJHM\n\n\n\n\n\n\n  \n\n\n\n\nWeber, Work, Energy\n\n\n\n\n\n\n\nWeber\n\n\nenergy\n\n\nwork\n\n\n\n\n\n\n\n\n\n\n\nJan 2, 2024\n\n\nJHM\n\n\n\n\n\n\n  \n\n\n\n\nDold Thom Theorem. Configuration Spaces and Homology. (3/3)\n\n\n\n\n\n\n\nDold Thom\n\n\nhomology\n\n\nsweepouts\n\n\nOT\n\n\n\n\n\n\n\n\n\n\n\nMar 13, 2023\n\n\nJHM\n\n\n\n\n\n\n  \n\n\n\n\nEinstein’s Field Equations, Tensors, and the Point Coincidence Argument.\n\n\n\n\n\n\n\nGR\n\n\nEinstein\n\n\nPCA\n\n\ntensors\n\n\n\n\nWe review Einstein’s Point Coincidence Argument, and the motivation for GR’s tensor vacuum field equation Ric - R.g=0 and the proposed field equation Ric-R g = κ T where κ T is the so called energy momentum pseudotensor.\n\n\n\n\n\n\nMar 11, 2023\n\n\nJHM\n\n\n\n\n\n\n  \n\n\n\n\nDold Thom Theorem. Configuration Spaces and Homology. (1/3)\n\n\n\n\n\n\n\nDold Thom\n\n\nhomology\n\n\n\n\nWe begin our discussion of the greatest theorem you’ve never heard of, namely Dold-Thom’s Theorem. We will present a general relative version of Dold-Thom which leads for example, to the canonical isomorphism between the long exact sequence in homotopy with the long exact sequence in relative homology. The ideas behind DT lead to many interesting questions in optimal transport, which this margin is too small to contain.\n\n\n\n\n\n\nMar 10, 2023\n\n\nJHM\n\n\n\n\n\n\n  \n\n\n\n\nDold Thom Theorem. Configuration Spaces and Homology. (2/3)\n\n\n\n\n\n\n\nDold Thom\n\n\nhomology\n\n\n\n\nWe continue our discussion of Dold-Thom’s Theorem (DT). In this post we look to develop the connections between DT and optimal transportation (OT).\n\n\n\n\n\n\nMar 10, 2023\n\n\nJHM\n\n\n\n\n\n\n  \n\n\n\n\nModified Fizeau Sansbury Spinning Wheel Experiment\n\n\n\n\n\n\n\nSR\n\n\nc\n\n\nlight\n\n\nfizeau\n\n\nsansbury\n\n\n\n\nWe propose a modified Fizeau-Sansbury spinning sawtooth experiment to answer the outstanding question: is light even something that travels in vacuum at all?\n\n\n\n\n\n\nMar 9, 2023\n\n\nJHM\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-03-09-DoldThom_Part1/index.html",
    "href": "posts/2023-03-09-DoldThom_Part1/index.html",
    "title": "Dold Thom Theorem. Configuration Spaces and Homology. (1/3)",
    "section": "",
    "text": "Dold-Thom theorem provides an important representation result for constructing singular homology. The essence is to replace singular homology on \\(X\\) with spherical maps on a configuration space \\(A(X)\\). One of the challenges in formulating DT is discovering the proper configuration space based on \\(X\\).\n\nConfiguration Spaces\nVarious configuration spaces are defined in the literature. The configurations typically have a physical interpretation based on hard disks, water droplets, electric charges, etc.. The basic models are:\n\n\nHard disks (where collisions of point particles is prohibited);\n\n\nHard disks with basepoint (where collisions are prohibited except with an annihilating basepoint);\n\n\nWater droplets (c.f. Gromov’s category of finite probability spaces [ref], where collisions are additive);\n\n\nElectrostatic droplets (e.g. signed Borel-Radon measures on \\(X\\), possibly with indivisible quanta, e.g. Millikan’s electron).\n\n\nIn example (1) the configuration space forms a monoid without identity, and the disks are not allowed to collide or intersect or join, just as hard disks cannot be joined together.\nIn example (2), a basepoint \\(pt\\) can be chosen on \\(X\\) and this basepoint serves as zero element, thus rendering the monoid into an additive topological group. However, as we commented above, a choice of basepoint is noncanonical when \\(\\pi_1\\neq 0\\).\nThe formal construction of earlier configuration spaces in the literature involves symmetric products with the diagonal removed, e.g. limits \\(\\lim_{N\\to +\\infty} (X^N - \\Delta)/Symm_N\\). Here \\(Symm_N\\) denotes the symmetric group on \\(N\\) letters. Example (3) arises from Gromov’s category of finite probability spaces, where the objects are so-called reductions \\(f: \\mu \\to \\nu\\) between finite probability spaces \\(\\mu, \\nu\\). See (Gromov 2014, 2012).\nThe category of (4) consists of finite electroneutral configurations, where the objects are again reductions \\(f:\\mu \\to \\nu\\). The proper formalization of (4) into a topological abelian group is provided below in the definition of \\(A_0(X)\\).\n\n\nBasepoints\nThe difficulty with the standard configuration spaces is their choice of basepoint. The usual models are based on the category of pointed topological spaces, from which we define the pointed fundamental group. But homology and reduced homology are essentially basepoint-free. This article was motivated by the author’s desiring a basepoint-free version of DT’s theorem for which the long exact sequence of relative homology is naturally isomorphic to the long exact sequence in homotopy (see DT theorem below).\n\n\nDold-Thom’s Topological Group \\(A(X)\\)\nWe introduce the basic definitions:\n\n(Def. 1) Let \\(X\\) be a topological space and \\(G\\) a topological group, e.g. \\(G=\\bf{Z}\\). Let \\(G(X)\\) be the group of of finitely-supported \\(G\\)-valued distributions on \\(X\\), i.e. \\[G(X):=\\{ \\sum n_x .x ~|~\\text{only finitely many nonzero~} n_x \\in G\\}.\\]\nLet \\(\\epsilon_X: G(X) \\to G\\) defined by \\(\\epsilon_X(\\sum n_x x)= \\sum n_x\\) be the canonical augmentation map.\nLet \\(A=A(X)=A_0(X):=\\ker(\\epsilon_X)\\) be the kernel of the augmentation map.\n\nWe make some remarks:\n\nIf \\(G\\) is an abelian group, we define \\(G(X)\\) and \\(A_0(X;G)\\) in the obvious way. However the cases of \\(G=\\bf{Z}\\) and \\(G=\\bf{Z} /2\\bf{Z}\\) appear to be cases of most interest, and henceforth we suppress the “\\(G\\)” from notation.\nMorever because \\(\\bf{Z}\\) is discrete the point charges (+), (-) are not infinitely divisible. This fails to be true if we replace \\(\\bf{Z}\\)-valued distributions with \\(\\bf{R}\\)-valued distributions, i.e. there exists unit charge quanta if and only \\(G\\) is discrete.\nWe observe that \\(G(X)\\) is a topological abelian group, with zero element \\(\\emptyset:=\\sum 0 x=0\\) corresponding to the zero distribution, and whose zero element is essentially the zero element \\(0\\) of \\(G\\). One might consider \\(\\emptyset\\) the “vacuum state on \\(X\\)”. It follows that \\(A_0(X)\\) is a topological subgroup, consisting of all distributions with zero net charge.\nOne naturally sees \\(A_0\\) as a discrete version of all signed Borel-Radon measures \\(\\mu\\) on \\(X\\) which integrate to zero \\(\\int_X 1. d\\mu(x)=0\\), i.e. which are orthogonal to all constant functions on \\(X\\).\nThe topology on \\(A_0(X)\\) implies recombinations of \\(\\emptyset\\) satisfy local conservation of charge. Thus point charges (+) and (-) must be born from the same spatial position on \\(X\\). Therefore the recombinations require the point charges to continuously move on \\(X\\), and without “teleportation”. This is analogous to the distinction between the standard \\(L^1\\) and \\(L^2\\) optimal transport.\n\nThe essence of DT theorems is to represent homology groups in terms of homotopy groups on configuration spaces. In otherwords to identify the homology functors as homotopy functors. The basic idea of this article is that the vacuum state \\(\\emptyset\\) serves as a type of “canonical basepoint on \\(X\\)”, and this enables a natural equivalence between the long exact sequences of relative homology and homotopy.\n\n\nRelative Dold-Thom Group\nIt’s important to establish the relative version of DT. Let \\(Y\\) be a closed subset of \\(X\\). Our goal is to define a relative configuration space \\(A_0(X/Y)\\), and the key identification will be the canonical isomorphism \\[A_0(X/Y)=A_0(X)/A_0(Y).\\] In terms of net zero charged particle configurations, the idea is to view \\(Y\\) as a where excess charges can “ground out”. That is, spheres in the relative DT group \\(A_0(X/Y)\\) basepointed at \\(\\emptyset\\) are recombinations which either neutralize away from \\(Y\\), or neutralize at \\(Y\\).\nHere is the formal definition. If \\(Y\\) is a closed subset of \\(X\\), then \\(G(Y)\\) and \\(A_0(Y)\\) is a closed subgroup of \\(G(X)\\) and there is a canonical quotient \\(G(X) \\to G(X)/A_0(Y).\\) Morever the augmentation map \\(\\epsilon_X\\) canonically descends to the quotient as a type of augmentation map \\[\\epsilon_{ X / Y}: G(X)/A_0(Y) \\to G, \\] and we identify \\[A_0(X/Y):=\\ker(\\epsilon_{X/Y}).\\]\n\n\nDold-Thom’s Theorem\nWe follow the original approach of (Dold and Thom 1958), which theorems are stated in terms of natural quasifibrations between the configuration spaces.\nRecall the definition of quasifibration:\n\n(Def. 2) A continuous map \\(f: X\\to Y\\) between topological spaces \\(X, Y\\) is a quasifibration if the canonical inclusion of fibres \\(f^{-1}(y)\\) into the homotopy fibre of \\(f\\) is a weak homotopy equivalence for every \\(y\\in Y\\).\n\nHere is the main statement of the Relative Dold-Thom Theorem:\n\n(Dold Thom Theorem) Let \\(Y\\) be closed subspace of \\(X\\). Then the short exact sequence of topological abelian groups \\[0\\to A_0(Y) \\to A_0(X) \\to A_0(X /Y) \\to 0,\\] is a quasifibration inducing a long exact sequence of \\(\\emptyset\\)-pointed homotopy groups \\[\\cdots \\to \\pi_{*+1}(A_0(Y), \\emptyset) \\to \\pi_{*+1}(A_0(X), \\emptyset) \\to \\pi_{*+1}(A_0(X/Y), \\emptyset) \\to \\pi_{*}(A_0(Y), \\emptyset) \\to \\cdots\\] which is naturally equivalent to the long-exact sequence of relative homology groups \\[\\cdots \\to \\tilde{H}_{*+1}(Y) \\to \\tilde{H}_{*+1}(X) \\to H_{*+1}(X,Y) \\to \\tilde{H}_*(Y) \\to \\cdots .\\]\n\n(Sketch of proof:) Following the original argument of Dold-Thom, the theorem reduces to verifying that the functors \\(X\\mapsto \\pi_* A_0(X)\\) satisfy the Eilenberg–Steenrod axioms. By a standard argument it follows that the functors \\(\\pi_* A_0\\) and \\(\\tilde{H}_*\\) are naturally equivalent.\nSome remarks.\n\nIn terms of category theory, DT says that if \\(G\\) is an abelian group, then the functor \\(X\\mapsto \\pi_*(A_0(X;G), \\emptyset)\\) is naturally equivalent to the reduced singular homology functor \\(X\\mapsto \\tilde{H}_*(X;G)\\) in the category \\(TOP\\) of basepoint-free topological spaces. Again, we emphasize that Dold-Thom Theorem is basepoint independant, with the vacuum state \\(\\emptyset\\) serving as “canonical basepoint” on \\(X\\).\nFormally the homotopy groups \\(\\pi_q(A_0(X), \\emptyset)\\) consist of homotopy classes of pointed continuous maps \\(f: (S^q, pt)\\to (A_0(X), \\emptyset)\\). Identifying \\(pt\\) with the point at-infinity, we can thus model homotopy classes as compactly supported \\(q\\)-parameter family of distributions on \\({\\bf{R}}^q\\), where “compactly supported” means the distribution is equal to vacuum state outside a compact subset.\nOne immediate consequence of DT is that we find new topological models for the \\(K(G,n)\\) spaces. For example, we find \\(K(\\bf{Z}, 2)\\) is modelled by \\(A_0(S^2)\\), as opposed to the usual \\({\\bf{C}}P^\\infty\\) model. Identifying \\(S^2\\) with the Riemann sphere \\({\\bf{C}} P^1\\), we can identify \\(A_0(S^2)\\) with the set of rational functions on \\({\\bf{C}} P^1\\), i.e. the space of meromorphic functions on the Riemann sphere. The correspondance from meromorphic functions to \\(A_0(S^2)\\) is given by assigning to every meromorphic function \\(f\\) it’s divisors, i.e. poles and zeros of \\(f\\). More generally we find \\(K(G,n)\\) modelled by \\(A_0(S^n;G)\\) for every integer \\(n\\geq 1\\) and abelian group \\(G\\).\n\n\n\n\nNontrivial \\(1\\)-cycle on \\(S^1\\) represented as a loop in \\(A(X)\\)\n\n\n\n\nNaive Electrical Interpretation of Relative Dold-Thom\nAs a consequence of the Dold-Thom Theorem we have the following lemma:\n\n(Lemma) If \\(Y\\) is closed subset of \\(X\\), then we have canonical isomorphism \\[H_*(X,Y) / image(H_*(X)) = \\ker(H_{*-1}(Y) \\to H_{*-1}(X)),\\] where \\(image(H_*(X))\\) is the image of \\(H_*(X)\\) in the relative homology group \\(H_*(X,Y)\\), and the morphism \\(H_{*-1}(Y) \\to H_{*-1}(X)\\) is induced by the inclusion \\(Y\\hookrightarrow X\\).\n\nProof: Long exactness in DT’s theorem implies \\[image(H_*(X))=\\ker(\\delta)\\] and \\[image(\\delta)=\\ker(H_{*-1}(Y) \\to H_{*-1}(X)).\\] But we have canonical isomorphism \\[H_*(X,Y)/\\ker(\\delta)=image(\\delta),\\] and the result follows.\nWhen the source space \\(X\\) is contractible, then we find the isomorphism \\(H_*(X,Y) \\approx H_{*-1}(Y)\\) is canonical between homology groups. However the morphism is noncanonical on the singular chain groups. The construction of such a morphism requires a well-defined “filling” operation by which the cycles on \\(Y\\) can be filled to relative cycles on \\(X\\) mod \\(Y\\).\nHere is our “electrical” interpretation of the above lemma. We see that relative cycles on \\(X\\) (mod \\(Y\\)) are represented either by:\n\n\nchains \\(w\\) which are already cycles \\(\\del w=0\\) on \\(X\\), or\n\n\nchains \\(w\\) whose chain boundaries \\(\\del w\\) are nontrivial on \\(Y\\) but null homologous in \\(X\\).\n\n\nIn terms of DT, this says that if \\(Y\\) is a ground reservoir for charges on \\(X\\), then recombinations of \\(\\emptyset\\) will have parts which are recombinations in \\(X\\) separated from \\(Y\\) and other parts representing charges grounding out at \\(Y\\). For example, any excess charge on a conductive plate will either ground out at the boundary, or recombine and neutralize in the interior. For the relative homology groups, we can identify a component of \\(A_0(X/Y)\\) which is represented by “spontaneous” transports from \\(\\emptyset\\) to the reservoir \\(Y\\). These relative cycles one the one-dimensional disk with boundary are represented in the figure below.\n\n\n\nNontrivial relative 1-cycle on the interval\n\n\n\n\nConclusion\nThis is the first article of a three-part series on Dold-Thom. Our purpose here was to present the statement of the theorem, which allows an interesting spherical representation of homology cycles. There are many connections and ideas to develop, and which we will describe in future posts.\n-JHM.\n\n\n\n\n\nReferences\n\nDold, A., and R. Thom. 1958. “Quasifaserungen Und Unendliche Symmetrische Produkte.” Annals of Mathematics 67 (2): 239–81. http://www.jstor.org/stable/1970005.\n\n\nGromov, M. 2012. “In a Search for a Structure, Part 1: On Entropy.” http://www.ihes.fr/~gromov/PDF/structre-serch-entropy-july5-2012.pdf.\n\n\n———. 2014. “Six Lectures on Probability, Symmetry, Linearity.” http://www.ihes.fr/~gromov/PDF/probability-huge-Lecture-Nov-2014.pdf."
  },
  {
    "objectID": "posts/2023-03-11-GR_Tensor_PCA/index.html",
    "href": "posts/2023-03-11-GR_Tensor_PCA/index.html",
    "title": "Einstein’s Field Equations, Tensors, and the Point Coincidence Argument.",
    "section": "",
    "text": "The purpose of this post is to review the principles behind Einstein’s famous gravitational equation \\[R_{ij}-R g_{ij}=\\kappa T_{ij}.\\] We do not develop any analytic or mathematical properties of the equation, but rather indicate it’s history and motivation. Recall that:\n\n\\((R_{ij})\\) is the Ricci quadratic tensor,\n\\(R\\) is the trace of \\((R_{ij})\\),\n\\(g_{ij}\\) is the Lorentzian metric,\n\\(T_{ij}\\) is the so-called “energy momentum tensor”, and\n\\(\\kappa\\) is an absolute constant.\n\n\nTensors and Einstein’s Point-Coincidence Argument\nEinstein’s famous GR field equation is a tensor equation, and we recall the historical motivation behind this choice traditionally known as the point coincidence argument (PCA). The point-coincidence argument appears well known to historians of GR, arising from Einstein’s confused “hole argument” and the material meaning of coordinates. This argument however is the key heuristic by which Einstein arrived at tensor equations as the formal basis for his general relativity theory. The following quotation is Einstein’s and is taken from (Norton 1993):\n\n“The general laws of nature are to be expressed by equations which hold good for all systems of coordinates, that is, are covariant with respect to any substitutions whatever (generally covariant). It is clear that a physical theory which satisfies this postulate will also be suitable for the general postulate of relativity. For the sum of all substitutions in any case includes those which correspond to all relative motions of three-dimensional systems of coordinates. \\(\\ldots\\)\nThat this requirement of general covariance, which takes away from space and time the last remnant of physical objectivity, is a natural one, will be seen from the following reflexion. All our space-time verifications invariably amount to a determination of space-time coincidences. If, for example, events consisted merely in the motion of material points, then ultimately nothing would be observable but the meetings of two or more of these points. Moreover, the results of our measurings are nothing but verifications of such meetings of the material points of our measuring instruments with other material points, coincidences between the hands of a clock and points on the clock dial, and observed point-events happening at the same place and the same time.\nThe introduction of a system of reference serves no other purpose than to facilitate the description of the totality of such coincidences. We allot to the universe four space-time variables \\(x_0, x_1, x_2, x_3\\) in such a way that for every point-event there is a corresponding system of values of the variables \\(x_0, x_1, x_2, x_3\\). To two coincident point-events there corresponds one system of values of the variables \\(x_0, x_1, x_2, x_3\\), i.e. coincidence is characterized by the identity of the coordinates. If, in the place of the variables \\(x_0, x_1, x_2, x_3\\) we introduce functions of them, \\(x'_0, x'_1, x'_2, x'_3\\), as a new system of coordinates, so that the system of values are made to correspond to one another without ambiguity, the equality of all four coordinates in the new system will also serve as an expression for the space-time coincidence of the two point-events. As all our physical experience can be ultimately reduced to such coincidences, there is no immediate reason for preferring certain systems of coordinates to others, that is to say, we arrive at the requirement of general covariance.”\n\n\n\nCovariant Tensors and Invariant Divisors\nEinstein was much influenced by the absolute differential tensor calculus of Ricci, Grossmann, Levi-Civita, and especially the fundamental local property of tensors that (Levi-Civita, n.d., sec. IV.12):\n\n[T]he vanishing of a tensor (i.e. all its elements) is an invariant property, and which holds in general for any change of variables of any kind. In other words, if all the elements of a generic tensor \\(A_{i_1, i_2, \\ldots}^{j_1, j_2, \\ldots}\\) referred to a particular system of variables are zero, we may be sure that the equations \\(A_{i_1, i_2, \\ldots}^{j_1, j_2, \\ldots}=0\\) continue to hold however the variables may be changed.\n\nIt follows that tensors are covariant objects whose zeros and poles (called “divisors” by algebraic geometers) are topologically invariant subsets of the manifold. But again we must emphasize that the components and coefficients of a tensor are not invariant. According to Einstein’s PCA, we find covariant and contravariant tensors are the only valid objects of the GR theory.\nThe canonical tensor element in Einstein’s GR is the Lorentz quadratic form \\(g=ds^2\\). The volume form \\(det(g_{ij})^{1/n}\\) is another important tensor canonically associated with \\(g\\). According to the fundamental property of tensors, the zeros and poles of \\(g\\), \\(det(g_{ij})^{1/n}\\) are also invariant.\n\n\nCoordinate versus Physical versus Curvature Singularities:\nNow regarding the common terminology of “coordinate singularities” versus “physical singularity” (or more accurately “curvature singularity”), we make the following observation. The so-called coordinate singularity represents the locus of points where \\(det(g_{ij})=0\\), and here \\(det\\) represents the full four-dimensional determinant of the Lorentz metric \\((g_{ij})\\). But \\(g\\) and \\(det(g)\\) are well-defined tensors, so only their zero locus and poles is coordinate independant and “physical” according to Einstein’s PCA.\nFor example in spherical coordinates \\((r, \\theta)\\) on the usual Euclidean space \\(\\bf{R}^2\\), the Riemannian metric has volume form \\(\\det(g)= r dr \\wedge d\\theta\\). But this form vanishes at \\(r=0\\). Therefore we would argue that this zero is invariant with respect to diffeomorphisms.\nBy contrast, there is a rather ridiculous image in Misner-Thorne-Wheeler’s textbook “Gravitation” (Thorne, Wheeler, and Misner 2000). In our view their “mere” coordinate singularity is a bonafide singularity, being formed by the external force of the boot! It seems evident to us that a boot crushing a coordinate system is a physical singularity.\n\n\n\nMTW’s “mere” coordinate singularity\n\n\nMTW’s viewpoint leads to their introduction of “carets” and their so-called “geometrodynamic” approach, which approach we find unncessessary and not as rigorous as claimed.\n\n\nVacuum Equation\nNow we consider Einstein’s field equations. The vacuum equation (absence of matter) has the form \\[Ric  -R g =0.\\] All the quantities \\(Ric\\), \\(R\\), \\(g\\), and \\(Ric-Rg\\) are tensorial. Therefore their values are not invariantly defined, but their zeros and poles are invariantly defined. Notice that \\(g=(g_{ij})\\) is the unknown in the field equations, while \\(Ric\\) and \\(R\\) depend nonlinearly on \\(g\\).\n\\(\\newcommand{\\del}{\\partial}\\)\nWe pause to make trivial but important observation. The classical expression for particle velocities, namely \\(\\frac{\\del x_1}{\\del x_0}\\) are not tensorial, and therefore not independant of the system of coordinates. Here \\(x_0\\) is assumed to correspond to “time”. But neither is the contravariant derivative \\(\\nabla_{\\frac{\\del}{\\del x_0}} \\frac{\\del}{\\del x_1}\\) an invariant object in the theory. The contravariant derivative returns a tensor, and therefore only it’s zeros and poles constitute a coordinate-independant quantity in the theory. This makes the discussion of “velocities” always dependant on the chosen coordinate system, and only vanishing velocities \\(\\nabla=0\\) (or diverging to \\(+\\infty\\)) are invariantly defined. Likewise if \\(\\gamma\\) is a smooth curve in \\(X\\), then the covariant “acceleration” evaluated as \\(\\nabla_{\\gamma'} \\gamma'\\) is again tensorial, and therefore only its divisors are invariant. For example in special relativity, we have the Lorentzian line element \\(ds\\) which is a Lorentz invariant tensor. Therefore \\(\\delta \\int ds=0\\) is invariant equation for timelike curves \\(ds^2&gt;0\\).\n\n\nMatter Equations: Is the Stress-Energy Tensor Defined?\nWhat about Einstein’s field equations in the presence of matter? In the presence of matter, Einstein’s field equations has a nontrivial right hand term, namely \\[Ric - Rg= \\kappa T_{ij}\\] where \\(\\kappa\\) is said to be a constant of proportionality and \\(T=(T_{ij})\\) is the stress-energy tensor. The field equations in the presence of matter raise two objections.\n\nThe constant of proportionality \\(\\kappa\\) is arguably not a proper object of tensor calculus, unless it is absolutely constant. But if constant, then it has no meaning since it’s absorbed into the tensor vector \\(\"\\kappa T = T\"\\). Typically the purpose of \\(\\kappa\\) is to “dimensionally” align the physical units of the left- and right-hand sides of the field equation. But this raises the question, not without it’s own controversy, of “What are the units of the left-hand terms \\(Ric\\) and \\(Rg\\)?”\nIs \\(T=(T_{ij})\\) properly defined as a tensor object? By convention \\(T\\) is assumed to represent all the matter-energy in the spacetime. And in special cases, Einstein establishes formulas for \\(T\\). Yet do all these examples determine \\(T\\) as a tensor?\n\nRegarding our second objection, we agree that Einstein provided expressions for the stress-energy in certain coordinate systems for various systems. But how does one formula for the tensor in a coordinate system provide the definition of a tensor for all coordinate systems?\nThe issues are more subtle than they might first appear…\n\n\nUnits, \\(ds^2\\), and Proper Time\nThe previous section introduced the field equations, tensors, PCA, and raised some questions about physical units in the theory. Here we continue the discussion about units, but now focussing on the quadratic line element \\[g=ds^2=-c^2dt^2+dx^2+dy^2+dz^2.\\]\nIn previous posts we have warned readers from naively applying Riemannian ideas to Lorentzian metrics. It cannot be overemphasized that Lorentzian metrics are not Riemannian. When a Riemannian geometer starts working with Lorentz metrics, they naturally want to use the line element \\(ds':=\\sqrt{-ds^2}\\) on timelike curves. They are inclined therefore to refer to integrals \\(\\int_\\gamma ds'\\) as “lengths” along the curves \\(\\gamma\\). But again we warn the reader against taking this interpretation literally, for the reverse triangle inequality proves the “Lorentz distance” is not a Riemannian distance at all! This sounds trivial, but is actually nontrivial.\nRelated to the Lorentz metric is the so-called “proper time” function on timelike curves, usually represented by a definition type formula \\(ds'=c d\\tau\\). But we ultimately find this “definition” to not be a definition of “time” at all, because we ask the question: “What are the physical units of \\(d\\tau\\) ?”\nAs strange as it first sounds, we believe it’s a significant mistake to believe \\(d\\tau\\) has physical units of time. The fundamental problem is again that \\(ds'\\) is emphatically not a measure of metric distance. This is because we must contrast the Pythagorean theorem involving sum of squares and which is inherently Riemannian, with the difference of squares defining the Lorentzian measure.\nIn euclidean geometry, the squareroot of a sum of orthogonal squares represents a length because of Pythagorean identity. There are right angled triangles to be constructed whose hypotenuses are the lengths in question. However the squareroot of a sum of signed squares is not readily identified as a length to the Pythagorean, even if one assumes the squares are orthogonal. There are no right angled triangles to draw except on the null cone \\(ds^2=0\\) where the Pythagorean \\[c^2 dt^2=dx^2+dy^2+dz^2\\] is equivalently the hypothesis on the velocity of \\(c\\) in vacuum. However outside the null cone the Pythagoreans need to reasion cautiously, especially inside the null cone along the timelike curves.\nWe acknowledge the temptation of interpreting \\(\\sqrt{-ds^2}\\) as a length element along timelike curves. We ask these seemingly pedantic question to clarify whether the supposed identity \\[ds':=\\sqrt{-ds^2}=c d\\tau\\] provides a definition of the so-called “proper time” \\(d\\tau\\), and does \\(ds'\\) really have units of \\([length]\\)? Does \\(d\\tau\\) have units of \\([time]\\)?\nLet us include a frequent argument in the general relativity, alleging to establish the identity \\(ds'=c d\\tau\\), where \\(d\\tau\\) represents the so-called “proper time” of an inertial observer as measured by their local “clock”. This interpretation comes from the use of a so-called “instantaneous rest frame”. This requires the observer to find coordinates \\((\\tau, \\xi, \\eta, \\zeta)\\) where \\(\\tau\\) represents “time” and all the partial derivatives vanish\n\\[\\frac{\\del \\xi}{\\del \\tau}=\\frac{\\del \\eta}{\\del \\tau}=\\frac{\\del \\zeta}{\\del \\tau}=0.\\]\nIn this particular coordinate system one finds \\(ds^2=-c^2d\\tau^2\\) and \\(ds'=\\sqrt{-ds^2}=c d\\tau\\).\nBut can we really conclude that \\(ds'\\) has units of \\([length]\\) as a general rule, or equivalently that \\(ds'/c\\) has units of \\([time]\\) based on a computation in one coordinate system? Observe that the system of equations is not tensorial since the partial derivatives \\(\\del \\xi/ \\del \\tau\\) are not covariant.\n\n\nCuriel’s Arguments\nOur discussions are much influenced by the writings of J.D. Norton and E. Curiel.\nCuriel has alot to say about GR and especially the energy tensor. In fact, nobody poses the question better than Curiel: “Is energy even defined in GR?”. And it’s a serious question.\nCuriel makes the argument that the Einstein tensor \\(Ric - Rg\\) is the variation of Gauss curvature, therefore the gravitational energy tensor must depend solely on the Riemannian curvature tensor. But this essentially recovers Einstein’s original critique against Grossman and Levi-Civita’s suggestion that the Einstein tensor itself represents the gravitational energy tensor! For Einstein objected that such a definition would render the total energy of any closed system equal to zero. Curiel documents the rather confusing state-of-affairs involved in attempting to defined gravitational energy tensor and it’s role in Einstein’s field equations.\n[Incomplete]\n\n\nCarets and Non Tensors\nThe mere coordinate singularities, so named by MTW, leads to the introduction of “geometrodynamics” and “carets” in the GR. For example, an introduction to these ideas are discussed in (Klauber 2001) and briefly in (Thorne, Wheeler, and Misner 2000).\nThe caret argument is that despite the components of the tensors being not invariantly defined, the projections of the components on a “local Lorentz frame” are well-defined and physical relative to the local frame. Taking projections, they claim the results of tensor equations can be compared to laboratory measurements.\nThis idea of taking projections in a local Lorentz frame is curious, and though perhaps intuitively appealing, the mathematics is not clear.\nIn our view there is no canonical local Lorentz frame. And even given a frame, there is no well-defined projection from the tensors onto the frame. In practice, they take a projection by dividing and renormalizing “component wise”, but this appears to be ad hoc and only convenient when the metric is diagonal. Besides, they don’t seem aware of the fundamental property of tensors that their “components” are in no sense canonical. By analogy, in euclidean geometry, there is no canonical choice of orthogonal basis. Projections onto the coordinate axes will all be different, and the only invariant quantity is the sum of squares. That is the essence of Pythagoras: that the sum of squares is the only invariant quantity that can be measured by different choices of orthogonal bases. Likewise for Lorentz frames: it is only the Lorentz line element which is invariant. All other projections are noninvariant and non contravariant.\nBut again, most mathematicians/physicists are not persuaded by these critiques. And indeed the projection onto coordinates axes seems to be inspired by projections in quantum mechanics. In the Born-Dirac interpretation, the projections amount to “observations”. The carets attempts to develop a parallel idea in GR. Many will disagree here, but it depends on the answer to the question: Who taught you GR and how were you taught? There appears to be no large-scale consensus anyway, though we welcome any comments to the contrary.\nThe following paragraph is a good idea which is still very early in development.\n\n\nFrom Einstein Topological PCA to Probabilistic Gromov-Mendel\nFor Einstein, the only objective reality is the topological coincidence of points. This is the motivation for his seeking tensor equations, wherein the zeros \\(a_{ij}=0\\) are invariant with respect to change of variable, as the basis for a general theory of gravitation. For example, the left hand side of Einstein’s field equations \\[R_{ij}-R g_{ij}=\\kappa T_{ij}\\] is evidently a tensor quantity, since the Ricci tensor \\(Ric=(R_{ij})\\) is indeed tensorial, as is its trace \\(R\\) (scalar curvature).\nIn our research [ref] we investigate a probabilistic formulation of Einstein’s PCA, which is much closer to Gromov’s category of finite probability spaces. In this work, we replace “tensor equations” – which in our mind are much too idealized objects over the real numbers \\(\\bf{R}\\) – with stochastic “frequency equations”.\nIdea: the basic tensor expression \\[d\\xi=\\frac{\\partial \\xi}{\\partial x_1} dx_1+\\cdots + \\frac{\\partial \\xi}{\\partial x_n} dx_n\\] has a probabilistic interpretation when we replace \\(d\\xi\\) with the time-rate \\(d\\xi/dt\\). Compare Gromov’s paper on Mendel (Gromov 2007) where tensors and distributions are also identified.\n\n\n\n\n\nReferences\n\nGromov, M. 2007. “Mendelian Dynamics and Sturtevant’s Paradigm.” Geometric and Probabilistic Structures in Dynamics, 227–42.\n\n\nKlauber, Robert D. 2001. “Physical Components, Coordinate Components, and the Speed of Light.” arXiv. https://doi.org/10.48550/ARXIV.GR-QC/0105071.\n\n\nLevi-Civita, Tullio. n.d. The Absolute Differential Calculus (Calculus of Tensors).\n\n\nNorton, John D. 1993. “General Covariance and the Foundations of General Relativity: Eight Decades of Dispute.” Reports on Progress in Physics 56 (7): 791.\n\n\nThorne, Kip S, John Archibald Wheeler, and Charles W Misner. 2000. Gravitation. Freeman San Francisco, CA."
  },
  {
    "objectID": "posts/2024-01-02-Bookstore/Bookstores and Image to ISBN Problem [JHM Labs].html",
    "href": "posts/2024-01-02-Bookstore/Bookstores and Image to ISBN Problem [JHM Labs].html",
    "title": "Bookstores and Image to ISBN Problem",
    "section": "",
    "text": "Random Bookshelf\n\n\n\nBookstore Problem.\n“People don’t buy what they don’t see”. Or equivalently “People only buy what they see”.\nWe walk into Book Bazaar on Bank Street, and we see shelves on shelves filled with books. And great titles. But who is buying? This is the same problem as the vintage clothing shops. The owners are still running the old fashioned brick and mortar model where the owner waits until:\n\nclients walk into the store;\nclients browse the shelves;\nclients maybe identify an item they want;\nclients maybe buy the item.\n\nWe ask the owner of BB: “Do you have a searchable index of these books?” I.e. do you even know what your inventory is? The owner says “No.” In many cases the owner indeed knows alot of titles in his/her head. But the problem is the owner remains the only person with a virtual approximate index in their head, and who can search it?\nSo what’s the problem, and what’s the solution? We need a basic function to take images of bookshelves, and output lists of books (i.e. authors, titles, etc..). This is like image_to_ISBN_list routine.\n\nInput: digital images of book stacked in bookshelves.\nOutput: Searchable index of ISBNs and titles.\n\nWe see the algorithm factored into three steps: 1) individualization, 2) image-to-text, 3) text-to-isbn.\n\nStep 1: (Individualization) Isolate rectangles of the individual books in the image.\n\nConcretely this means partitioning the digital image into rectangles where each rectangle contains the “spine” of the book and the relevant individual text. The text is unstructured and fragmented.\nRemark. If the books are randomly distributed on the book shelf, then with large probability the books are different colours and different shapes and there is alot of contrast between adjacent books. Therefore the individual rectangles/squares/individualization should perform best on random bookshelves.\nBy contrast the BB has a shelf of “penguin classics” and they are paperbacks which look identical in size and colour, and the only difference becomes the small faint text. However we don’t think this setting is relevant to the BB example, since these books are already “worthless”.\n\nStep 2: (Image-to-text) We compose the individualized rectangles into a basic image_to_text function. This provides some unordered text data. I.e., just words or letters, etc, and whatever symbols are available on the spine image.\n\nThere is limited text on the spine therefore we need extract all the words, as much as possible. But the text is unstructured, i.e. not consisting of sentences and sometimes images, i.e. publisher’s logo.\n\nStep 3: (Text to ISBN) Finally we compose the text output from Step 2 with a text_to_isbn function.\n\nAgain the text extracted from Step 2 is partly unstructured since the spine contains limited publisher information. In Step 3 we take a “best guess” of the book title (ISBN) using the text extracted from Step 2.\nClaim: The composition of steps \\(3 \\circ 2 \\circ 1\\) gives an image_to_isbn mapping. This is the basic tool which we offer to the owners of the bookstores.\nBookstore owners login to a gmail account, input images to a python input which we provide, and an google spreadsheet is automatically being updated with the inventory. Therefore owners have searchable index of ISBN objects.\nRemark. There is important sequel to this idea which is the eventual shipping of the books, and their specific locations when an online sale is made. For practical application to the book store, another key problem is updating the index based on daily or weekly in-flows and out-flows of books. Most stores keep a written list of sales outgoing which can be photographed at end of week and then broadcast/merged into the index."
  },
  {
    "objectID": "posts/2024-01-02-MeasuringTape/Measuring Tape.html",
    "href": "posts/2024-01-02-MeasuringTape/Measuring Tape.html",
    "title": "Measuring Tapes and Physics",
    "section": "",
    "text": "We have an idea about digital measuring tapes motivated from repeatedly overhearing the question “Dude would you hold my tape?” on work sites. We think it’s absurd question, and two people are not needed to take an accurate measurement. Our goal is a “point and click” tool to find distances between arbitrary points in an open space volume.\nIf we are measuring distances inside a volume \\(V\\) with boundary \\(\\partial V\\), this means evaluating distances for arbitrary pairs $ (x,y) V V$. All the laser pointers currently on the markey, e.g. Amazon, strictly find distances between boundary points \\((x,y) \\in \\partial V \\times \\partial V\\). This distinction between distances in the volume and distance on the boundary is key.\nFormally the problem is this: let \\(C\\subset {\\bf{R}}^3\\) be a set of centres in space. Suppose we are given two arbitrary points \\(x, y\\in {\\bf{R}}^3\\) and we are given the pairwise distances from \\(x,y\\) to the centres, i.e. we are given the distances $ { dist(x,c)}$ and \\(\\{dist(y,c)\\}\\) for every \\(c\\in C\\). The problem is how to decide \\(dist(x,y)\\) from these measurements. In the literature this problem is called “trilation” as opposed to “triangulation” where distances are inferred from distances instead of angles. There is direct interpretation of the trilation problem via sphere intersections. We also explore a Choquet max entropy method below.\nRemark. We must admit there is wonderful simplicity to conventional measuring tapes. The contractor spans the tape across the distance, hooking the edge of the tape at a given edge, and then estimates the distance with his eye looking at the “end” of the tape in their hand. This basic application requires the tape have a well layed position and be human readable. For simple situations this works fine. But the contractor has many instances throughout a day where awkward measurements need be taken. Here we think the point and click distance finder could work. No physical spans are required."
  },
  {
    "objectID": "posts/2024-01-02-MeasuringTape/Measuring Tape.html#digital-measuring-tapes.-our-answer-to-dude-would-you-hold-my-tape",
    "href": "posts/2024-01-02-MeasuringTape/Measuring Tape.html#digital-measuring-tapes.-our-answer-to-dude-would-you-hold-my-tape",
    "title": "Measuring Tapes and Physics",
    "section": "",
    "text": "We have an idea about digital measuring tapes motivated from repeatedly overhearing the question “Dude would you hold my tape?” on work sites. We think it’s absurd question, and two people are not needed to take an accurate measurement. Our goal is a “point and click” tool to find distances between arbitrary points in an open space volume.\nIf we are measuring distances inside a volume \\(V\\) with boundary \\(\\partial V\\), this means evaluating distances for arbitrary pairs $ (x,y) V V$. All the laser pointers currently on the markey, e.g. Amazon, strictly find distances between boundary points \\((x,y) \\in \\partial V \\times \\partial V\\). This distinction between distances in the volume and distance on the boundary is key.\nFormally the problem is this: let \\(C\\subset {\\bf{R}}^3\\) be a set of centres in space. Suppose we are given two arbitrary points \\(x, y\\in {\\bf{R}}^3\\) and we are given the pairwise distances from \\(x,y\\) to the centres, i.e. we are given the distances $ { dist(x,c)}$ and \\(\\{dist(y,c)\\}\\) for every \\(c\\in C\\). The problem is how to decide \\(dist(x,y)\\) from these measurements. In the literature this problem is called “trilation” as opposed to “triangulation” where distances are inferred from distances instead of angles. There is direct interpretation of the trilation problem via sphere intersections. We also explore a Choquet max entropy method below.\nRemark. We must admit there is wonderful simplicity to conventional measuring tapes. The contractor spans the tape across the distance, hooking the edge of the tape at a given edge, and then estimates the distance with his eye looking at the “end” of the tape in their hand. This basic application requires the tape have a well layed position and be human readable. For simple situations this works fine. But the contractor has many instances throughout a day where awkward measurements need be taken. Here we think the point and click distance finder could work. No physical spans are required."
  },
  {
    "objectID": "posts/2024-01-02-MeasuringTape/Measuring Tape.html#sphere-intersections",
    "href": "posts/2024-01-02-MeasuringTape/Measuring Tape.html#sphere-intersections",
    "title": "Measuring Tapes and Physics",
    "section": "Sphere Intersections",
    "text": "Sphere Intersections\nThe measuring tape problem is related to the fast solving of sphere intersections. If \\(S_1 = S(c_1, r_1)\\) and \\(S_2 = S(c_2, r_2)\\) are spheres in \\({\\bf{R}}^3\\), then their intersection \\(S_1 \\cap S_2\\) is a sphere orthogonal to the vector \\(c_{12}:=c_1-c_2\\). This is immediately seen in figure below [insert image]. Thus the immediate problem is to decide when spheres have empty or nonempty intersection. Generically the intersection is either empty or a codimension one sphere orthogonal to \\(c_{12}\\). Thus we have a bisection type method to decide sphere intersections, namely $S_1 S_2 S_3 $ is equal to \\(S_{12} \\cap S_{34} \\cap \\cdots\\) which is equal to \\(S_{1234} \\cap \\cdots\\) etc.\nRemark. If the centres \\(c\\in C\\) are infinitely far away from the points \\(x,y\\). Then the spheres centred at \\(c\\) will intersect \\(x,y\\) at large radius and appear as affine subspaces. Therefore when the centres \\(C\\) are at infinity, the problem reduces to linear algebra. Given normal vectors \\(\\{n_i\\}\\), decide the distance \\(dist(x,y)\\) from the values of the linear functionals \\(\\{\\langle n_i, x \\rangle\\}_i\\), \\(\\{\\langle n_i, y \\rangle\\}_i\\) for given \\(x,y\\). This means solving an inhomogeneous linear system of equations."
  },
  {
    "objectID": "posts/2024-01-02-MeasuringTape/Measuring Tape.html#euclidean-distance-formula-from-choquet-representation.",
    "href": "posts/2024-01-02-MeasuringTape/Measuring Tape.html#euclidean-distance-formula-from-choquet-representation.",
    "title": "Measuring Tapes and Physics",
    "section": "Euclidean Distance Formula from Choquet Representation.",
    "text": "Euclidean Distance Formula from Choquet Representation.\nNow our idea is to use Choquet representation theorem. Given the distance to centres \\(dist(x,c)\\), \\(c\\in C\\), we construct the maximal entropy measure \\(\\lambda_x\\) supported on \\(C\\) which represents \\(x\\). The measure \\(\\lambda_x\\) represents \\(x\\) in the sense of Choquet Representation theorem [ref] if \\[\\ell(x)=\\int_C \\ell(\\bar{x}) d\\lambda_x(\\bar{x})\\] for every linear functional \\(\\ell\\) on \\({\\bf{R}}^3\\). We emphasize the linearity of \\(\\ell\\).\nIf we study the Euclidean distance, then we deduce the following representation formula:\n\\[dist(x,y)^2:=|x-y|^2~=~\\iint_{C \\times C} \\langle c_i, c_j \\rangle ~~ d(\\lambda_x - \\lambda_y) \\otimes d(\\lambda_x - \\lambda_y).\\] Therefore we find the squared euclidean distance is a \\(\\lambda_x\\) weighted sum of the signed dot products \\(\\langle c_i, c_j \\rangle\\) of centres .\nWe define \\(\\lambda=\\lambda_x\\) as the unique probability measure on \\(C\\) which maximizes the entropy \\(H(\\lambda)\\) subject to the linear constraints \\(\\sum_i \\lambda_i = 1\\) and \\(\\sum_i \\lambda_i c_i=x\\). For [eq1] to be an efficient formula, we need a fast algorithm to represent \\(\\lambda_x\\) given the distance to centres \\(dist(x,c)\\).\nThe method of Lagrange multipliers is based on the observation that \\(H\\) is optimized given the constraints when \\(\\nabla_\\lambda H\\) is linearly dependant with the constraint gradients. The Lagrangian for this optimization problem is \\[L(\\lambda, \\alpha, \\beta):=H(\\lambda)+\\alpha(\\sum_i \\lambda_i -1) + \\langle \\beta, ~~\\sum_i \\lambda_i c_i -x \\rangle\\] where \\(\\lambda \\in {{\\bf{R}}^I}\\), \\(\\alpha \\in \\bf{R}\\), \\(\\beta \\in {\\bf{R}}^3\\).\nIf \\(\\lambda\\) is maximizer, then we have vanishing partial derivatives \\[\\frac{\\partial L}{\\partial \\lambda_i}=\\log \\lambda_i +1  + \\alpha + \\langle \\beta, ~ c_i \\rangle=0, ~~\\frac{\\partial L}{\\partial \\alpha}=0, ~~ \\nabla_\\beta L=0.\\]\nNow we need solve for the variables \\(\\alpha, \\beta, \\lambda\\) using the above equations. We reproduce the calculation below.\nFirst we have \\(\\lambda_i = e^{-(1+\\alpha)} ~~ e^{-\\langle \\beta, c_i \\rangle}.\\)\nThe condition \\(\\sum \\lambda_i=1\\) implies \\(e^{1+\\alpha} =\\sum_i e^{-\\langle \\beta, c_i \\rangle}\\). Consequently we find\n\\[\\lambda_i = \\frac{1}{\\sum_i e^{-\\langle \\beta, c_i \\rangle}}~~ e^{-\\langle \\beta, c_i \\rangle}.\\]\nNext the condition \\(\\sum \\lambda_i c_i = x\\) implies \\[\\sum_i e^{-\\langle \\beta, c_i \\rangle} ~ (c_i - x)=0.\\] Thus we have reduced everything to solving for \\(\\beta \\in {\\bf{R}}^3\\) in this equation. In this case the auxiliary mapping \\(F:{\\bf{R}}^3 \\to {\\bf{R}}^3\\) defined by \\(F(\\beta):= \\sum_i e^{-\\langle \\beta, c_i \\rangle} ~ (c_i - x)\\) is nonlinear. The max entropy measure \\(\\lambda\\) is defined by the equation [lambda] where \\(\\beta\\) is the solution to \\(F(\\beta)=0\\). We observe that \\(F(0)=\\sum_i c_i-x\\). The basic idea is that \\(F\\) is monotone in \\(\\beta \\in {\\bf{R}}^3\\), i.e. the partial derivatives \\(\\frac{\\partial F}{\\partial \\beta_1}\\), \\(\\frac{\\partial F}{\\partial \\beta_2}\\), \\(\\frac{\\partial F}{\\partial \\beta_3}\\) are nonvanishing and have constant sign."
  },
  {
    "objectID": "posts/2024-01-02-MeasuringTape/Measuring Tape.html#action-at-a-distance-how-to-physically-measure-distances",
    "href": "posts/2024-01-02-MeasuringTape/Measuring Tape.html#action-at-a-distance-how-to-physically-measure-distances",
    "title": "Measuring Tapes and Physics",
    "section": "Action at a Distance: How to Physically Measure Distances",
    "text": "Action at a Distance: How to Physically Measure Distances\nWe have commented on the simplicity of the physical measuring tape. But are there other physical principles to determine distances? Evidently the measuring tape requires a physical tape connecting two distinct points in space. But can we measure distance at a distance? How did the Mars rovers determine their positions on the Mars surface. Was position and distance found indirectly via satellite imagery? For example, how does one really decide the Earth-Moon distance?\nStellar Parallax Method\nWe are inspired to reconsider action at a distance forces.\nCan we use forces to measure distances?\nSir Isaac Newton’s 1640s gravitational force law introduced instantaneous action at a distance into physics. Newton unified the forces of the tides, the moon, orbits of the planets, falling bodies. Before Newton it was believed that the orbits of the planets were circular, and that uniform circular motion was perfect and requiring no external causes to persist. However for our purposes gravity is too weak a force to use for interaction measurements.\nElectrostatic forces between a proton-electron pair has a magnitude \\(39 x\\) greater than gravitational attraction. For example, one might naively consider using Coloumb’s law of electrostatic repulsion to determine distance. This would involve a unit charge at position \\(x_1\\), and a unit charge at \\(x_2\\), and measuring the net force. However the net resultant force \\(F\\) at \\(x_1\\) is not necessarily equal to the Coulomb force \\(F_{21}\\). This is because the system is not necessarily electrically isolated! Instead we consider \\[F_{\\text{net}, 1}=F_{21} + F_{\\text{env},1}\\] where \\(F_{\\text{env},1}\\) is the resultant environmental force on \\(x_1\\). In this formula assume that the introduction of the electrical charge at \\(x_1\\) and \\(x_2\\) does not affect the environmental forces \\(F_{\\text{env},1}\\), \\(F_{\\text{env},2}\\). This is an idealized situation. The Coulomb force is deduced only after subtracting the net background force \\(F_{\\text{env},1}\\) from the resultant force. And this is the main idea: we have to make two measurements for \\(F_{\\text{env}}\\) and \\(F_{\\text{net}}\\) and deduce the interaction as a difference.\nWe emphasize the deduction because this is a logical construct. As we have commented there is possibility that the introduction of the electric charges at \\(x_1\\), \\(x_2\\) has an effect on the environmental forces.\nThe basic interaction \\(F_{21}\\) does not necessarily have to be Coulomb’s interaction. We could replace the point charges at \\(x_1, x_2\\) with, say, ring currents or dipoles.\n[to be continued … JHM]"
  },
  {
    "objectID": "posts/2024-01-02-Weber/index.html",
    "href": "posts/2024-01-02-Weber/index.html",
    "title": "Weber, Work, Energy",
    "section": "",
    "text": "“Work, Weber’s Critical Radius, Energy”\nEinstein’s \\(E=mc^2\\) is the most famous formula in history, but what does it mean? Here we emphasize that Einstein derived the formula from mathematics in 1905, but Wilhelm Weber derived the formula from his fundamental electodynamic force law. Weber’s derivation leads to the amazing prediction of 2 equal to 1 plus 1 as a stable electronic system.\nConsider the problem of work and moving particles “upstream” of the potential. Recall the Coulomb-type expression that “opposite charges attract” and “like charges repel”. If we arranged some charged particles on a plate, then in a short time these particles would either be repelled outwards to the boundary of the plate or the opposite charges would cancel on the interior. Again this is with respect to the Coulomb model \\(V(r)=1/r\\).\nBut Weber’s model has the following amazing prediction: “opposite charges attract except at a small critical distance \\(r=r_c\\) where the charges acquires a negative inertial mass and the Weber force becomes repulsive”. Furthermore, “like charges repel except at a small critical distance where the Weber force becomes attractive”. This latter prediction leads to Weber’s *planetary model of the atom”.\nIn the (cgs) units the critical radius \\(r_c\\) is computed as \\(r_c=\\frac{1}{m c^2}\\) where \\(m\\) is the relative inertial mass of one of the particles. The proof of this formula is an application of Newton’s second law, that \\(m a = \\bf{F}\\) where \\(\\bf{F}\\) is Weber’s force.\nNow the question arises: if we have two identical but opposite electrically charged particles, say \\((-1)e\\) and \\((+1)e\\), where \\(e\\) is a small mass, then how much work is required to drive \\((-1)e\\) and \\((+1)e\\) to within Weber’s critical radius \\(r_c\\)?\nWeber comments in his final memoir [ref] that an infinite amount of work would be required, however no technical details are provided. In fact this author thinks its evident that a large finite amount of work is required, and the computation of this work required is the subject of this post.\nHow to compute Work: We imagine a particle moving through a potential \\(U\\) along a trajectory \\(\\gamma.\\) Our goal is to determine how much work is required to breach (“pass through”) the critical radius \\(r_c=\\frac{1}{mc^2}\\) in units where \\(4 \\pi\\epsilon_0 = 1\\).\nWe begin assuming the particle’s trajectory is rectilinear. This means the tangent \\(\\gamma'\\) and \\(\\gamma\\) are parallel for all values of the parameter. Moreover we can use the Riemannian idea of parameterizing the curve by arclength. Then we ask how much work is required to move the particle through \\(U\\) at a constant rate of speed, namely \\(||\\gamma'||=1\\).\nWeber’s force is conservative, so there is some path independance, however the terminal conditions are not fixed a priori. It might happen that breaching the Weber radius is easier if the particles velocity is nearly parallel to the “virtual” surface of the Weber’s critical radius. That is to say, the curvature term \\(r r''/c^2\\) in Weber’s force law might sometimes reduce the work needed, depending on the sign.\nIn terms of the relational variables recall the useful formula/definition \\[ r' = \\frac{\\bf{r} \\cdot \\bf{r'}}{r}.\\] In our setting we find \\[r'=\\frac{\\gamma \\cdot \\gamma'}{||\\gamma||}.\\] Applying the Cauchy-Schwartz inequality, we find \\[r'=1.\\] I.e. equality is obtained in Cauchy-Schwartz because \\(\\gamma, \\gamma'\\) are parallel by hypothesis and \\(||\\gamma'||=1\\).\nMoreover the rectilinear motion implies \\(r''=0\\), i.e. the trajectory has zero curvature, since the direction of the particle does not change. Therefore Weber’s force leads to work being computed by the integral \\[W=(1-\\frac{1}{2c^2})\\int_{+\\infty}^{r_c} \\frac{1}{r^2} dr     = (1-\\frac{1}{2c^2})mc^2=mc^2-m/2.\\] So what is the total energy of the above system? The total energy is the work required \\(mc^2 - m/2\\) plus the initial kinetic energy \\(T_i=m|\\gamma'|^2/2=m/2\\). Therefore the total energy required to breach the Weber critical radius is \\(E=W+T_i=mc^2\\).\nN.B. All the while our particles are “travelling upstream”. So the above computation indicates that if a particle is given sufficient energy (i.e. sufficient kinetic energy, then the particle could breach the Weber critical radius, and arriving at the Weber radius with almost zero kinetic energy).\nN.B. The integral representing \\(W\\) is parameterization independant. Therefore the work required by the unit-parameterized path is not overly specialized, but represents the general computation.\nThe above discussion was restricted to rectilinear trajectories. But the possibility remains that curvilinear (“spiralling”) trajectories require less work to breach the Weber radius. Thus while it appears that breaching the Weber radius via rectilinear paths requires large energy \\(\\approx mc^2\\), perhaps the spiralling paths – where the curvature term maintains a definite sign – are the more interesting.\nProblem: Determine the minimum energy required for an isolated two-body system to breach the Weber critical radius.\nAnswer: The minimum energy required to breach the critical radius is \\(E=mc^2\\).\nThis is consequence of the fact that \\(\\bf{F}\\) is a conservative force, therefore dependant only on the initial and terminal states, and not the path taken. Moreover the work done by a particle traversing a path \\(\\gamma\\) depends only on the difference in potential energies. This implies that the above evaluation in the rectilinear case is essentially the same for all paths from some initial point to to within the critical radius.\n\n\nEinstein’s NonPhysical Derivation\nk we apparently all know that \\(E=mc^2\\) is one of the great achievements of Albert Einstein. He first published the idea in 1905 in a paper called “Does the inertia of a body depend upon its energy content?”\nBut what does the formula mean?\nSupposedly it’s one of the greatest formulas in history, yet what is the content of this supposed formula?\nWe give the reader a few minutes to consider what they understand about this formula. For example, what can you say about it?\nThe standard explanations are not so useful. They say that a body of mass possesses some intrinsic energy which is independant of the observer frame. And this intrinsic energy is computed – according to Einstein’s suggestion – as \\(m_0c^2\\), where \\(m=m_0\\) is the inertial rest mass of the object.\nNow here enters another idea, that the inertial mass of an object increases with its velocity, i.e. we have \\(E=mc^2 = \\beta m_0 c^2\\) where \\(\\beta\\) is one of the Lorentz-type beta factor arising so frequently in special relativity, that is \\(\\beta = 1 /\\sqrt{1-v^2/c^2}\\).\nNow look, for most students in university, or even adults, this is the end of the story. You can take physics in undergraduate college or university, and I think this is the standard treatment.\nYou can ask “Why is the formula true?”, and the teacher might say “You won’t need to prove it on the exam, don’t worry about it!”, or “Well, I don’t know, nobody ever asked me, but I guess you can read Einstein’s work”, or “I don’t know, but look at wikipedia.”\nAnd reading Einstein’s original paper is a good idea, although it won’t likely be satisfying. Another good reference is Levi-Civita’s “Absolute Differential Calculus” textbook which has very detailed mathematical review of Einstein’s “physical principles”.\nIn Levi-Civita’s textbook, the formula is derived via an argument using Hamilton’s principle. It’s something like this: we start with the usual Lagrangian \\(L\\), and find the equations of motion are \\(\\delta \\int L = 0\\), where \\(\\delta \\int\\) is the variational derivative of the functional \\(\\alpha \\mapsto \\int_\\alpha L\\). (Here \\(\\alpha\\) denotes a path, not the earlier Lorentz gamma factor.)\nActually in Einstein and Levi-Civita’s approach, which is based on Hamilton principle. The starting point is a Lagrangian \\(L\\) defined on the statespace, and then with Hamiltonian \\(H=L^*\\). The Hamiltonian principle says \\[\\delta \\int L=0.\\] But Einstein introduces the variational equation \\[\\delta \\int c^2 = 0.\\] And he says this equation is trivial like \\[\\delta \\int dt =0 .\\]\nSo Einstein introduces the Lagrangian \\(c^2 - L\\) and the Hamilton principle becomes \\[\\delta \\int (c^2 - L) = 0.\\]\nThis apparently trivial modification of the Lagrangian is what introduces (imports) the constant \\(c^2\\) energy term in the Hamiltonian.\nWe omit the details from Levi-Civita, but the main formula obtained is \\[H^*=c^2 - L = c^2 \\sqrt{ 1-\\beta -2U/c } = c^2 +  v^2/2 - U,\\] since \\(L = v^2/2 + U\\). Einstein remarks that if \\(v=0\\) and \\(U=0\\), i.e. if the potential energy and kinetic energy vanish, then there still remains an intrinsic energy represented by \\(c^2\\), i.e. the Hamiltonian \\(H^*\\) does not vanish.\nIn the above argument, we remark that it’s not always clear that \\(U=0\\) is meaningful or nonarbitrary. As well known, it’s the gradient of the potential \\(\\nabla U\\) which enters into the dynamical equations, and not the scalar values of \\(U\\). So on this point we are not entirely persuaded that \\(v=0, U=0\\) represents an intrinsic energy.\nSo in summary, what’s the story behind this amazing formula \\(E=mc^2\\) ? In the above argument, there is no story except trivial mathematics. There is no physics!\nLevi-Civita makes this same remark in his textbook, but points a posteriori to radioactive substances.\n\n\nWeber’s Physical Interpretation of E=mu.c^2 ?\nWilhelm Weber (1804 – 1891) was a great physicist mathematician who succeeded C.F. Gauss as directory of the Gottingen Observatory. Weber was the first physicist to define \\(c\\), with Kirchoff using Weber’s force equations, as the velocity of electrical impulses in a material wire.\nIn his works on particle electrodynamics, he discovered in 1860s an amazing physical model of \\(E=\\mu c^2\\) where \\(\\mu\\) is the reduced mass of the system of electric particles. We have written on this amazing fact in a previous post “Weber’s Critical Radius, Work, and E=mc2 Formulas”\nWe repeat the fundamental physical idea (and this is what is totally absent in Einstein’s derivation).\nConsider two equal and identical unit electric charges \\(q_1\\) and \\(q_2\\) which are separated by some relative distance \\(r&gt;0\\). The Couloumbian motto is that “like charges repel, and opposite charges attract”. So a force is required to push the charges \\(q_1\\) and \\(q_2\\) closer together. In otherwords, there is work that needs be done to push the identical charges \\(q_1, q_2\\) together.\nWhat Weber discovererd in 1860s (maybe earlier) was that there is a critical distance, where if a sufficient amount of work is performed, and the charges \\(q_1, q_2\\) become sufficiently close $r&lt; r_c $ (passing through Weber’s critical radius \\(r_c\\)) then there is a sign in Weber’s force law which predicts that the equal charged particles become attractive within the critical radius \\(r&lt; r_c\\). In otherwords it’s possible for two-body system of net charge \\(-2\\) \\((=-1 -1)\\) to be a stable system with large potential energy! There is a large amount of potential energy within this system because alot of work was performed to push the particles through the critical radius.\nMore specifically, Weber’s force predicts that the amount of work required to push the particles through the critical radius is \\(E= \\mu c^2\\), where \\(\\mu\\) is the reduced mass of the system.\nThis is amazing fact! This is a physical explanation of what the formula means. The formula represents an amount of work which has been invested into the system. Moreover, Weber’s formula predicts that this potential energy is stored in surprisingly stable many body systems, e.g. two-body systems of the form \\(-2=-1-1\\).\nI don’t know if the reader can appreciate how EPIC this idea of Weber’s is. It’s the beginning of something incredible, basically returning physics to classical paradigm, and appreciating Wilhelm Weber’s tremendous contributions. Here I have been much influenced by AKT Assis’ recent translation into English (and Spanish, Portugese) of Wilhelm Weber’s collected works (in four volumes)."
  },
  {
    "objectID": "posts/2024-01-03-WeberGibbsLiouville/2024-01-03-WeberGibbsLiouville.html",
    "href": "posts/2024-01-03-WeberGibbsLiouville/2024-01-03-WeberGibbsLiouville.html",
    "title": "Weber Hamiltonian Gibbs Liouville Measure on State Space.",
    "section": "",
    "text": "What are Hamilton’s equations? In a sense the equations are a trivial formalization of Newton’s equations. But when applied to Weber’s force law, the Hamilton equations provide a useful formalism."
  },
  {
    "objectID": "posts/2024-01-03-WeberGibbsLiouville/2024-01-03-WeberGibbsLiouville.html#two-dimensional-xv-relational-state-space",
    "href": "posts/2024-01-03-WeberGibbsLiouville/2024-01-03-WeberGibbsLiouville.html#two-dimensional-xv-relational-state-space",
    "title": "Weber Hamiltonian Gibbs Liouville Measure on State Space.",
    "section": "Two Dimensional xv Relational State Space",
    "text": "Two Dimensional xv Relational State Space\nWe let \\(x,v\\) be position and velocity state variables. Let \\(J\\) be the standard almost-complex structure, i.e. \\(J=\\begin{pmatrix} 0 & 1 \\\\ -1 & 0 \\end{pmatrix}\\) with \\(J^2 = -Id\\).\nThe classical Gibbs-Liouville theorem [ref] says that the canonical volume form \\(dx dv\\) on state space is invariant with respect to Hamiltonian evolution. In otherwords, for a given rectange \\(dx dv\\) of initial conditions in statespace, the forward time evolution of the Hamiltonian system with these given initial conditions occupies a region in \\(xv\\)-spave having the same volume with respect to \\(dx dv\\).\nFor classical Hamilton equations, this follows from the vector field \\(J \\cdot \\nabla H\\) being divergence free: \\[div(J \\nabla H)=0.\\] If we integrate the Hamilton equations, then we obtain \\[\\frac{d}{dt}(x,v)=(\\dot{x}, \\dot{v})= \\begin{pmatrix} \\partial H / \\partial v \\\\ - \\partial H/\\partial x   \\end{pmatrix},\\] and the point is that this field has zero divergence.\nFor example, if the \\(H = \\omega (x^2 + v^2)\\) is the Hamiltonian for a one-particle oscillator, then the energy levels are circles, and the Hamilton evolution is like a rigid rotation. Therefore it’s evident that the area form \\(dx dv\\) is invariant with respect to time evolution.\n[Another example?]\nOur interpretation of the time evolution being volume invariant means the overall “uncertainty” in the state is constant with respect to time. In other words if we know position \\(x\\) with some error \\(dx\\) and velocity \\(v\\) with error \\(dv\\), then the overall uncertainty of the system remains \\(dx dv\\) square-error throughout the evolution. This is relatively good, for example if the particles are spatially closeby (so \\(dx\\) is small), then we are certain that \\(dv\\) is large (i.e. they have large relative motion). Likewise if the particles have very small relative motion (\\(dv\\) small) then the particles have large relative position (\\(dx\\) large).\nThe existence of an invariant background measure \\(dx dv\\) on state space is necessary to define the entropy of states \\(\\rho\\), where entropy is defined relative to this background mesaure. However our researches into Weber electrodynamics has led us to review the basic Gibbs Liouville measure. We find the usual Gibbs-Liouville volume form \\(dx dv\\) is not invariant when the state space evolves according to Weber’s Hamiltonian. Our goal in this article is to derive the proper Weberian invariant measure on state space."
  },
  {
    "objectID": "posts/2024-01-03-WeberGibbsLiouville/2024-01-03-WeberGibbsLiouville.html#distance-independant-densities",
    "href": "posts/2024-01-03-WeberGibbsLiouville/2024-01-03-WeberGibbsLiouville.html#distance-independant-densities",
    "title": "Weber Hamiltonian Gibbs Liouville Measure on State Space.",
    "section": "Distance Independant Densities",
    "text": "Distance Independant Densities\nSuppose that \\(f_{,x}=0\\) identically. Then the basic equation becomes \\[d_v \\log f = -\\log (1-\\frac{v^2}{2c^2}).\\] Therefore we find \\(f= (1-\\frac{v^2}{2c^2})^{-1}\\) is an invariant density which is position independant.\n\nProposition: The density \\[\\rho =(1-\\frac{v^2}{2c^2})^{-1} ~dx dv\\] is the unique invariant measure on state space \\(\\{(x,v)~~|~x&gt;0\\}\\) which is position independant and satisfies \\(\\rho(x,0)=1\\) identically.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-3, 3, 80)\ny =(1-x**2/2)**-1\nplt.plot(x, y)\nplt.show()"
  },
  {
    "objectID": "posts/2024-01-03-WeberGibbsLiouville/2024-01-03-WeberGibbsLiouville.html#velocity-independant-densities",
    "href": "posts/2024-01-03-WeberGibbsLiouville/2024-01-03-WeberGibbsLiouville.html#velocity-independant-densities",
    "title": "Weber Hamiltonian Gibbs Liouville Measure on State Space.",
    "section": "Velocity Independant Densities",
    "text": "Velocity Independant Densities\nWe assume that \\(f_{,v}=0\\) identically. Then the basic equation simplifies to \\[ (f~\\mu_{\\text{eff}}^{-1}~\\frac{\\alpha}{c^2x^2}+f_{,x}) v =0.\\] But the left hand side vanishes iff \\[f~\\mu_{\\text{eff}}^{-1}~\\frac{\\alpha}{c^2x^2}+f_{,x}=0,\\] which rearranges into the logarithmic derivative \\[d_x \\log f = \\frac{f_{,x}}{f} = -\\mu_{\\text{eff}}^{-1}~\\frac{\\alpha}{c^2 x^2}.\\] The definition of \\(\\mu_{\\text{eff}}=\\mu-\\frac{\\alpha}{c^2 x}\\) implies \\[d_x \\log f = \\frac{1}{x} + \\frac{\\beta}{1-\\beta x}\\] where \\(\\beta:=\\mu c^2 /\\alpha\\).\nTaking antiderivatives we find \\[\\log f = \\log x - \\log (1-\\beta x) + C =\\log \\frac{x}{1-\\beta x}+C\\] where \\(C\\) is a constant of integration.\nIf we assume initial conditions \\(f(1,v)=1\\) independantly of \\(v\\), then \\(C=\\log(1-\\beta)\\). Therefore \\(f = (1-\\beta) \\frac{x}{1-\\beta x}\\) is the unique solution of the basic equation such that \\(f_{,v}=0\\) identically and \\(f(1)=1\\).\nTherefore we obtain:\n\nProposition: The density \\[\\rho = (1-\\beta)~\\frac{x}{1-\\beta x} ~dx dv\\] is the unique invariant measure on state space \\(\\{(x,v)~~|~x&gt;0\\}\\) which is velocity independant and satisfies \\(\\rho(1,v)=1\\) identically.\n\nWe should emphasize that this derivation is somewhat specific to \\(xv\\) being strictly a two-dimensional state space. For applications we are of course interested in \\(N\\)-body electrodynamic systems, and this will be discussed in sequel.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nbeta0 = .1\nbeta1= - 10**2\n\nx = np.linspace(0, 1000, 100)\ny =1-  (1-beta0)*x*(1-beta0*x)\nplt.plot(x, y)\nplt.show()"
  },
  {
    "objectID": "posts/2024-01-03-WeberGibbsLiouville/2024-01-03-WeberGibbsLiouville.html#conclusion",
    "href": "posts/2024-01-03-WeberGibbsLiouville/2024-01-03-WeberGibbsLiouville.html#conclusion",
    "title": "Weber Hamiltonian Gibbs Liouville Measure on State Space.",
    "section": "Conclusion",
    "text": "Conclusion\nSo we derive the invariant measure! This allows us to define the entropy of measures relative to \\(\\rho dxdv\\). From here we can begin to study a Weberian approach to the Second “Law” of Thermodynamics.\n[ To be continued – JHM]"
  },
  {
    "objectID": "posts/2024-01-04-FibonacciComplexity/index.html",
    "href": "posts/2024-01-04-FibonacciComplexity/index.html",
    "title": "Computational Complexity of Fibonacci Sequences.",
    "section": "",
    "text": "This article was originally written circa October 2022. [-JHM]\nCraig Alan Feinstein’s papers on \\(P\\) not equal to \\(NP\\) have caused us to review algorithms and complexity. We study “computational complexity” from the pragmatic perspective. This in contrast to “theoretical” computation results, which is a contradiction in terms.\nHere we consider the question”:“What is the computational complexity of computing the \\(n\\)th Fibonacci number?"
  },
  {
    "objectID": "posts/2024-01-04-FibonacciComplexity/index.html#naive-fibonacci-computation",
    "href": "posts/2024-01-04-FibonacciComplexity/index.html#naive-fibonacci-computation",
    "title": "Computational Complexity of Fibonacci Sequences.",
    "section": "Naive Fibonacci Computation",
    "text": "Naive Fibonacci Computation\nRecall that the Fibonacci sequence is a sequence of integers \\(f_0, f_1, f_2, \\ldots\\) defined recursively by: \\[f_0=1, ~~ f_1=1, ~~\\text{and}~~ f_{n+1}=f_n+f_{n-1}~~\\text{for}~~ n\\geq 1.\\]\nTo naively compute \\(f_{2022}\\) would require we repeatedly apply the recursive definition, finding \\(f_[2022}:=f_{2021}+f_{2020}\\), etc.. This procedure requires we compute all the values \\(f_k\\) for \\(k&lt; 2022\\).\nThe pseudocode for this naive procedure is based on “memoization”. If we apply the above recursive formula to evaluate \\(f_{2022}\\), then we’ll be reusing the values \\(f_k\\) for \\(k &lt; 2022\\). Memoization is simply a device for recording and reusing these intermediate values. So to compute \\(f_{2022}\\) we will need to store the previous values, although there is a method (Bottom-Up evaluation) which is similar, and which inductively evaluates \\(f_1\\), \\(f_2\\), \\(f_3\\), etc., until we reach \\(f_{2022}\\).\nSo if we compute \\(f_{2022}\\) and memoize all the terms \\(m=\\{f_1, f_2, f_3, \\ldots\\}\\), then the memo \\(m\\) grows unbounded in length. Some authors modify the memo to only contain the two “largest” elements, thus reducing the length of the list. This would require updating the memo \\[\\{f_1, f_2\\} \\leadsto \\{f_2, f_3\\} \\leadsto \\{f_3, f_4\\} \\leadsto \\cdots \\leadsto \\{f_{n-1}, f_n\\} \\leadsto \\cdots.\\] But still the number of bytes necessary to represent the fibonacci elements grows exponentially with \\(n\\) since \\(f_{n+1} \\approx \\varphi^n\\) where \\(\\varphi\\) is the golden ratio \\(\\varphi=\\frac{1+\\sqrt{5}}{2} \\approx 1.618033\\).\nThe memoization gives a time complexity of \\(O(n)\\) to compute \\(f_n\\). But memoization requires memory to store the memo! And we think this needs to be accounted for in the computational complexity, since obviously it’s an essential part of the computation. Indeed computation as represented by Turing machines, say, involves motion and read/write/react at different locations on the “tape”. It seems that a proper accounting of complexity should involve every time-step, namely every motion and site operation. In analyzing the complexity of the usual \\(O(n)\\) complexity of computing \\(f_n\\), the action of calling and retrieving the data in the memo are considered to be constant.\n\n## Code taken from https://stackoverflow.com/a/61604929/17406611\n\nfib_cache = {0 : 0, 1 : 1}\n\ndef fib(n):\n    if n in fib_cache:\n        return fib_cache[n]\n    for i in range(100, n, 100):\n      fib(i)\n    fib_cache[n] = result = fib(n-1) + fib(n-2)\n    # print(fib_cache) # &lt;--this line is diagnostic.\n    return result\n\n  # There's interesting comment on why the range(100, n, 100) line is important\n  # to avoid a \"recursive depth error\". The point is that the memo needs to be filled\n  # as the algorithm develops, otherwise it recurses too far and returns an error!\n\n\n# crashes the RAM\n# fib(1111111)\n\nfib(11111)\n\n515449135231559341621591189426925989418721609167804403107087312453694294479381404009230187092526675635022414542794904158934368158350216751867828729213516067642461147232232268304004580596220514978296704097915617589481297010691749471374967376304898014174716132125169572206688299944881902864940487579850754037243411123276226268998274067232370656873037885028569262362061989878439356579125363739644638605976667733232134130196207453194213358616463005487086631652051025004934485196108344869244852506414543015664379038338857611347469102943415360234480491921571800239803284078147859161629100936007246749423449323827401152142684138017539299637210829955409666781554035555164800825902557894478979141680264821730580654526990976167873657740460977594388216677737796493623940501749951993194553070912364327001564086186444836587037180810655016948562608480145057901528396467327800369394725748856906177005886944277609832795419082424474419033931754123200248752600310587761729439189440527073799320938597514569967706344559861576816209214912702962065352672071494639021231263782338510241176219316180788341549329905272081790433223099472061887224254193326845457247107409050092252994931668934553671721376871177693036352993174131508107261653495025271505086039171034392185521383307925723081097129536244468148375789733582131797176285225457221029865658845503179175504307577930140222583997281098099332145783930418777810346276337273420733754768191403158839413163368990092771464626510432292314209966950363068432367028332284209840897425718364670733733609565321893240873729315360915814803137552560521106490937691421540344502423323064743545226360364012549367167257038202145921861042955299329942301124074181956428710271876930526019606797077558959445434943166179407403375284366340173639269807373108055388080201746447050804598946499248800891171987624229020766742994219485280547337990630263452898332213470171667603200991268579583095661682595442200149483262133621860660302141160974707437100532341443580636798210704649175613121627855118061762876137389590812891131603206815601843823369210865672605256743142632199819790960079549275267815983406188538500072911327187912330503064073869613282412315795790671452556371408354045852898125070873750632615799469070245720036053071341314252092446074260578417947762296896685389368685659291620443322232933074723685001342680075497489"
  },
  {
    "objectID": "posts/2024-01-04-FibonacciComplexity/index.html#wolframs-nondefinition-of-computational-irreducibility.",
    "href": "posts/2024-01-04-FibonacciComplexity/index.html#wolframs-nondefinition-of-computational-irreducibility.",
    "title": "Computational Complexity of Fibonacci Sequences.",
    "section": "Wolfram’s (non)Definition of Computational Irreducibility.",
    "text": "Wolfram’s (non)Definition of Computational Irreducibility.\nIn Wolfram’s book “A New Kind of Science”, there is introduced the idea of computational irreducibility. We quote from Chapter 13, pp.739-740.\n[Begin quote:]\n“Computational irreducibility leads to a much more fundamental problem with prediction. For it implies that even if in principle one has all the information one needs to work out how some particular system will behave, it can still take an irreducible amount of computational work actually [sic] to do this.\nIndeed, whenever computational irreducibility exists in a system it means that in effect there can be no way to predict how the system will behave except by going through almost as many steps of computation as the evolution of the system itself.\nIn traditional science it has rarely even been recognized that there is a need to consider how systems that are used to make predictions actually operate.\nBut what leads to the phenomenon of computational irreducibility is that there is in fact always a fundamental competition between systems used to make predictions and systems whose behaviour one tries to predict.\nFor if meaningful general predictions are to be possible, it must at some level be the case that the system making the predictions be able to outrun* the system it is trying to predict. But for this to happen the system making the predictions must be able to perform more sophisticated computations than the system is trying to predict.*\nBut the remarkable assertion that the Principle of Computational Equivalence makes is that this assumption is not correct, and that in fact almost any system whose behaviour is not obviously simple performs computations that are in the end exactly equivalent in their sophistication.\nSo what this means is that systems one uses to make predictions cannot be expected to do computations that are any more sophisticated than the computations that occur in all sorts of systems whose behaviour we might try to predict. And from this it follows that for many systems no systematic prediction can be done, so that there is no general way to shortcut their process of evolution, and as a result their behaviour must be considered computationally irreducible.”\n[End quote.]\nHere’s what we like about Wolfram’s idea of “computational irreducibility”. Firstly it aligns with experience. Mathematicians who actually compute things know there’s no shortcuts for most operations. It’s very difficult to add two numbers \\(a+b\\) without actually “adding” them arithmetically. This relates to Craig Alan Feinstein’s arguments that \\(P \\neq NP\\), and specifically that the subset-sum problem is computationally irreducible. To decide whether a set of signed integers \\(S\\) contains a zero subset-sum, there really is nothing we can do except exhaustively search through all the subsets and add the elements. But most theoretical computer scientists “dream” of somehow there existing a shortcut, a black box, a sophisticated magical algorithm that will compute the sums without actually computing them. So Wolfram’s idea of “computational irreducibility” is in opposition to the theoretical computer scientists’ optimism.\nBut what’s lacking is any rigorous proof or definition of computational irreducibility. If I had to make a pseudo-definition (intuitive) I would first say:\n\na computation is the evaluation of a function \\(f: X \\to Y\\), and the evaluation \\(f\\) is computationally irreducible if the complexity of any equivalent composition \\(g: X \\to Z\\) and \\(h: Z \\to Y\\) with \\(f = h \\circ g\\) satisfies \\[c(f) \\leq c(h) + c(g),\\] where \\(c(f)\\) represents the “computational complexity” of evaluating a function \\(f\\).\n\nHowever the complexity \\(c(f)\\) is not well-defined at this point.\nBut the idea is that a function \\(f\\) always admits (nonunique!) compositions \\(f=h \\circ g\\), and the question is whether these compositions \\(h, g\\) can be any simpler than the evaluation of \\(f\\) itself!\nThe above is not sufficiently rigorous since we have not defined \\(c(f)\\). Moreover \\(c\\) is not defined on the category of functinons, since \\(f=h\\circ g\\) are identical functions from the categorical viewpoint. Our point is simply that identical functions are not necessarily identical computations!\nFor example, Wolfram would argue that the Collatz function arising in the Collatz conjecture \\(3x+1\\) problem is computationally irreducible. There are no shortcuts for evaluating the Collatz function, and therefore there will never be a proof of Collatz’ conjecture. This is why no mathematician should study the problem, because the results/outputs of the Collatz function cannot be logically established, they can only be empirically established by directly evaluating the Collatz function using the definition."
  },
  {
    "objectID": "posts/2024-01-04-FibonacciComplexity/index.html#topological-irreducibility",
    "href": "posts/2024-01-04-FibonacciComplexity/index.html#topological-irreducibility",
    "title": "Computational Complexity of Fibonacci Sequences.",
    "section": "Topological Irreducibility",
    "text": "Topological Irreducibility\nLet’s examine the word irreducibility somewhat more. Wikipedia has many references, but the primary definition (for our perspective) is the topological irreducibility. Here is the definition from Wikipedia\nDefinition: (Reducible and Irreducible Topological Spaces) A topological space \\(X\\) is reducible if it can be written as a union $ X= X_1 X_2$ of two closed proper subsets \\(X_1, X_2\\) of \\(X\\).\nDefinition: A topological space \\(X\\) is irreducible (or hyperconnected) if it is not reducible. Equivalently, if all non empty open subsets of X are dense, or if any two nonempty open sets have nonempty intersection.\nSo the computational aspect of irreducibility would involve computing intersections of nontrivial open subsets of the space. Formally it would be better to provide the relative definition of irreducibility for subsets \\(A \\subset X\\), but we just use the subspace topology on \\(A\\) in \\(X\\). I.e., open subsets of \\(A\\) are defined as intersections \\(U \\cap A\\) of open subsets \\(U\\) of \\(X\\). It’s useful to introduce the relative definition because our first idea for defining computational complexity is to consider the topology of the graph of the function in question, i.e. \\(graph(f) \\subset X \\times Y\\). Now if we have a composition \\(f = h \\circ g\\), then we obtain two graphs(!) namely \\[graph(h) \\subset Z \\times Y\\] and \\[graph(g)\\subset X \\times Z.\\]\nI suppose it would be interesting to determine whether the graph of the Collatz function is an irreducible topological subset of the product \\(\\bf{N} \\times \\bf{N}\\). And here we need specific the topology of \\(\\bf{N}\\), which is not so obvious, although perhaps the discrete topology is the most natural.\nTo this point we haven’t proved anything new, we’ve only conceptualized the problem, trying to more formally define “irreducibility”. We have yet to define the complexity \\(c(f)\\) of a function although our idea is that \\(c(f)\\) should reflect a property of \\(graph(f)\\). Dependancy on the graph of \\(f\\) allows us to compare compositions \\(f=h\\circ g\\) and maybe develop comparisons between \\(c(f)\\), \\(c(g)\\), \\(c(h)\\), etc..\nBut the point is that we have a strategy now, and we can consult the literature by looking for techniques to prove the irreducibility of topological spaces. This is a general problem, and we can even begin with the more specialized case of algebraic geometry.\nIn algebraic geometry, if a variety is irreducible then what does that mean in concrete terms for the equations defining the variety? For hypersurfaces it means the variety can be described by two equations instead of one. For example a polynomial \\(p(x)\\) is reducible if \\(p(x)=q_1(x) . q_2(x)\\) for nontrivial polynomials \\(q_1, q_2\\). What’s interesting for polynomials is that the degrees of the products are strictly smaller, namely \\[deg(p) = deq(q_1)+deg(q_2).\\]\nBut let’s hypothesize that the complexity of evaluating a degree \\(d\\) polynomial is \\(d\\). Then the complexity of computing \\(p=q_1 q_2\\) is not \\(deg(p)\\) but actually \\[\\max\\{ deg(q_1), deg(q_2) \\}.\\] This is assuming that the product \\(p=q_1 . q_2\\) has constant time complexity, which is reasonable for real or complex numbers. So this is elementary examples of how topological reducibility relates to computational complexity. And this seems to be the beginning of maybe something more interesting."
  },
  {
    "objectID": "posts/2024-01-04-FibonacciComplexity/index.html#complexity-of-exponentiation",
    "href": "posts/2024-01-04-FibonacciComplexity/index.html#complexity-of-exponentiation",
    "title": "Computational Complexity of Fibonacci Sequences.",
    "section": "Complexity of Exponentiation",
    "text": "Complexity of Exponentiation\nThere is a recurring idea in evaluating the complexity of exponentiations, and it’s based on taking successive squares based on the binary representations. This is where the \\(O(log_2 ~n)\\) complexity algorithm for evaluating \\(f_n\\) is derived. This speedup depends on the fact that the rule for defining the fibonacci sequence is linear and static, namely the infinite sequence of rules \\(f_{n+1}=f_n+f_{n-1}\\) for \\(n\\geq 1\\) is almost only one rule \\[f_{*+1} = f_* + f_{*-1}.\\]\nThis relates to the observation that iterated matrix powers of \\[Q=\\begin{pmatrix}\n1 & 1 \\\\\n1 & 0 \\end{pmatrix} \\] generate the Fibonacci sequence. Specifically we have the identity \\[\n\\begin{pmatrix}\nf_{n+1} & f_n \\\\\nf_n & f_{n-1} \\end{pmatrix} = \\begin{pmatrix}\n1 & 1 \\\\\n1 & 0 \\end{pmatrix}^n.\n\\]\nThis identity has alot of different explanations, but it suggests the following “speedup” for computing \\(f_n\\) for \\(n\\geq 2\\). For example, to compute \\(f(16)=f(2^4)\\) we can evaluate \\[Q, ~~~~~Q^2, ~~~~ Q^4=(Q^2)^2, ~~~~ Q^8=(Q^4)^2.\\] And this successive squaring of the previous result allows a \\(log_2(n)\\) speedup of the Fibonacci computation.\nLikewise \\(f(15)\\) requires we compute all the powers \\(Q, Q^2, Q^4, Q^8\\) and then take the product \\[Q.Q^2.Q^4.Q^8=Q^{15}.\\] This represents somewhat the “worst case”. Notice that here we need to store the powers \\(Q^{2^k}\\), \\(k&gt;0\\), in memory and then multiply these powers together. So what precisely is the complexity? We find it curious how the complexity of matrix multiplication remains an open question in computer science. There are various speedups, although the naive complexity is \\(O(n^3)\\) although speedups to \\(O(n^{2+\\epsilon})\\) are possible for some specific small values of \\(\\epsilon\\). We’ll let \\(\\tilde{m}\\) represent the complexity of multiplication of two matrices.\nSo the total complexity is counted like:\n\nComplexity of evaluating \\(log_2(n)\\) matrix powers is \\(\\tilde{m}. log_2(n)\\) time steps.\nComplexity of \\(log_2(n)\\) matrix multiplications is \\(\\tilde{m}. log_2(n)\\) time steps.\nTotal Complexity is the sum of the complexities in 1. and 2., and therefore equal to \\(\\tilde{m}. log_2(n)\\), or \\(O(log_2n)\\)\n\nAre the methods of computing the fibonacci sequence really different? Obviously one appears to be faster, requiring nominally less time steps or elementary operations. However there are increasing memory constraints arising from the matrix form, having to store the powers \\(Q^{2^k}\\), and where of course the entries are becoming super exponentially large."
  },
  {
    "objectID": "posts/2024-01-04-FibonacciComplexity/index.html#so-what",
    "href": "posts/2024-01-04-FibonacciComplexity/index.html#so-what",
    "title": "Computational Complexity of Fibonacci Sequences.",
    "section": "So What?",
    "text": "So What?\nFair question. But as we said, this is the beginning of maybe something deeper. So we need to compare the two algorithms, and perhaps address whether the final \\(O(log_2~n)\\) is really irreducible in Wolfram’s sense.\n[To be continued … -JHM]"
  }
]