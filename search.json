[
  {
    "objectID": "posts/2024-01-24-GRCritiqueProperTime/index.html",
    "href": "posts/2024-01-24-GRCritiqueProperTime/index.html",
    "title": "What is Proper Time in Special Relativity?",
    "section": "",
    "text": "This post discusses “proper time” as defined by the special relativity theorists. Briefly we are not persuaded that this quantity denoted \\(d\\tau\\) represents a measure of “time” at all. The definition of proper time is a Lorentzian idea which involves a difference of squares. But the strict Riemannian geometer must understand that “differences of squares” do not represent “sums of squares”. Indeed how does a Riemannian geometer talk about “distances which satisfy reverse triangle inequalities” with a straight face? This implies that the Lorentzian “proper time” is not a measure of “time” at all, but a heterogeneous quantity relating Lorentz to Riemannian lengths. This is cause of much confusion in the literature.\nIt’s typically said that proper time is the “self time” as measured by an observer’s clock in the observer’s inertial reference frame. And this is a strange expression. We have discussed elsewhere “What is Time?” that “time is matter in motion” ex definitio. Therefore a “self time” is any periodic motion with a counter (wrist watch, pendulum, dog wagging it’s tail, comet appearing and disappearing, sun rising and setting). There is no assumption of uniformity in the motion, although for practical purposes this is useful. But special relativity deliberately undermines the possibility of “objective uniformity”. Clocks and measuring sticks are flexible and wobbly in the SR worldview (which view we do not share)."
  },
  {
    "objectID": "posts/2024-01-24-GRCritiqueProperTime/index.html#what-are-the-units-of-proper-time-dtau-in-special-relativity",
    "href": "posts/2024-01-24-GRCritiqueProperTime/index.html#what-are-the-units-of-proper-time-dtau-in-special-relativity",
    "title": "What is Proper Time in Special Relativity?",
    "section": "",
    "text": "This post discusses “proper time” as defined by the special relativity theorists. Briefly we are not persuaded that this quantity denoted \\(d\\tau\\) represents a measure of “time” at all. The definition of proper time is a Lorentzian idea which involves a difference of squares. But the strict Riemannian geometer must understand that “differences of squares” do not represent “sums of squares”. Indeed how does a Riemannian geometer talk about “distances which satisfy reverse triangle inequalities” with a straight face? This implies that the Lorentzian “proper time” is not a measure of “time” at all, but a heterogeneous quantity relating Lorentz to Riemannian lengths. This is cause of much confusion in the literature.\nIt’s typically said that proper time is the “self time” as measured by an observer’s clock in the observer’s inertial reference frame. And this is a strange expression. We have discussed elsewhere “What is Time?” that “time is matter in motion” ex definitio. Therefore a “self time” is any periodic motion with a counter (wrist watch, pendulum, dog wagging it’s tail, comet appearing and disappearing, sun rising and setting). There is no assumption of uniformity in the motion, although for practical purposes this is useful. But special relativity deliberately undermines the possibility of “objective uniformity”. Clocks and measuring sticks are flexible and wobbly in the SR worldview (which view we do not share)."
  },
  {
    "objectID": "posts/2024-01-24-GRCritiqueProperTime/index.html#lorentz-lengths-versus-riemannian-lengths",
    "href": "posts/2024-01-24-GRCritiqueProperTime/index.html#lorentz-lengths-versus-riemannian-lengths",
    "title": "What is Proper Time in Special Relativity?",
    "section": "Lorentz Lengths versus Riemannian Lengths",
    "text": "Lorentz Lengths versus Riemannian Lengths\nConsider the Lorentz Minkowski line element \\[g=ds^2=-c^2dt^2+dx^2+dy^2+dz^2.\\] In our critical analysis of special relativity we have warned readers from naively applying Riemannian ideas to Lorentzian metrics. We repeat: Lorentzian metrics are not Riemannian! When a Riemannian geometer starts working with Lorentz metrics, they typically begin with the line element \\(ds':=\\sqrt{-ds^2}\\) on timelike curves \\(\\gamma\\) where \\(ds&lt;0\\). By habit they refer to integrals \\(\\int_\\gamma ds'\\) as “lengths” along the curves \\(\\gamma\\). And they assume that this Lorentz length describes a Riemannian length. But this interpretation is misleading since Lorentz “lengths” satisfy the reverse triangle inequality, and thus “Lorentz lengths” are antithetical to “Riemannian length” This is trivial observation and repeatedly overlooked. Again we insist the measures represented by “Lorentz length” expressions \\(\\int_\\gamma ds'\\) are not “lengths” as understood by Riemannian geometry. Countless errors are introduced by confusing these two definitions.\nRelated to the Lorentz metric is the so-called “proper time” function on timelike curves, usually represented by a definition type formula \\(ds'=c d\\tau\\). But we find this “definition” insufficient and not strictly defining a “proper time”. Why? Because \\(d\\tau\\) does not have units of time. This is directly related to the error of interpreting \\(ds'\\) as a measure of “Riemannian length”. In otherwords, \\(d\\tau\\) formally represents a ratio of “Lorentz length” over “Riemannian length” multiplied by “time”, i.e. \\[[d\\tau] = \\frac{[\\text{Lorentz length}]}{[\\text{Riemannian length}]} [\\text{time}].\\]"
  },
  {
    "objectID": "posts/2024-01-24-GRCritiqueProperTime/index.html#sums-of-squares-versus-differences-of-squares-pythagoras-and-anti-pythagoras",
    "href": "posts/2024-01-24-GRCritiqueProperTime/index.html#sums-of-squares-versus-differences-of-squares-pythagoras-and-anti-pythagoras",
    "title": "What is Proper Time in Special Relativity?",
    "section": "Sums of Squares versus Differences of Squares: Pythagoras and Anti-Pythagoras",
    "text": "Sums of Squares versus Differences of Squares: Pythagoras and Anti-Pythagoras\nAs strange as it first sounds, we believe it’s a significant mistake to believe \\(d\\tau\\) has physical units of time. The fundamental problem is again that \\(ds'\\) is emphatically not a measure of metric distance. Indeed the Pythagorean theorem involves sum of squares and and is inherently Riemannian. But Lorentz lengths are represented by differences of squares, and there is no “Pythagoras” in this setting. In euclidean geometry, the squareroot of a sum of orthogonal squares represents a length because of Pythagorean identity. There are right angled triangles to be constructed whose hypotenuses are the lengths in question. However the squareroot of a sum of signed squares is not readily identified as a length to the Pythagorean, even if one assumes the squares are orthogonal. There are no right angled triangles to draw except on the null cone \\(ds^2=0\\) where the Pythagorean \\[c^2 dt^2=dx^2+dy^2+dz^2\\] is equivalently the hypothesis on the velocity of \\(c\\) in vacuum. But on the null cone the proper time \\(d\\tau\\) is identically zero."
  },
  {
    "objectID": "posts/2024-01-24-GRCritiqueProperTime/index.html#brief-remark-on-diagonalized-rest-frames",
    "href": "posts/2024-01-24-GRCritiqueProperTime/index.html#brief-remark-on-diagonalized-rest-frames",
    "title": "What is Proper Time in Special Relativity?",
    "section": "Brief Remark on Diagonalized Rest Frames",
    "text": "Brief Remark on Diagonalized Rest Frames\n\\(\\newcommand{\\del}{\\partial}\\)\nHere we include a frequent argument in the general relativity, alleging to establish the identity \\(ds'=c d\\tau\\) where \\(d\\tau\\) represents the so-called “proper time” of an inertial observer as measured by their local “clock”. This interpretation comes from the use of a so-called “instantaneous rest frame”. This requires the observer to find coordinates \\((\\tau, \\xi, \\eta, \\zeta)\\) where \\(\\tau\\) represents “time” and all the partial derivatives vanish: \\[\\frac{\\del \\xi}{\\del \\tau}=\\frac{\\del \\eta}{\\del \\tau}=\\frac{\\del \\zeta}{\\del \\tau}=0.\\] In this particular coordinate system one finds \\(ds^2=-c^2d\\tau^2\\) and \\(ds'=\\sqrt{-ds^2}=c d\\tau\\).\nBut can we really conclude that \\(ds'\\) has units of \\([length]\\) as a general rule, or equivalently that \\(ds'/c\\) has units of \\([time]\\), based on a computation in one coordinate system? We observe that the system of equations is not tensorial since the partial derivatives \\(\\del \\xi/ \\del \\tau\\) are not covariant. However the vanishing of the covariant derivatives is an invariant quantity, i.e. invariant with respect to change of variable rather than covariant (contravariant) with respect to change of variables. Replacing the partial derivatives with covariant derivatives, the vanishing no longer implies the coordinates \\(\\tau, \\xi, \\eta, \\zeta\\) can be integrated into the “diagonal” form for \\(g\\). Therefore we are not really persuaded by this argument’s identification of “proper time”, and nonetheless the derivation still has the inherent problem of not representing “proper time” in units of “time”."
  },
  {
    "objectID": "posts/2024-01-24-GRCritiqueProperTime/index.html#carets-and-non-tensors",
    "href": "posts/2024-01-24-GRCritiqueProperTime/index.html#carets-and-non-tensors",
    "title": "What is Proper Time in Special Relativity?",
    "section": "Carets and Non Tensors",
    "text": "Carets and Non Tensors\nThe “mere coordinate singularities”, so named by Misner-Thorne-Wheeler in their influential textbook on “Gravitation” leads to their introduction of “geometrodynamics” and “carets” in the GR. An introduction to these ideas are discussed in (Klauber 2001) and briefly in (thorne2000gravitation?). The caret argument says: although the components of tensors are noninvariant, the projections of these tensors onto coordinate components in a “local Lorentz frame” are well defined, observable and measurable. Thus carets are the coordinates of tensors in the so-called local Lorentz frame.\nThis idea of taking projections in a local Lorentz frame is curious, and though perhaps intuitively appealing, the mathematics is not clear. For fixed spatial coordinates \\(\\hat{x},\\hat{y},\\hat{z}\\) we know from Riemannian geometry how to construct the projections of an arbitrary vector \\(v\\) onto the basis vectors, i.e. \\(\\hat{x}\\cdot v\\). But there is no projection onto the time variable, i.e. \\(t\\) and \\(\\hat{t}\\) has no meaning. There really is no objective length or magnitude or “unit” of time.\nIn practice MTW (thorne2000gravitation?) take projections by dividing and renormalizing “component wise” in the tensors, but we claim this is ad hoc procedure and only convenient when the metric is diagonal. The same argument is encountered in (Klauber 2001). Most mathematicians/physicists are not persuaded by these critiques. The projection onto coordinates axes seems to be inspired by projections in quantum mechanics. In the Born-Dirac Copenhagen interpretation, the projections amount to “observations”. The carets of MTW attempt to develop a parallel idea in GR.\nMany will disagree here, but it depends on “Who taught you GR and how were you taught?” There appears to be no large-scale consensus anyway, and we welcome comments to the contrary.\n[-JHM]"
  },
  {
    "objectID": "posts/2024-01-19-WhatIsTime/index.html",
    "href": "posts/2024-01-19-WhatIsTime/index.html",
    "title": "What is Time?",
    "section": "",
    "text": "Question: What is time?\nAnswer: Time is matter in motion.\nQuestion: What is motion?\nAnswer: Motion is change, i.e. changes in position and velocity.\nQuestion: What does it mean for “time to stop?”\nAnswer: Time stops when all motions stops, i.e. time stops when matter stops changing. Time stops when everything is at absolute zero Kelvin and there is zero energy in the universe.\nQuestion: Is there a direction for time, like past, present, and future?\nAnswer: Direction refers to position and motion. Only matter has position and motion. Time has no motion and no direction. Time is what matter does. The qualities and attributes of time are strictly derived from attributes of “matter in motion”. The past are those motions which we have observed. The present consists of the motions we see now. The future is the motions that we will see.\nQuestion: Is time travel possible?\nAnswer: Time is not a position or location. Time travel would presumably mean moving from one state of matter in motion to another state of matter in motion. This happens all the time everyday at every moment everywhere we look. Motion is moving. But we cannot interact with all matter in all states of motion. However we can interfere locally and can do work on isolated systems. Therefore we say “time travel is possible in isolated systems”. In isolated systems, we can bring in energy from the environment and do work on the system and thereby transform states. To travel in time means working on the system to change a present state into a past state. There is nothing mysterious here, we are simply saying that “winding the hands of a clock” is work which time travels the clock back to a previous state. But winding the clock does work only on the clock and not on the environment. In the isolated system consisting of only the clock, we have time travelled. But we have not done any work on the larger environment and therefore have not time travelled in any wider sense."
  },
  {
    "objectID": "posts/2024-01-19-WhatIsTime/index.html#what-is-time",
    "href": "posts/2024-01-19-WhatIsTime/index.html#what-is-time",
    "title": "What is Time?",
    "section": "",
    "text": "Question: What is time?\nAnswer: Time is matter in motion.\nQuestion: What is motion?\nAnswer: Motion is change, i.e. changes in position and velocity.\nQuestion: What does it mean for “time to stop?”\nAnswer: Time stops when all motions stops, i.e. time stops when matter stops changing. Time stops when everything is at absolute zero Kelvin and there is zero energy in the universe.\nQuestion: Is there a direction for time, like past, present, and future?\nAnswer: Direction refers to position and motion. Only matter has position and motion. Time has no motion and no direction. Time is what matter does. The qualities and attributes of time are strictly derived from attributes of “matter in motion”. The past are those motions which we have observed. The present consists of the motions we see now. The future is the motions that we will see.\nQuestion: Is time travel possible?\nAnswer: Time is not a position or location. Time travel would presumably mean moving from one state of matter in motion to another state of matter in motion. This happens all the time everyday at every moment everywhere we look. Motion is moving. But we cannot interact with all matter in all states of motion. However we can interfere locally and can do work on isolated systems. Therefore we say “time travel is possible in isolated systems”. In isolated systems, we can bring in energy from the environment and do work on the system and thereby transform states. To travel in time means working on the system to change a present state into a past state. There is nothing mysterious here, we are simply saying that “winding the hands of a clock” is work which time travels the clock back to a previous state. But winding the clock does work only on the clock and not on the environment. In the isolated system consisting of only the clock, we have time travelled. But we have not done any work on the larger environment and therefore have not time travelled in any wider sense."
  },
  {
    "objectID": "posts/2024-01-19-WhatIsTime/index.html#gps",
    "href": "posts/2024-01-19-WhatIsTime/index.html#gps",
    "title": "What is Time?",
    "section": "GPS",
    "text": "GPS\nWe have a question: “Do Lorentz transformations and Einstein’s special relativity actually enter into real world GPS calculations?” Or do we simply use classical Newtonian physics combined with expectation that \\(c\\) really is the speed of light propagation in vacuum? This is like classical Maxwellian theory. We don’t know. And we really wonder if these calculations are accurate or not. Nobody would really know. Is a detailed study of GPS necessary or profitable?\nReviewing the basic history of navigation, latitude and longitude, it’s interesting to first consider the latitude problem, which historically was much simpler to solve. Here the navigator takes advantage of the “fixed” northern star, and measuring the angle from the horizontal to the northern star we find a measure \\(\\theta\\) of the latitude, i.e. our angular distance from the Earth’s equator. It’s interesting to see how errors \\(\\Delta \\theta\\) compound the distance errors. This uses the fundamental definitions of spherical distances, i.e. \\(\\Delta C = R_{\\text{Earth}} \\cdot \\Delta theta\\).\nNote: this method allows us to construct the latitude coordinate. This is not quite a distance measurement. Given two points \\(x,y\\), we could measure the angles \\(\\theta_x\\), \\(\\theta_y\\) from the horizontal to the fixed north star. But the distance \\(dist(x,y)\\) is underdetermined without another measurement, and here enters the longitude problem. Longitude requires a \\(\\Delta t\\) computation, i.e. relative time difference between sunsets at Greenwich and at sea. The history of the longitude problem is quite amazing…"
  },
  {
    "objectID": "posts/2024-01-19-WhatIsTime/index.html#what-is-a-clock",
    "href": "posts/2024-01-19-WhatIsTime/index.html#what-is-a-clock",
    "title": "What is Time?",
    "section": "What is a Clock?",
    "text": "What is a Clock?\n*What is a clock?\nAccording to David Wineland’s lecture a clock consists of a periodic event generator and a counter. This definition might appear circular unless one adopts Mach’s view that time is matter in motion. The periodic event generator, whatever it is, is precisely the motion which is defining the clock. Whether the motion is regular or irregular, whatever that means, doesn’t matter. Even a broken clock tells a time at all times. And it’s always right from it’s own perspective.\nWhat are regular time intervals?\nWhen the matter in motion is periodic, and also when the periods appear relatively identical and indistinguishable, then we consider the motion to be regular or uniform. For example in determining the longitude, it’s assumed that the sun rises and sets regularly along the Greenwich meridian line. This is not strictly the case, as known by the ancients by the precession of the equinox and the great year.\nAre all periodic motions candidates for clocks?\nBriefly, yes. The simplest periodic systems are the spring, and the simple pendulum. In measuring longitude, the problem is the interaction of these periodic motions with the environment. For example, the simple pendulum is affected by the motion of the ocean waves. The longitude problem required a stable accurate clock for sea vessels on long voyages in deep waters.\nMoreover simple pendulums are affected by the gravitational potential, therefore the periodicity of the simple pendulum is affected during high altitude flights. Assis makes this point clearly in (Assis 1999) where the periodicity of the pendulum is well known to depend on the gravitational constant \\(g\\).\nThe problem of stable clocks unaffected by the motion and bulk interaction with the oceans is very interesting. We should assume that it’s a fundamental problem. We like to imagine the pendulum as an isolated system, but this idealization is disturbed by the reality of the pendulum’s interaction with the environment.\nWe think it’s important to study the basic examples of oscillatory motion, because if the Weber-Sansbury model is correct, then oscillatory atomic motions are modelled precisely on electrical planetary models. Furthermore Sansbury emphasizes Bohr’s idea that Planck’s spring oscillators can be seen as planetary orbits when seen “along an axis”. See (Sansbury, n.d.), (Sansbury 2012)."
  },
  {
    "objectID": "posts/2024-01-19-WhatIsTime/index.html#simple-pendulum",
    "href": "posts/2024-01-19-WhatIsTime/index.html#simple-pendulum",
    "title": "What is Time?",
    "section": "Simple Pendulum",
    "text": "Simple Pendulum\nExample. A small mass with inertial mass \\(m_i\\) and gravitational mass \\(m_g\\) attached to a string (not a spring). The period of the simple pendulum becomes \\[T_{period}=2\\pi \\sqrt{\\frac{m_i}{m_g} \\frac{\\ell}{g}}.\\] So it becomes evident that the period depends on \\(g\\).\nQuestion: For the instability of pendulums in boats, does the moving ocean cause variations in \\(g\\) or is there another effect? Likewise during high altitude flight navigation, with many electrodynamic interactions and huge differences in the gravitational potential. Of course Assis’ point is that time meanwhile is running the same in all cases. That is, pendulums and clocks don’t measure global time. But they define their own time.\nRemark. Experiments indicate that the half-lives of accelerated unstable radioactive mesons is much greater than the half-lives of mesons at rest in the laboratory. But what does this really demonstrate? We agree with Assis, Phipps, etc, that the experiments rather indicate the half lives depends on their accelerations and large velocities relative the stars at infinity, and on the strong electromagnetic fields being used to accelerate the mesons.\nLike the pendulum in the ocean, the problem is that everything interacts with everything all the time. There is no real shielding or isolated systems."
  },
  {
    "objectID": "posts/2024-01-15-RisksOfPrematureQuantumAdoption/index.html",
    "href": "posts/2024-01-15-RisksOfPrematureQuantumAdoption/index.html",
    "title": "Remark on the Inherent Double Risk of Premature Quantum Adoption",
    "section": "",
    "text": "We have to reflect on the risk inherent in rushing to market an untested product. Our premise is that classical encryption is secure, as secure as its ever been, with or without any development of so-called quantum computers. This is a consequence of \\(P\\) not equal to \\(NP\\) as we understand it from C.A. Feinstein. This article will be brief, and here’s our main points:\n\nPoint 1: The implementation of so-called quantum encryption schemes is not necessary. The classical encryption schemes have not been broken by any quantum algorithms.\nPoint 2: Even if quantum computers are supposed to “break” classical cryptography, there is still the problem of deciding whether any quantum encryption algorithm is provably quantum secure.\n\nQuantum computers are fashionable. Obviously there is a prestige and recognition associated with quantum physics. Thus public institutions feel pressured to implement quantum algorithms as if they are “state of the art”. But the so-called quantum algorithms remain highly experimental. Here we are speaking about the physical implementations. Not the linear algebra as implemented on Qiskit. The linear algebra is perfect, but the engineers working on Qiskit Pulse are the hardest working technical team on the planet. Indeed these are the engineers that have to actually physically implement Rabi oscillations at specified wavelengths for specific time intervals, and there is alot of reference to “simultaneity” in their works. The Qiskit Pulse engineers have a myriad of “behind the curtain” problems, and the Wizard of Oz is frantically at work behind the curtains. Thus we see no urgency to replace classical protocols with experimental “beta” versions.\n\n\n\nQiskit Pulse are Wizards Behind the Curtain. Brilliant Team of Engineers Trying to Solve the Impossible!\n\n\nNotice the double pressure: quantum computers simultaneously claim to threaten classical encryption, yet also simultaneously claim to give improved security over classical encryption. Therefore organizations are doubly prompted to adopt these experimental beta protocols.\nBut again we argue there is no indication of any “quantum speedup”. The accounting by which quantum computers measure “complexity” is not physical and does not coincide with the classical measure of complexity and resources. The quantum complexity of a quantum gate cannot be counted as a fixed unit. We argue the correct complexity of a gate is the size of its support. Therefore we are not persuaded by the supposed complexity of Shor’s algorithm in breaking RSA encryption and factoring large numbers. In other words, the complexity of Hadamard gates cannot be counted as a unit, but again rather the log of the support. This is contrary to most computer scientist opinion. In practice it means classical RSA should not surrender it’s flagship position as secure and unbeatable, quantum or no quantum.\nThe fundamental error, we argue, in the quantum interpretation is their miscounting the complexity of superpositions, images of Hadamard gates, as consisting of a single state. Yes, it’s a single state, but it’s a single nonpure state, i.e. a state with large support! We think this point is consistently overlooked in the quantum computer literature. Therefore superposition gates are more complex than identity gates.\nThis is the point of our critique of Deutsch’s algorithm as discussed in (Nielsen and Chuang 2010, 1.4.3–4). Deutsch’s algorithm looks to solve the following decision problem: Given a deterministic binary function \\(f: \\{ 0,1 \\} \\to \\{ 0,1 \\}\\), then how many resources are required to decide whether \\(f\\) is a constant or non constant function. In otherwords, deterministically decide whether \\(f(0)=f(1)\\) or \\(f(0)\\neq f(1)\\). The question is basically whether the complexity is equal to two units or one unit. (“\\(2\\) versus \\(1\\)”).\nClassically it seems clear that equality “\\(=\\)” is a binary function, and therefore evaluation requires a single binary argument be given. This implies two evaluations of \\(f\\) are required. Deutsch and the “standard” quantum interpretation is that superposition allows for only a single call to \\(f\\). Specifically, they claim to define a unitary operator \\(U_f\\) defined by \\[U_f:|x\\rangle |y\\rangle \\mapsto |x \\rangle |y\\oplus f(x)\\rangle,\\] then pre- and post-composing with Hadamard-type gates they find a unitary transformation which concentrates on a state with the algebraic form \\(\\pm |f(0) \\oplus f(1) \\rangle (|0\\rangle - |1 \\rangle)\\). We don’t think the conclusion of a single call to \\(f\\) is justified, and we maintain the classical position that the question is irreducible, and there is no escaping the fact that \\(f(0)\\) and \\(f(1)\\) need to be evaluated. This requires two function evaluations instead of Deutsch’s alleged one evaluation.\nWhat is written above is this author’s opinion drawn from scientific study. But what’s important is experiment. Is there any way to really test whether Deutsch’s algorithm is actually twice as fast as the classical algorithm?\nOur critique of Deutsch’s algorithm naturally extends to a critique of so-called “quantum parallelism”, for example as introduced in this interesting reference.\n[-JHM]\n\n\n\n\nReferences\n\nNielsen, Michael A, and Isaac L Chuang. 2010. Quantum Computation and Quantum Information. Cambridge university press."
  },
  {
    "objectID": "posts/2024-01-05-FibonacciComplexity/index.html",
    "href": "posts/2024-01-05-FibonacciComplexity/index.html",
    "title": "Computational Complexity of Fibonacci Sequences.",
    "section": "",
    "text": "This article was originally written circa October 2022. [-JHM]\nCraig Alan Feinstein’s papers on \\(P\\) not equal to \\(NP\\) have caused us to review algorithms and complexity. We study “computational complexity” from the pragmatic perspective. This in contrast to “theoretical” computation results, which is a contradiction in terms.\nHere we consider the question”:“What is the computational complexity of computing the \\(n\\)th Fibonacci number?"
  },
  {
    "objectID": "posts/2024-01-05-FibonacciComplexity/index.html#naive-fibonacci-computation",
    "href": "posts/2024-01-05-FibonacciComplexity/index.html#naive-fibonacci-computation",
    "title": "Computational Complexity of Fibonacci Sequences.",
    "section": "Naive Fibonacci Computation",
    "text": "Naive Fibonacci Computation\nRecall that the Fibonacci sequence is a sequence of integers \\(f_0, f_1, f_2, \\ldots\\) defined recursively by: \\[f_0=1, ~~ f_1=1, ~~\\text{and}~~ f_{n+1}=f_n+f_{n-1}~~\\text{for}~~ n\\geq 1.\\]\nTo naively compute \\(f_{2022}\\) would require we repeatedly apply the recursive definition, finding \\(f_[2022}:=f_{2021}+f_{2020}\\), etc.. This procedure requires we compute all the values \\(f_k\\) for \\(k&lt; 2022\\).\nThe pseudocode for this naive procedure is based on “memoization”. If we apply the above recursive formula to evaluate \\(f_{2022}\\), then we’ll be reusing the values \\(f_k\\) for \\(k &lt; 2022\\). Memoization is simply a device for recording and reusing these intermediate values. So to compute \\(f_{2022}\\) we will need to store the previous values, although there is a method (Bottom-Up evaluation) which is similar, and which inductively evaluates \\(f_1\\), \\(f_2\\), \\(f_3\\), etc., until we reach \\(f_{2022}\\).\nSo if we compute \\(f_{2022}\\) and memoize all the terms \\(m=\\{f_1, f_2, f_3, \\ldots\\}\\), then the memo \\(m\\) grows unbounded in length. Some authors modify the memo to only contain the two “largest” elements, thus reducing the length of the list. This would require updating the memo \\[\\{f_1, f_2\\} \\leadsto \\{f_2, f_3\\} \\leadsto \\{f_3, f_4\\} \\leadsto \\cdots \\leadsto \\{f_{n-1}, f_n\\} \\leadsto \\cdots.\\] But still the number of bytes necessary to represent the fibonacci elements grows exponentially with \\(n\\) since \\(f_{n+1} \\approx \\varphi^n\\) where \\(\\varphi\\) is the golden ratio \\(\\varphi=\\frac{1+\\sqrt{5}}{2} \\approx 1.618033\\).\nThe memoization gives a time complexity of \\(O(n)\\) to compute \\(f_n\\). But memoization requires memory to store the memo! And we think this needs to be accounted for in the computational complexity, since obviously it’s an essential part of the computation. Indeed computation as represented by Turing machines, say, involves motion and read/write/react at different locations on the “tape”. It seems that a proper accounting of complexity should involve every time-step, namely every motion and site operation. In analyzing the complexity of the usual \\(O(n)\\) complexity of computing \\(f_n\\), the action of calling and retrieving the data in the memo are considered to be constant.\n\n## Code taken from https://stackoverflow.com/a/61604929/17406611\n\nfib_cache = {0 : 0, 1 : 1}\n\ndef fib(n):\n    if n in fib_cache:\n        return fib_cache[n]\n    for i in range(100, n, 100):\n      fib(i)\n    fib_cache[n] = result = fib(n-1) + fib(n-2)\n    # print(fib_cache) # &lt;--this line is diagnostic.\n    return result\n\n  # There's interesting comment on why the range(100, n, 100) line is important\n  # to avoid a \"recursive depth error\". The point is that the memo needs to be filled\n  # as the algorithm develops, otherwise it recurses too far and returns an error!\n\n\n# crashes the RAM\n# fib(1111111)\n\nfib(11111)\n\n515449135231559341621591189426925989418721609167804403107087312453694294479381404009230187092526675635022414542794904158934368158350216751867828729213516067642461147232232268304004580596220514978296704097915617589481297010691749471374967376304898014174716132125169572206688299944881902864940487579850754037243411123276226268998274067232370656873037885028569262362061989878439356579125363739644638605976667733232134130196207453194213358616463005487086631652051025004934485196108344869244852506414543015664379038338857611347469102943415360234480491921571800239803284078147859161629100936007246749423449323827401152142684138017539299637210829955409666781554035555164800825902557894478979141680264821730580654526990976167873657740460977594388216677737796493623940501749951993194553070912364327001564086186444836587037180810655016948562608480145057901528396467327800369394725748856906177005886944277609832795419082424474419033931754123200248752600310587761729439189440527073799320938597514569967706344559861576816209214912702962065352672071494639021231263782338510241176219316180788341549329905272081790433223099472061887224254193326845457247107409050092252994931668934553671721376871177693036352993174131508107261653495025271505086039171034392185521383307925723081097129536244468148375789733582131797176285225457221029865658845503179175504307577930140222583997281098099332145783930418777810346276337273420733754768191403158839413163368990092771464626510432292314209966950363068432367028332284209840897425718364670733733609565321893240873729315360915814803137552560521106490937691421540344502423323064743545226360364012549367167257038202145921861042955299329942301124074181956428710271876930526019606797077558959445434943166179407403375284366340173639269807373108055388080201746447050804598946499248800891171987624229020766742994219485280547337990630263452898332213470171667603200991268579583095661682595442200149483262133621860660302141160974707437100532341443580636798210704649175613121627855118061762876137389590812891131603206815601843823369210865672605256743142632199819790960079549275267815983406188538500072911327187912330503064073869613282412315795790671452556371408354045852898125070873750632615799469070245720036053071341314252092446074260578417947762296896685389368685659291620443322232933074723685001342680075497489"
  },
  {
    "objectID": "posts/2024-01-05-FibonacciComplexity/index.html#wolframs-nondefinition-of-computational-irreducibility.",
    "href": "posts/2024-01-05-FibonacciComplexity/index.html#wolframs-nondefinition-of-computational-irreducibility.",
    "title": "Computational Complexity of Fibonacci Sequences.",
    "section": "Wolfram’s (non)Definition of Computational Irreducibility.",
    "text": "Wolfram’s (non)Definition of Computational Irreducibility.\nIn Wolfram’s book “A New Kind of Science”, there is introduced the idea of computational irreducibility. We quote from Chapter 13, pp.739-740.\n[Begin quote:]\n“Computational irreducibility leads to a much more fundamental problem with prediction. For it implies that even if in principle one has all the information one needs to work out how some particular system will behave, it can still take an irreducible amount of computational work actually [sic] to do this.\nIndeed, whenever computational irreducibility exists in a system it means that in effect there can be no way to predict how the system will behave except by going through almost as many steps of computation as the evolution of the system itself.\nIn traditional science it has rarely even been recognized that there is a need to consider how systems that are used to make predictions actually operate.\nBut what leads to the phenomenon of computational irreducibility is that there is in fact always a fundamental competition between systems used to make predictions and systems whose behaviour one tries to predict.\nFor if meaningful general predictions are to be possible, it must at some level be the case that the system making the predictions be able to outrun* the system it is trying to predict. But for this to happen the system making the predictions must be able to perform more sophisticated computations than the system is trying to predict.*\nBut the remarkable assertion that the Principle of Computational Equivalence makes is that this assumption is not correct, and that in fact almost any system whose behaviour is not obviously simple performs computations that are in the end exactly equivalent in their sophistication.\nSo what this means is that systems one uses to make predictions cannot be expected to do computations that are any more sophisticated than the computations that occur in all sorts of systems whose behaviour we might try to predict. And from this it follows that for many systems no systematic prediction can be done, so that there is no general way to shortcut their process of evolution, and as a result their behaviour must be considered computationally irreducible.”\n[End quote.]\nHere’s what we like about Wolfram’s idea of “computational irreducibility”. Firstly it aligns with experience. Mathematicians who actually compute things know there’s no shortcuts for most operations. It’s very difficult to add two numbers \\(a+b\\) without actually “adding” them arithmetically. This relates to Craig Alan Feinstein’s arguments that \\(P \\neq NP\\), and specifically that the subset-sum problem is computationally irreducible. To decide whether a set of signed integers \\(S\\) contains a zero subset-sum, there really is nothing we can do except exhaustively search through all the subsets and add the elements. But most theoretical computer scientists “dream” of somehow there existing a shortcut, a black box, a sophisticated magical algorithm that will compute the sums without actually computing them. So Wolfram’s idea of “computational irreducibility” is in opposition to the theoretical computer scientists’ optimism.\nBut what’s lacking is any rigorous proof or definition of computational irreducibility. If I had to make a pseudo-definition (intuitive) I would first say:\n\na computation is the evaluation of a function \\(f: X \\to Y\\), and the evaluation \\(f\\) is computationally irreducible if the complexity of any equivalent composition \\(g: X \\to Z\\) and \\(h: Z \\to Y\\) with \\(f = h \\circ g\\) satisfies \\[c(f) \\leq c(h) + c(g),\\] where \\(c(f)\\) represents the “computational complexity” of evaluating a function \\(f\\).\n\nHowever the complexity \\(c(f)\\) is not well-defined at this point.\nBut the idea is that a function \\(f\\) always admits (nonunique!) compositions \\(f=h \\circ g\\), and the question is whether these compositions \\(h, g\\) can be any simpler than the evaluation of \\(f\\) itself!\nThe above is not sufficiently rigorous since we have not defined \\(c(f)\\). Moreover \\(c\\) is not defined on the category of functinons, since \\(f=h\\circ g\\) are identical functions from the categorical viewpoint. Our point is simply that identical functions are not necessarily identical computations!\nFor example, Wolfram would argue that the Collatz function arising in the Collatz conjecture \\(3x+1\\) problem is computationally irreducible. There are no shortcuts for evaluating the Collatz function, and therefore there will never be a proof of Collatz’ conjecture. This is why no mathematician should study the problem, because the results/outputs of the Collatz function cannot be logically established, they can only be empirically established by directly evaluating the Collatz function using the definition."
  },
  {
    "objectID": "posts/2024-01-05-FibonacciComplexity/index.html#topological-irreducibility",
    "href": "posts/2024-01-05-FibonacciComplexity/index.html#topological-irreducibility",
    "title": "Computational Complexity of Fibonacci Sequences.",
    "section": "Topological Irreducibility",
    "text": "Topological Irreducibility\nLet’s examine the word irreducibility somewhat more. Wikipedia has many references, but the primary definition (for our perspective) is the topological irreducibility. Here is the definition from Wikipedia\nDefinition: (Reducible and Irreducible Topological Spaces) A topological space \\(X\\) is reducible if it can be written as a union $ X= X_1 X_2$ of two closed proper subsets \\(X_1, X_2\\) of \\(X\\).\nDefinition: A topological space \\(X\\) is irreducible (or hyperconnected) if it is not reducible. Equivalently, if all non empty open subsets of X are dense, or if any two nonempty open sets have nonempty intersection.\nSo the computational aspect of irreducibility would involve computing intersections of nontrivial open subsets of the space. Formally it would be better to provide the relative definition of irreducibility for subsets \\(A \\subset X\\), but we just use the subspace topology on \\(A\\) in \\(X\\). I.e., open subsets of \\(A\\) are defined as intersections \\(U \\cap A\\) of open subsets \\(U\\) of \\(X\\). It’s useful to introduce the relative definition because our first idea for defining computational complexity is to consider the topology of the graph of the function in question, i.e. \\(graph(f) \\subset X \\times Y\\). Now if we have a composition \\(f = h \\circ g\\), then we obtain two graphs(!) namely \\[graph(h) \\subset Z \\times Y\\] and \\[graph(g)\\subset X \\times Z.\\]\nI suppose it would be interesting to determine whether the graph of the Collatz function is an irreducible topological subset of the product \\(\\bf{N} \\times \\bf{N}\\). And here we need specific the topology of \\(\\bf{N}\\), which is not so obvious, although perhaps the discrete topology is the most natural.\nTo this point we haven’t proved anything new, we’ve only conceptualized the problem, trying to more formally define “irreducibility”. We have yet to define the complexity \\(c(f)\\) of a function although our idea is that \\(c(f)\\) should reflect a property of \\(graph(f)\\). Dependancy on the graph of \\(f\\) allows us to compare compositions \\(f=h\\circ g\\) and maybe develop comparisons between \\(c(f)\\), \\(c(g)\\), \\(c(h)\\), etc..\nBut the point is that we have a strategy now, and we can consult the literature by looking for techniques to prove the irreducibility of topological spaces. This is a general problem, and we can even begin with the more specialized case of algebraic geometry.\nIn algebraic geometry, if a variety is irreducible then what does that mean in concrete terms for the equations defining the variety? For hypersurfaces it means the variety can be described by two equations instead of one. For example a polynomial \\(p(x)\\) is reducible if \\(p(x)=q_1(x) . q_2(x)\\) for nontrivial polynomials \\(q_1, q_2\\). What’s interesting for polynomials is that the degrees of the products are strictly smaller, namely \\[deg(p) = deq(q_1)+deg(q_2).\\]\nBut let’s hypothesize that the complexity of evaluating a degree \\(d\\) polynomial is \\(d\\). Then the complexity of computing \\(p=q_1 q_2\\) is not \\(deg(p)\\) but actually \\[\\max\\{ deg(q_1), deg(q_2) \\}.\\] This is assuming that the product \\(p=q_1 . q_2\\) has constant time complexity, which is reasonable for real or complex numbers. So this is elementary examples of how topological reducibility relates to computational complexity. And this seems to be the beginning of maybe something more interesting."
  },
  {
    "objectID": "posts/2024-01-05-FibonacciComplexity/index.html#complexity-of-exponentiation",
    "href": "posts/2024-01-05-FibonacciComplexity/index.html#complexity-of-exponentiation",
    "title": "Computational Complexity of Fibonacci Sequences.",
    "section": "Complexity of Exponentiation",
    "text": "Complexity of Exponentiation\nThere is a recurring idea in evaluating the complexity of exponentiations, and it’s based on taking successive squares based on the binary representations. This is where the \\(O(log_2 ~n)\\) complexity algorithm for evaluating \\(f_n\\) is derived. This speedup depends on the fact that the rule for defining the fibonacci sequence is linear and static, namely the infinite sequence of rules \\(f_{n+1}=f_n+f_{n-1}\\) for \\(n\\geq 1\\) is almost only one rule \\[f_{*+1} = f_* + f_{*-1}.\\]\nThis relates to the observation that iterated matrix powers of \\[Q=\\begin{pmatrix}\n1 & 1 \\\\\n1 & 0 \\end{pmatrix} \\] generate the Fibonacci sequence. Specifically we have the identity \\[\n\\begin{pmatrix}\nf_{n+1} & f_n \\\\\nf_n & f_{n-1} \\end{pmatrix} = \\begin{pmatrix}\n1 & 1 \\\\\n1 & 0 \\end{pmatrix}^n.\n\\]\nThis identity has alot of different explanations, but it suggests the following “speedup” for computing \\(f_n\\) for \\(n\\geq 2\\). For example, to compute \\(f(16)=f(2^4)\\) we can evaluate \\[Q, ~~~~~Q^2, ~~~~ Q^4=(Q^2)^2, ~~~~ Q^8=(Q^4)^2.\\] And this successive squaring of the previous result allows a \\(log_2(n)\\) speedup of the Fibonacci computation.\nLikewise \\(f(15)\\) requires we compute all the powers \\(Q, Q^2, Q^4, Q^8\\) and then take the product \\[Q.Q^2.Q^4.Q^8=Q^{15}.\\] This represents somewhat the “worst case”. Notice that here we need to store the powers \\(Q^{2^k}\\), \\(k&gt;0\\), in memory and then multiply these powers together. So what precisely is the complexity? We find it curious how the complexity of matrix multiplication remains an open question in computer science. There are various speedups, although the naive complexity is \\(O(n^3)\\) although speedups to \\(O(n^{2+\\epsilon})\\) are possible for some specific small values of \\(\\epsilon\\). We’ll let \\(\\tilde{m}\\) represent the complexity of multiplication of two matrices.\nSo the total complexity is counted like:\n\nComplexity of evaluating \\(log_2(n)\\) matrix powers is \\(\\tilde{m}. log_2(n)\\) time steps.\nComplexity of \\(log_2(n)\\) matrix multiplications is \\(\\tilde{m}. log_2(n)\\) time steps.\nTotal Complexity is the sum of the complexities in 1. and 2., and therefore equal to \\(\\tilde{m}. log_2(n)\\), or \\(O(log_2n)\\)\n\nAre the methods of computing the fibonacci sequence really different? Obviously one appears to be faster, requiring nominally less time steps or elementary operations. However there are increasing memory constraints arising from the matrix form, having to store the powers \\(Q^{2^k}\\), and where of course the entries are becoming super exponentially large."
  },
  {
    "objectID": "posts/2024-01-05-FibonacciComplexity/index.html#so-what",
    "href": "posts/2024-01-05-FibonacciComplexity/index.html#so-what",
    "title": "Computational Complexity of Fibonacci Sequences.",
    "section": "So What?",
    "text": "So What?\nFair question. But as we said, this is the beginning of maybe something deeper. So we need to compare the two algorithms, and perhaps address whether the final \\(O(log_2~n)\\) is really irreducible in Wolfram’s sense.\n[To be continued … -JHM]"
  },
  {
    "objectID": "posts/2024-01-04-YaoMillionaireProblemPart1/MillionaireGame.html",
    "href": "posts/2024-01-04-YaoMillionaireProblemPart1/MillionaireGame.html",
    "title": "On Yao’s Millionaire Problem. Part 1.",
    "section": "",
    "text": "[Originally written February 2022 … –JHM ]\nThe purpose of this article is to investigate whether there is strategy or skill possible in the following variation of Yao’s “Millionaire Problem”.\nHere is the game. We have a huge grid \\(\\mathbf{R}^2\\). Now let two players \\(A,B\\) have secret locations \\(s_A=(x_A, y_A)\\) and \\(s_B=(x_B, y_B)\\). These secrets are points in the euclidean plane \\(\\mathbf{R}^2\\).\nNow the players \\(A, B\\) are going to take turns guessing affine functions (or affine lines in \\(\\mathbf{R}^2\\)) and the first player to guess an affine function which separates the secrets wins!\nThe gameplay is something like this: The players \\(A,B\\) take turns. If player \\(A\\) goes first, then player \\(A\\) chooses an affine function \\(\\ell\\) on \\(\\mathbf{R}^2\\), and asks player \\(B\\) to reply with the sign of \\(\\ell(s_B)\\). We require that \\(B\\) replies honestly with \\(sgn(\\ell(s_B))\\). This is the end of player \\(A\\)’s turn. If \\(\\ell\\) separates the secrets, then player A wins. Otherwise it’s player B’s turn. Next player \\(B\\) chooses an affine function \\(\\ell'\\), and asks player \\(A\\) to reply with the sign of \\(\\ell'(s_A)\\). Once player \\(A\\) replies, then this is the end of player \\(B\\)’s turn. Again, if \\(\\ell'\\) separates the secrets, then player B wins. Otherwise it’s player A’s turn.\nThe object of the game is to determine an affine function \\(\\ell\\) which separates the secrets, i.e. for which $ sgn((s_A)) sgn((s_B)).$ The first player to demonstrate an affine function which separates the secrets wins!\nOur interest is to find optimal strategies for this game. Firstly, we have to consider whether any strategy is even possible. For example, can player \\(A\\) use the cumulative history of both player \\(A\\) and \\(B\\)’s affine guesses to better inform their next guess? For example, if player \\(A\\) guesses an affine function \\(\\ell\\) which does not separate, then player \\(B\\) can use the knowledge of their own private secret to determine which halfspace contains \\(s_A\\). And indeed, by the same reasoning player \\(A\\) can use their knowledge of \\(s_A\\) to likewise determine which halfspace contains \\(s_B\\). So obviously the initial distribution \\(d\\lambda\\) is updated to the restricted distribution \\(d\\lambda\\cdot 1_H\\), where \\(H\\) is the halfspace defined by \\(\\ell\\) and containing \\(s_A, s_B\\). With successive guesses, the distribution becomes a descending chain of closed convex sets, namely the intersection of successive halfspaces, having the form \\[d\\lambda \\leadsto d\\lambda \\cdot 1_H \\leadsto d\\lambda \\cdot 1_H 1_{H'} \\leadsto d\\lambda \\cdot 1_H 1_{H'} 1_{H''} \\leadsto \\cdots. \\]\nThe notation is somewhat strange, but simply expresses that we remain uncertain of the specific location of the secrets \\(s_A, s_B\\), except we know the possibly location is becoming more restricted.\nIn the millionaire game, the players \\(A,B\\) have an interest in privacy. Their secrets \\(s_A, s_B\\) are intended to be secret. This means the players \\(A,B\\) might not choose affine functions which potentially reveal information about their own secrets. In practice this means players determined to maintain their privacy will always choose affine functions which do not bound compact convex sets. Similarly, an opponent will not readily choose affine functions which separates the domain into a bounded component, since the probability that the opponent’s secret lies in the bounded component is relatively small, while the probability of its lying in the unbounded component is much greater.\nThe subject of so-called zero knowledge proofs in cryptography is related to the millionaires problem. Here we try to find a balance where the players can choose to reveal as much as they wish of their own balances, while their own guesses are signals/indications in-themselves of the secret balance.\nOur question is whether there is any strategy or skill in this game. What is the optimal strategy? Can the player use the knowledge of the opponent’s affine functions to improve their own selection of affine function??\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Now we simulate the millionaire problem on the euclidean two-dimensional plane.\n# For convenience we rename the players $A,B$ as players $+1, -1$, respectively.\n# for testing purposes we suppose the players A,B have secrets below:\n\n#s_A=input(\"What is player A's secret position?\")\ns_A=[16,0]\ns_A=np.array(s_A)\n\n#s_B=input(\"What is player B's secret position?\")\ns_B=[0, 0.2]\ns_B=np.array(s_B)\n\n\n# now we define some basic functions, i.e. to compute affine functions based on\n# their normal n and height b.\ndef affine(n,x,b):\n    n=np.array(n)\n    x=np.array(x)\n   # return n.dot(x)+b\n    return n[0]*x[0]+n[1]*x[1] + b\n\n\n# to protect the secret we really only need the sign of the affine function.\ndef sign(x_Real):\n    if x_Real&lt;0:\n        return -1\n    else:\n        return +1\n\n# here t defines the test function, which returns True iff the affine function\n# separates the secrets. True is returned if the signs of the affine function\n# evaluated on the secrets are not equal.\ndef t(n,b):\n    n=np.array(n)\n    if sign(affine(n,s_A,b)) != sign(affine(n,s_B, b)):\n         return True\n    else:\n         return False\n\n\n# Now we setup the basic routine, i.e. sequence of gameplay.\n\n#initial conditions.\noutcome=False\nhistory=[]\nvector_history=[]\nplayer=+1\ni=0\ncolor=[]\n\n\nwhile outcome == False:\n    print(\"\\n Player \" + str(player) + \"'s turn to play:\" )\n    print(\"Given the history \" + str(history) + \" choose your affine function:\")\n\n    n0 = float(input())\n    n1 = float(input())\n    b = float(input())\n    history = history + [[n0, n1, b]]\n    vector_history=vector_history + [[n0, n1]]\n    i=i+1\n\n\n    if t([n0, n1], b) == True:\n        outcome = True\n        print(\"Winner! Player \" + str(player)+ \" has separated the secrets with \" + str([n0, n1, b]) + \". End of Game!\")\n    else:\n        print(\"Fail! Player \" + str(player) + \" has failed to separate the secrets... End of turn.\")\n        player=player*(-1)\n\n\n\n# the following plots the various normals chosen by the players, but we would\n# prefer to have the half spaces.\n    V=np.array(vector_history)\n    origin=np.array([[0]*i, [0]*i])\n    plt.quiver(*origin, V[:,0], V[:,1], scale=21)\n    plt.show()\n\n\n Player 1's turn to play:\nGiven the history [] choose your affine function:\n1\n1\n0\nFail! Player 1 has failed to separate the secrets... End of turn.\n\n Player -1's turn to play:\nGiven the history [[1.0, 1.0, 0.0]] choose your affine function:\n3\n1\n0\nFail! Player -1 has failed to separate the secrets... End of turn.\n\n Player 1's turn to play:\nGiven the history [[1.0, 1.0, 0.0], [3.0, 1.0, 0.0]] choose your affine function:\n1\n4\n0\nFail! Player 1 has failed to separate the secrets... End of turn.\n\n Player -1's turn to play:\nGiven the history [[1.0, 1.0, 0.0], [3.0, 1.0, 0.0], [1.0, 4.0, 0.0]] choose your affine function:\n-3\n0\n-5\nFail! Player -1 has failed to separate the secrets... End of turn.\n\n Player 1's turn to play:\nGiven the history [[1.0, 1.0, 0.0], [3.0, 1.0, 0.0], [1.0, 4.0, 0.0], [-3.0, 0.0, -5.0]] choose your affine function:\n-1\n-4\n2\nWinner! Player 1 has separated the secrets with [-1.0, -4.0, 2.0]. End of Game!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe above is a very simple gameplay, where it happens by chance that the two points can be separated by a flat strip, namely the space between two parallel halfspaces. We included the above simply as an example.\n[To Do:] 1. Use matplotlib to plot the halfspaces, and not simply the normal vector, which is what we have above.\n\nDetermine some automatic routine to compete with a human opponent.\nThe millionaire’s problem implicitly assumes the players \\(A,B\\) have large funds, i.e. enough to pay for a dinner! Therefore we might need assume our secrets \\(s_A, s_B\\) are sufficiently far from the origin. (?)\nIf the domain is essentially infinite, then a certain amount of privacy will always be maintained, because it’s better to bisect the unknown into two halfspaces of equal (possibly infinite) area. If the affine function indeed separates the secrets, then the position of that secret is only known to occupy an infinite area domain, and thus essentially remains private in a restricted sense. Although of course the direction of the secret, and not necessarily its magnitude will be better known to the opponent, i.e. there will be a definite reduction of uncertainty in the direction of the opponents secret, but not necessarily a reduction in uncertainty in its magnitude.\n\nIf an opponent proposes an affine function which separates the domain into a bounded and unbounded component, then that is huge risk for the player, i.e. it’s unlikely that the small bounded domain (chosen at random) will contain the secret as opposed to the infinite domain. At the risk of belabouring the point: a random infinite domain is more likely to contain an unknown secret than a compact domain. We find this an interesting point…\n\nfrom scipy.spatial import HalfspaceIntersection\n\nprehistory = history[:-1]\nsigns=[]\nsph=[]\n\nfor x in prehistory:\n    epsilon=sign(affine([x[0], x[1]], s_A, x[2]))\n    signs=signs+[epsilon]\n    sph=sph+[epsilon*np.array(x)]\n\nsph=np.array(sph)\n\n# for illustration we have the secret s_A as feasible_point.\n# its interesting question to select a feasible point which\n# does not reveal too much information about the secrets...\n# but obviously any point on the convex hull formed by the secrets s_A, s_B\n# will be a feasible point. But there are many more choices, so which choice reveals\n# the least information about the secrets s_A, s_B ? I.e. which feasible point can be chosen\n# which reveals the least information about s_A, s_B?\n\nfeasible_point = np.array([16.0, 0.0])\n\nhalfspaces = sph*(-1)\n\n\n# we need reverse-signs to align with the convention in qhull that\n# the halfspaces are defined by the inequality Ax+b &lt;= 0.\nhs = HalfspaceIntersection(halfspaces, feasible_point)\n\nimport matplotlib.pyplot as plt\nfig = plt.figure()\nax = fig.add_subplot('111', aspect='equal')\nxlim, ylim = (-100, 100), (-100, 100)\nax.set_xlim(xlim)\nax.set_ylim(ylim)\nx = np.linspace(-100, 100, 1000)\nsymbols = ['-', '+', 'x', '*']\nsigns = [0, 0, -1, -1]\nfmt = {\"color\": None, \"edgecolor\": \"b\", \"alpha\": 0.5}\nfor h, sym, sign in zip(halfspaces, symbols, signs):\n    hlist = h.tolist()\n    fmt[\"hatch\"] = sym\n    if h[1]== 0:\n        ax.axvline(-h[2]/h[0], label='{}x+{}y+{}=0'.format(*hlist))\n        xi = np.linspace(xlim[sign], -h[2]/h[0], 1000)\n        ax.fill_between(xi, ylim[0], ylim[1], **fmt)\n    else:\n        ax.plot(x, (-h[2]-h[0]*x)/h[1], label='{}x+{}y+{}=0'.format(*hlist))\n        ax.fill_between(x, (-h[2]-h[0]*x)/h[1], ylim[sign], **fmt)\nx, y = zip(*hs.intersections)\nax.plot(x, y, 'o', markersize=8)\n\n[[ 1.  1.  0.]\n [ 3.  1.  0.]\n [ 1.  4.  0.]\n [ 3. -0.  5.]]\n&lt;class 'numpy.ndarray'&gt;\n\n\n\n\n\nThe above intersection of halfspaces isn’t what i expected. The complete intersection is the sector in the upper right hand corner.\nNow if we are truly taking secret points \\(s_A, s_B\\) at random in \\(\\mathbf{R}^2\\), then almost all random choices of affine functions will not separate the secrets. For example, given the homogeneity of \\(\\mathbf{R}^2\\), we can consider the secrets \\(s_A, s_B\\) as being extremely close such that they are basically coincident, or at least as seen from a far distance. But then a random choice of affine function is extremely unlikely to contain the two points. Thus it appears that truly random choices of affine functions have essentially zero probability of separating the secrets.\nThis leads to the next step in our study of the Millionaire Problem, namely where the initial distribution on \\(\\mathbf{R}^2\\) is not necessarily uniform. E.g., perhaps we know that the secrets are distributed within a given large radius ball. If we have no other information about the secrets except that it lies somewhere on the large ball \\(D\\), then one probabilistic strategy is to bisect the ball with affine functions, i.e. randomly guess an affine \\(\\ell\\) such that $ 1_D 1_{&gt;0}$ and \\(1_D 1_{\\ell &gt; 0}\\) have equal area.\nBut what about privacy? In the previous case where the distribution was uniform on its support on \\(\\mathbf{R}^2\\), the privacy of the secrets was maintained so long as the affine functions were unbounded (from above and below). And the opponents would always guess such affine functions because there odds of correctly separating the secrets is significantly increased. But now its possible that the distribution will not be uniform on its support, and therefore the secrets can be learned with a reduction in uncertainty, i.e. perhaps we know that the opponents secret lies in a bounded set, or that 90% percent of the time the opponents secret lies in a given domain.\nWe remark that there is something like a “maximum likelihood” principle being used here. Now regarding privacy: if the domain \\(D\\) is bounded, then depending on the distribution, the secret might have diminished privacy. This leads to Part 2 of our study, where the secrets are distibuted according to nonuniform distributions \\(\\mu_A\\), \\(\\mu_B\\) on \\(\\mathbf{R}^2\\).\n\nWhat is the optimal strategy for nonuniform initial distributions \\(\\mu_A, \\mu_B\\) ?\nCan we quantify the “loss” of privacy when the distributions are supported on unbounded domains versus bounded domains?\n\n(To be continued…)"
  },
  {
    "objectID": "posts/2024-01-03-WeberGibbsLiouville/2024-01-03-WeberGibbsLiouville.html",
    "href": "posts/2024-01-03-WeberGibbsLiouville/2024-01-03-WeberGibbsLiouville.html",
    "title": "Weber Hamiltonian Gibbs Liouville Measure on State Space.",
    "section": "",
    "text": "What are Hamilton’s equations? In a sense the equations are a trivial formalization of Newton’s equations. But when applied to Weber’s force law, the Hamilton equations provide a useful formalism."
  },
  {
    "objectID": "posts/2024-01-03-WeberGibbsLiouville/2024-01-03-WeberGibbsLiouville.html#two-dimensional-xv-relational-state-space",
    "href": "posts/2024-01-03-WeberGibbsLiouville/2024-01-03-WeberGibbsLiouville.html#two-dimensional-xv-relational-state-space",
    "title": "Weber Hamiltonian Gibbs Liouville Measure on State Space.",
    "section": "Two Dimensional xv Relational State Space",
    "text": "Two Dimensional xv Relational State Space\nWe let \\(x,v\\) be position and velocity state variables. Let \\(J\\) be the standard almost-complex structure, i.e. \\(J=\\begin{pmatrix} 0 & 1 \\\\ -1 & 0 \\end{pmatrix}\\) with \\(J^2 = -Id\\).\nThe classical Gibbs-Liouville theorem [ref] says that the canonical volume form \\(dx dv\\) on state space is invariant with respect to Hamiltonian evolution. In otherwords, for a given rectange \\(dx dv\\) of initial conditions in statespace, the forward time evolution of the Hamiltonian system with these given initial conditions occupies a region in \\(xv\\)-spave having the same volume with respect to \\(dx dv\\).\nFor classical Hamilton equations, this follows from the vector field \\(J \\cdot \\nabla H\\) being divergence free: \\[div(J \\nabla H)=0.\\] If we integrate the Hamilton equations, then we obtain \\[\\frac{d}{dt}(x,v)=(\\dot{x}, \\dot{v})= \\begin{pmatrix} \\partial H / \\partial v \\\\ - \\partial H/\\partial x   \\end{pmatrix},\\] and the point is that this field has zero divergence.\nFor example, if the \\(H = \\omega (x^2 + v^2)\\) is the Hamiltonian for a one-particle oscillator, then the energy levels are circles, and the Hamilton evolution is like a rigid rotation. Therefore it’s evident that the area form \\(dx dv\\) is invariant with respect to time evolution.\n[Another example?]\nOur interpretation of the time evolution being volume invariant means the overall “uncertainty” in the state is constant with respect to time. In other words if we know position \\(x\\) with some error \\(dx\\) and velocity \\(v\\) with error \\(dv\\), then the overall uncertainty of the system remains \\(dx dv\\) square-error throughout the evolution. This is relatively good, for example if the particles are spatially closeby (so \\(dx\\) is small), then we are certain that \\(dv\\) is large (i.e. they have large relative motion). Likewise if the particles have very small relative motion (\\(dv\\) small) then the particles have large relative position (\\(dx\\) large).\nThe existence of an invariant background measure \\(dx dv\\) on state space is necessary to define the entropy of states \\(\\rho\\), where entropy is defined relative to this background mesaure. However our researches into Weber electrodynamics has led us to review the basic Gibbs Liouville measure. We find the usual Gibbs-Liouville volume form \\(dx dv\\) is not invariant when the state space evolves according to Weber’s Hamiltonian. Our goal in this article is to derive the proper Weberian invariant measure on state space."
  },
  {
    "objectID": "posts/2024-01-03-WeberGibbsLiouville/2024-01-03-WeberGibbsLiouville.html#distance-independant-densities",
    "href": "posts/2024-01-03-WeberGibbsLiouville/2024-01-03-WeberGibbsLiouville.html#distance-independant-densities",
    "title": "Weber Hamiltonian Gibbs Liouville Measure on State Space.",
    "section": "Distance Independant Densities",
    "text": "Distance Independant Densities\nSuppose that \\(f_{,x}=0\\) identically. Then the basic equation becomes \\[d_v \\log f = -\\log (1-\\frac{v^2}{2c^2}).\\] Therefore we find \\(f= (1-\\frac{v^2}{2c^2})^{-1}\\) is an invariant density which is position independant.\n\nProposition: The density \\[\\rho =(1-\\frac{v^2}{2c^2})^{-1} ~dx dv\\] is the unique invariant measure on state space \\(\\{(x,v)~~|~x&gt;0\\}\\) which is position independant and satisfies \\(\\rho(x,0)=1\\) identically.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-3, 3, 80)\ny =(1-x**2/2)**-1\nplt.plot(x, y)\nplt.show()"
  },
  {
    "objectID": "posts/2024-01-03-WeberGibbsLiouville/2024-01-03-WeberGibbsLiouville.html#velocity-independant-densities",
    "href": "posts/2024-01-03-WeberGibbsLiouville/2024-01-03-WeberGibbsLiouville.html#velocity-independant-densities",
    "title": "Weber Hamiltonian Gibbs Liouville Measure on State Space.",
    "section": "Velocity Independant Densities",
    "text": "Velocity Independant Densities\nWe assume that \\(f_{,v}=0\\) identically. Then the basic equation simplifies to \\[ (f~\\mu_{\\text{eff}}^{-1}~\\frac{\\alpha}{c^2x^2}+f_{,x}) v =0.\\] But the left hand side vanishes iff \\[f~\\mu_{\\text{eff}}^{-1}~\\frac{\\alpha}{c^2x^2}+f_{,x}=0,\\] which rearranges into the logarithmic derivative \\[d_x \\log f = \\frac{f_{,x}}{f} = -\\mu_{\\text{eff}}^{-1}~\\frac{\\alpha}{c^2 x^2}.\\] The definition of \\(\\mu_{\\text{eff}}=\\mu-\\frac{\\alpha}{c^2 x}\\) implies \\[d_x \\log f = \\frac{1}{x} + \\frac{\\beta}{1-\\beta x}\\] where \\(\\beta:=\\mu c^2 /\\alpha\\).\nTaking antiderivatives we find \\[\\log f = \\log x - \\log (1-\\beta x) + C =\\log \\frac{x}{1-\\beta x}+C\\] where \\(C\\) is a constant of integration.\nIf we assume initial conditions \\(f(1,v)=1\\) independantly of \\(v\\), then \\(C=\\log(1-\\beta)\\). Therefore \\(f = (1-\\beta) \\frac{x}{1-\\beta x}\\) is the unique solution of the basic equation such that \\(f_{,v}=0\\) identically and \\(f(1)=1\\).\nTherefore we obtain:\n\nProposition: The density \\[\\rho = (1-\\beta)~\\frac{x}{1-\\beta x} ~dx dv\\] is the unique invariant measure on state space \\(\\{(x,v)~~|~x&gt;0\\}\\) which is velocity independant and satisfies \\(\\rho(1,v)=1\\) identically.\n\nWe should emphasize that this derivation is somewhat specific to \\(xv\\) being strictly a two-dimensional state space. For applications we are of course interested in \\(N\\)-body electrodynamic systems, and this will be discussed in sequel.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nbeta0 = .1\nbeta1= - 10**2\n\nx = np.linspace(0, 1000, 100)\ny =1-  (1-beta0)*x*(1-beta0*x)\nplt.plot(x, y)\nplt.show()"
  },
  {
    "objectID": "posts/2024-01-03-WeberGibbsLiouville/2024-01-03-WeberGibbsLiouville.html#conclusion",
    "href": "posts/2024-01-03-WeberGibbsLiouville/2024-01-03-WeberGibbsLiouville.html#conclusion",
    "title": "Weber Hamiltonian Gibbs Liouville Measure on State Space.",
    "section": "Conclusion",
    "text": "Conclusion\nSo we derive the invariant measure! This allows us to define the entropy of measures relative to \\(\\rho dxdv\\). From here we can begin to study a Weberian approach to the Second “Law” of Thermodynamics.\n[ To be continued – JHM]"
  },
  {
    "objectID": "posts/2024-01-02-MeasuringTape/Measuring Tape.html",
    "href": "posts/2024-01-02-MeasuringTape/Measuring Tape.html",
    "title": "Measuring Tapes and Physics",
    "section": "",
    "text": "We have an idea about digital measuring tapes motivated from repeatedly overhearing the question “Dude would you hold my tape?” on work sites. We think it’s absurd question, and two people are not needed to take an accurate measurement. Our goal is a “point and click” tool to find distances between arbitrary points in an open space volume.\nIf we are measuring distances inside a volume \\(V\\) with boundary \\(\\partial V\\), this means evaluating distances for arbitrary pairs $ (x,y) V V$. All the laser pointers currently on the markey, e.g. Amazon, strictly find distances between boundary points \\((x,y) \\in \\partial V \\times \\partial V\\). This distinction between distances in the volume and distance on the boundary is key.\nFormally the problem is this: let \\(C\\subset {\\bf{R}}^3\\) be a set of centres in space. Suppose we are given two arbitrary points \\(x, y\\in {\\bf{R}}^3\\) and we are given the pairwise distances from \\(x,y\\) to the centres, i.e. we are given the distances $ { dist(x,c)}$ and \\(\\{dist(y,c)\\}\\) for every \\(c\\in C\\). The problem is how to decide \\(dist(x,y)\\) from these measurements. In the literature this problem is called “trilation” as opposed to “triangulation” where distances are inferred from distances instead of angles. There is direct interpretation of the trilation problem via sphere intersections. We also explore a Choquet max entropy method below.\nRemark. We must admit there is wonderful simplicity to conventional measuring tapes. The contractor spans the tape across the distance, hooking the edge of the tape at a given edge, and then estimates the distance with his eye looking at the “end” of the tape in their hand. This basic application requires the tape have a well layed position and be human readable. For simple situations this works fine. But the contractor has many instances throughout a day where awkward measurements need be taken. Here we think the point and click distance finder could work. No physical spans are required."
  },
  {
    "objectID": "posts/2024-01-02-MeasuringTape/Measuring Tape.html#digital-measuring-tapes.-our-answer-to-dude-would-you-hold-my-tape",
    "href": "posts/2024-01-02-MeasuringTape/Measuring Tape.html#digital-measuring-tapes.-our-answer-to-dude-would-you-hold-my-tape",
    "title": "Measuring Tapes and Physics",
    "section": "",
    "text": "We have an idea about digital measuring tapes motivated from repeatedly overhearing the question “Dude would you hold my tape?” on work sites. We think it’s absurd question, and two people are not needed to take an accurate measurement. Our goal is a “point and click” tool to find distances between arbitrary points in an open space volume.\nIf we are measuring distances inside a volume \\(V\\) with boundary \\(\\partial V\\), this means evaluating distances for arbitrary pairs $ (x,y) V V$. All the laser pointers currently on the markey, e.g. Amazon, strictly find distances between boundary points \\((x,y) \\in \\partial V \\times \\partial V\\). This distinction between distances in the volume and distance on the boundary is key.\nFormally the problem is this: let \\(C\\subset {\\bf{R}}^3\\) be a set of centres in space. Suppose we are given two arbitrary points \\(x, y\\in {\\bf{R}}^3\\) and we are given the pairwise distances from \\(x,y\\) to the centres, i.e. we are given the distances $ { dist(x,c)}$ and \\(\\{dist(y,c)\\}\\) for every \\(c\\in C\\). The problem is how to decide \\(dist(x,y)\\) from these measurements. In the literature this problem is called “trilation” as opposed to “triangulation” where distances are inferred from distances instead of angles. There is direct interpretation of the trilation problem via sphere intersections. We also explore a Choquet max entropy method below.\nRemark. We must admit there is wonderful simplicity to conventional measuring tapes. The contractor spans the tape across the distance, hooking the edge of the tape at a given edge, and then estimates the distance with his eye looking at the “end” of the tape in their hand. This basic application requires the tape have a well layed position and be human readable. For simple situations this works fine. But the contractor has many instances throughout a day where awkward measurements need be taken. Here we think the point and click distance finder could work. No physical spans are required."
  },
  {
    "objectID": "posts/2024-01-02-MeasuringTape/Measuring Tape.html#sphere-intersections",
    "href": "posts/2024-01-02-MeasuringTape/Measuring Tape.html#sphere-intersections",
    "title": "Measuring Tapes and Physics",
    "section": "Sphere Intersections",
    "text": "Sphere Intersections\nThe measuring tape problem is related to the fast solving of sphere intersections. If \\(S_1 = S(c_1, r_1)\\) and \\(S_2 = S(c_2, r_2)\\) are spheres in \\({\\bf{R}}^3\\), then their intersection \\(S_1 \\cap S_2\\) is a sphere orthogonal to the vector \\(c_{12}:=c_1-c_2\\). This is immediately seen in figure below [insert image]. Thus the immediate problem is to decide when spheres have empty or nonempty intersection. Generically the intersection is either empty or a codimension one sphere orthogonal to \\(c_{12}\\). Thus we have a bisection type method to decide sphere intersections, namely $S_1 S_2 S_3 $ is equal to \\(S_{12} \\cap S_{34} \\cap \\cdots\\) which is equal to \\(S_{1234} \\cap \\cdots\\) etc.\nRemark. If the centres \\(c\\in C\\) are infinitely far away from the points \\(x,y\\). Then the spheres centred at \\(c\\) will intersect \\(x,y\\) at large radius and appear as affine subspaces. Therefore when the centres \\(C\\) are at infinity, the problem reduces to linear algebra. Given normal vectors \\(\\{n_i\\}\\), decide the distance \\(dist(x,y)\\) from the values of the linear functionals \\(\\{\\langle n_i, x \\rangle\\}_i\\), \\(\\{\\langle n_i, y \\rangle\\}_i\\) for given \\(x,y\\). This means solving an inhomogeneous linear system of equations."
  },
  {
    "objectID": "posts/2024-01-02-MeasuringTape/Measuring Tape.html#euclidean-distance-formula-from-choquet-representation.",
    "href": "posts/2024-01-02-MeasuringTape/Measuring Tape.html#euclidean-distance-formula-from-choquet-representation.",
    "title": "Measuring Tapes and Physics",
    "section": "Euclidean Distance Formula from Choquet Representation.",
    "text": "Euclidean Distance Formula from Choquet Representation.\nNow our idea is to use Choquet representation theorem. Given the distance to centres \\(dist(x,c)\\), \\(c\\in C\\), we construct the maximal entropy measure \\(\\lambda_x\\) supported on \\(C\\) which represents \\(x\\). The measure \\(\\lambda_x\\) represents \\(x\\) in the sense of Choquet Representation theorem [ref] if \\[\\ell(x)=\\int_C \\ell(\\bar{x}) d\\lambda_x(\\bar{x})\\] for every linear functional \\(\\ell\\) on \\({\\bf{R}}^3\\). We emphasize the linearity of \\(\\ell\\).\nIf we study the Euclidean distance, then we deduce the following representation formula:\n\\[dist(x,y)^2:=|x-y|^2~=~\\iint_{C \\times C} \\langle c_i, c_j \\rangle ~~ d(\\lambda_x - \\lambda_y) \\otimes d(\\lambda_x - \\lambda_y).\\] Therefore we find the squared euclidean distance is a \\(\\lambda_x\\) weighted sum of the signed dot products \\(\\langle c_i, c_j \\rangle\\) of centres .\nWe define \\(\\lambda=\\lambda_x\\) as the unique probability measure on \\(C\\) which maximizes the entropy \\(H(\\lambda)\\) subject to the linear constraints \\(\\sum_i \\lambda_i = 1\\) and \\(\\sum_i \\lambda_i c_i=x\\). For [eq1] to be an efficient formula, we need a fast algorithm to represent \\(\\lambda_x\\) given the distance to centres \\(dist(x,c)\\).\nThe method of Lagrange multipliers is based on the observation that \\(H\\) is optimized given the constraints when \\(\\nabla_\\lambda H\\) is linearly dependant with the constraint gradients. The Lagrangian for this optimization problem is \\[L(\\lambda, \\alpha, \\beta):=H(\\lambda)+\\alpha(\\sum_i \\lambda_i -1) + \\langle \\beta, ~~\\sum_i \\lambda_i c_i -x \\rangle\\] where \\(\\lambda \\in {{\\bf{R}}^I}\\), \\(\\alpha \\in \\bf{R}\\), \\(\\beta \\in {\\bf{R}}^3\\).\nIf \\(\\lambda\\) is maximizer, then we have vanishing partial derivatives \\[\\frac{\\partial L}{\\partial \\lambda_i}=\\log \\lambda_i +1  + \\alpha + \\langle \\beta, ~ c_i \\rangle=0, ~~\\frac{\\partial L}{\\partial \\alpha}=0, ~~ \\nabla_\\beta L=0.\\]\nNow we need solve for the variables \\(\\alpha, \\beta, \\lambda\\) using the above equations. We reproduce the calculation below.\nFirst we have \\(\\lambda_i = e^{-(1+\\alpha)} ~~ e^{-\\langle \\beta, c_i \\rangle}.\\)\nThe condition \\(\\sum \\lambda_i=1\\) implies \\(e^{1+\\alpha} =\\sum_i e^{-\\langle \\beta, c_i \\rangle}\\). Consequently we find\n\\[\\lambda_i = \\frac{1}{\\sum_i e^{-\\langle \\beta, c_i \\rangle}}~~ e^{-\\langle \\beta, c_i \\rangle}.\\]\nNext the condition \\(\\sum \\lambda_i c_i = x\\) implies \\[\\sum_i e^{-\\langle \\beta, c_i \\rangle} ~ (c_i - x)=0.\\] Thus we have reduced everything to solving for \\(\\beta \\in {\\bf{R}}^3\\) in this equation. In this case the auxiliary mapping \\(F:{\\bf{R}}^3 \\to {\\bf{R}}^3\\) defined by \\(F(\\beta):= \\sum_i e^{-\\langle \\beta, c_i \\rangle} ~ (c_i - x)\\) is nonlinear. The max entropy measure \\(\\lambda\\) is defined by the equation [lambda] where \\(\\beta\\) is the solution to \\(F(\\beta)=0\\). We observe that \\(F(0)=\\sum_i c_i-x\\). The basic idea is that \\(F\\) is monotone in \\(\\beta \\in {\\bf{R}}^3\\), i.e. the partial derivatives \\(\\frac{\\partial F}{\\partial \\beta_1}\\), \\(\\frac{\\partial F}{\\partial \\beta_2}\\), \\(\\frac{\\partial F}{\\partial \\beta_3}\\) are nonvanishing and have constant sign."
  },
  {
    "objectID": "posts/2024-01-02-MeasuringTape/Measuring Tape.html#action-at-a-distance-how-to-physically-measure-distances",
    "href": "posts/2024-01-02-MeasuringTape/Measuring Tape.html#action-at-a-distance-how-to-physically-measure-distances",
    "title": "Measuring Tapes and Physics",
    "section": "Action at a Distance: How to Physically Measure Distances",
    "text": "Action at a Distance: How to Physically Measure Distances\nWe have commented on the simplicity of the physical measuring tape. But are there other physical principles to determine distances? Evidently the measuring tape requires a physical tape connecting two distinct points in space. But can we measure distance at a distance? How did the Mars rovers determine their positions on the Mars surface. Was position and distance found indirectly via satellite imagery? For example, how does one really decide the Earth-Moon distance?\nStellar Parallax Method\nWe are inspired to reconsider action at a distance forces.\nCan we use forces to measure distances?\nSir Isaac Newton’s 1640s gravitational force law introduced instantaneous action at a distance into physics. Newton unified the forces of the tides, the moon, orbits of the planets, falling bodies. Before Newton it was believed that the orbits of the planets were circular, and that uniform circular motion was perfect and requiring no external causes to persist. However for our purposes gravity is too weak a force to use for interaction measurements.\nElectrostatic forces between a proton-electron pair has a magnitude \\(39 x\\) greater than gravitational attraction. For example, one might naively consider using Coloumb’s law of electrostatic repulsion to determine distance. This would involve a unit charge at position \\(x_1\\), and a unit charge at \\(x_2\\), and measuring the net force. However the net resultant force \\(F\\) at \\(x_1\\) is not necessarily equal to the Coulomb force \\(F_{21}\\). This is because the system is not necessarily electrically isolated! Instead we consider \\[F_{\\text{net}, 1}=F_{21} + F_{\\text{env},1}\\] where \\(F_{\\text{env},1}\\) is the resultant environmental force on \\(x_1\\). In this formula assume that the introduction of the electrical charge at \\(x_1\\) and \\(x_2\\) does not affect the environmental forces \\(F_{\\text{env},1}\\), \\(F_{\\text{env},2}\\). This is an idealized situation. The Coulomb force is deduced only after subtracting the net background force \\(F_{\\text{env},1}\\) from the resultant force. And this is the main idea: we have to make two measurements for \\(F_{\\text{env}}\\) and \\(F_{\\text{net}}\\) and deduce the interaction as a difference.\nWe emphasize the deduction because this is a logical construct. As we have commented there is possibility that the introduction of the electric charges at \\(x_1\\), \\(x_2\\) has an effect on the environmental forces.\nThe basic interaction \\(F_{21}\\) does not necessarily have to be Coulomb’s interaction. We could replace the point charges at \\(x_1, x_2\\) with, say, ring currents or dipoles.\n[to be continued … JHM]"
  },
  {
    "objectID": "posts/2024-01-02-EconomicsVintageSelling/index.html",
    "href": "posts/2024-01-02-EconomicsVintageSelling/index.html",
    "title": "Mathematics of Vintage Selling",
    "section": "",
    "text": "Vintage selling is risk and reward. Here’s our fundamental premise about selling: “People only buy what they see.” They have to see it to believe it, and then they buy.\nHere’s some questions to introduce everything:\n\nPick a random item in the store. “How many people on average have to walk into the store until the item is sold? How many people until the item has a \\(50 \\%\\) chance of selling?”\n“How many customers per week are entering the store? What’s the value of increasing the foot traffic by 15 percent into the store?”\n“Is it better to sell a given item for twenty dollars today, or thirty dollars next month?”\n“How many customers have to see an item and say no until the item is purged from inventory or the selling price updated for clearance?”"
  },
  {
    "objectID": "posts/2024-01-02-EconomicsVintageSelling/index.html#risk-reward-and-people-see-to-believe-then-buy.",
    "href": "posts/2024-01-02-EconomicsVintageSelling/index.html#risk-reward-and-people-see-to-believe-then-buy.",
    "title": "Mathematics of Vintage Selling",
    "section": "",
    "text": "Vintage selling is risk and reward. Here’s our fundamental premise about selling: “People only buy what they see.” They have to see it to believe it, and then they buy.\nHere’s some questions to introduce everything:\n\nPick a random item in the store. “How many people on average have to walk into the store until the item is sold? How many people until the item has a \\(50 \\%\\) chance of selling?”\n“How many customers per week are entering the store? What’s the value of increasing the foot traffic by 15 percent into the store?”\n“Is it better to sell a given item for twenty dollars today, or thirty dollars next month?”\n“How many customers have to see an item and say no until the item is purged from inventory or the selling price updated for clearance?”"
  },
  {
    "objectID": "posts/2024-01-02-EconomicsVintageSelling/index.html#formal-model-time-to-sale-holding-cost-surplus.",
    "href": "posts/2024-01-02-EconomicsVintageSelling/index.html#formal-model-time-to-sale-holding-cost-surplus.",
    "title": "Mathematics of Vintage Selling",
    "section": "Formal Model: Time to Sale, Holding Cost, Surplus.",
    "text": "Formal Model: Time to Sale, Holding Cost, Surplus.\nFormally we assume there is a collection \\(X\\) of clothing items available to the vintage seller. The seller chooses to buy items \\(x\\) at price \\(-\\psi(x)\\) measured in units of dollars.\nThe item \\(x\\) is in inventory for a random period until “time to sale” which we denote by \\(\\tau=\\tau(x)\\). The time to sale occurs when some customer eventually purchases \\(x\\) at sale price \\(+ \\phi(x)\\). The time to sale \\(\\tau\\) is a random variable \\(\\tau: X \\to R_{&gt; 0}\\). The law of \\(\\tau\\) is unspecified at this stage.\nWe assume there is a holding cost \\(\\alpha: R_{&gt;0} \\to R_{&gt; 0}\\) representing the cost of holding an item in inventory over a period of time \\(t\\). We assume the holding cost \\(\\alpha\\) is independant of \\(x\\) and depends only on time. The inventory cost \\(\\alpha\\) depends on operating hours, rent, basically the expenses of running the business from day to day. Ideally these are fixed costs and \\(\\alpha\\) becomes linear function of \\(\\tau\\).\nDefinition:(surplus beta) If item \\(x\\) is sold at future time \\(\\tau\\) and price \\(\\phi(x)\\), the seller’s surplus \\(\\beta\\) on item \\(x\\) is represented as \\[\\beta(x) :=  -\\psi(x) -\\alpha. \\tau(x) + \\phi(x).\\]\nThe vintage seller’s goal is to find inventory and customers with prices \\(\\psi, \\phi\\) which maximize surplus beta \\(\\beta\\).\nMaximizing beta is art and science. The supply of clothing \\(X\\) is huge, and the seller puts capital at risk when purchasing the item (represented by \\(-\\psi\\)) and commiting capital to future holding costs (represented by the random cost \\(-\\alpha.\\tau\\)). The art is finding high beta inventory, where maximizing beta \\(\\beta\\) means optimizing the market conditions where supply of items \\(x\\) meets customers in timely manner (\\(\\tau(x)&gt;0\\) not too large) and at good price (\\(-\\psi+\\phi\\) positive nominal profit).\nBeta \\(\\beta\\) represents net gain when \\(\\beta=\\beta(x)\\) is positive \\(&gt; 0\\) for a given item \\(x\\). On average the seller is profitable if the average of \\(\\beta(x)\\) over the entire supply \\(x\\in X\\) is positive, but ideally we want a positive surplus for every inventory item. This is the simplest way to ensure the average is positive as desired.\nRemark: The time to sale \\(\\tau=\\tau(x)\\), the holding cost \\(\\alpha\\), and the surplus \\(\\beta\\) are random variables with unknown probability laws at this stage. To statistically sample we need apply a “Rule of Five”. This means taking a random sample of five customers, and noting their transaction and experience in detail. The averages of these five random customers will give useful information."
  },
  {
    "objectID": "posts/2024-01-02-EconomicsVintageSelling/index.html#how-long-until",
    "href": "posts/2024-01-02-EconomicsVintageSelling/index.html#how-long-until",
    "title": "Mathematics of Vintage Selling",
    "section": "How Long Until…?",
    "text": "How Long Until…?\nNaively one looks to maximize the price difference \\(-\\psi(x)+\\phi(x)\\). But realistically the item \\(x\\) might never sell at the price \\(\\phi(x)\\). The item will therefore sit in inventory for long time \\(T\\) and represent a persistent negative cost and liability represented in \\(-\\alpha.T\\). Sellers might consider defining a baseline to purge items from their inventory. This is to avoid the sunken cost fallacy, but also because unsold items can interfere with sale of other items. Unused items lock up inventory, waste the client’s time, and basically the seller has better things to sell. So there’s better use of the space. A rule of thumb for purging, for example, might be “never buy the same item twice” represented by the rule of purging \\(x\\) after time \\(t\\) if \\(\\psi(x) &lt; \\alpha(t)\\)."
  },
  {
    "objectID": "posts/2024-01-02-EconomicsVintageSelling/index.html#where-is-the-lowest-risk-return-in-vintage-market",
    "href": "posts/2024-01-02-EconomicsVintageSelling/index.html#where-is-the-lowest-risk-return-in-vintage-market",
    "title": "Mathematics of Vintage Selling",
    "section": "Where is the lowest risk return in vintage market?",
    "text": "Where is the lowest risk return in vintage market?\nWherever there is risk there needs be reward. The vintage sellers have capital at risk, as represented by the costs \\(-\\phi, \\alpha\\). So what is their fair reward? The standard economics textbook answer says the interest earned from government bonds, for example, is a risk free return. Therefore every vintage seller could buy bonds instead of clothes, but this is obviously unrealistic. Yet the environment of vintage selling somewhere has it’s own form of risk free return, and this lowest risk return needs be identified. It could be in the form of charitable donations and tax receipts.\n[To be continued - JHM]"
  },
  {
    "objectID": "posts/2023-03-13-DoldThom_Part3/index.html",
    "href": "posts/2023-03-13-DoldThom_Part3/index.html",
    "title": "Dold Thom Theorem. Configuration Spaces and Homology. (3/3)",
    "section": "",
    "text": "\\(\\newcommand{\\sub}{\\partial^c \\psi(y)}\\)\nThis will be final post on Dold-Thom theorem for a while. It’s a strange result which gives a reconceptualization of homology. The key is the introduction of the topological group Dold Thom group \\(A(X)\\) defined as the kernel of the augmentation map \\(\\epsilon: G(X) \\to G\\).\nWith these posts we have presented the main statement of a relative Dold-Thom theorem which allows us to further prove the natural equivalence between the long exact sequence in homotopy with the long exact sequence in reduced and relative homology, as arising from the short exact sequence and quasifibration \\[0 \\to A(Y) \\to A(X) \\to A(X/ (Y \\cup \\emptyset)) \\to 0.\\]\nWe were initially motivated by electric ideas, how charges behave and neutralize on a conductive plate. They either cancel/neutralize in the interior, or they escape to the boundary. That is the same idea in the short exact sequence above. Cycles are either cycles in the interior, or they are cycles modulo \\(Y\\) by their escaping to \\(Y\\) as a reservoir.\n\nSweepouts and Optimal Transport\nFrom Dold-Thom we moved to sweepouts and optimal transportation (OT). The goal is to construct interesting sweepouts of a given source Riemannian manifold \\((X,\\sigma)\\) using optimal transportation methods and variational data. If \\((Y, \\tau)\\) is a target space satisfying \\(\\dim(X) \\geq \\dim(Y)\\) and $_X _Y , $ then costs \\(c: X\\times Y \\to \\bf{R}\\) which satisfy assumptions labelled (A0123) admit unique \\(c\\)-optimal semicouplings and potentials solving Kantorovich’s dual max program, namely \\(c\\)-concave potentials \\(\\psi\\) on the target \\(Y\\). The \\(c\\)-concave potential, or specifically it’s \\(c\\)-subdifferential, generates a \\(Y\\)-parameter family of subsets \\(Z(y):=\\sub\\). We study \\(y\\mapsto Z(y)\\) as a topological sweepout of \\(X\\) in the sense of [Almgren, Guth]. This requires hypotheses on \\(c\\), \\(\\tau\\), \\(\\psi\\), such that:\n\n\nthe cells \\(Z(y) \\hookrightarrow X\\) are closed cycles on \\(X\\), i.e. \\(Z(y)\\) has vanishing boundary;\n\n\nand the \\(Y\\)-parameter sweepout \\(y\\mapsto Z(y)\\) of \\(X\\) is continuous in the flat Almgren topology: namely if \\(y_0, y_1\\in Y\\) are sufficiently close, then \\(Z(y_0)\\), \\(Z(y_1)\\) bound a Lipschitz chain \\(C\\) of arbitrarily small area with \\(\\partial C= Z(y_1)-Z(y_0)\\).\n\n\nThe Almgren continuity follows from the \\(c\\)-concave potential \\(\\psi\\) and its \\(c\\)-convex transform \\(\\phi:=\\psi^c\\) being locally Lipschitz on its domain. In otherwords, the source potential \\(\\phi\\) has locally bounded gradient everywhere and this keeps the area of bounding chains small.\nN.B. Without \\(\\phi, \\psi\\) being locally Lipschitz, it would be difficult to control the cell in the limit \\(\\lim_{y\\to y_1} Z(y)\\).\n\n\nProblem of Constructing Costs on Dold-Thom Groups\nWe need emphasize that geometry in OT begins with the construction of geometric costs \\(c: X\\times Y \\to \\bf{R}\\). Applications with DT would require costs defined say by \\(c: X \\times A(S^q) \\to \\bf{R}\\), where \\(S^q\\) is the topological \\(q\\)-sphere and \\(A(S^q)\\) is the DT group on the sphere.\nConstructing continuous maps \\(S^d \\to A(S^q)\\) involves different ideas from the construction of continuous maps \\(S^d \\to S^q\\). It’s an interesting problem.\nNaive question: consider the Hopf mapping \\(S^3 \\to S^2\\). Is there a natural cost \\(c: S^3\\times S^2 \\to \\bf{R}\\) for which the Hopf map arises as \\(c\\)-optimal transport between the canonical measures on \\(S^3, S^2\\), respectively?\n\n\nTwo sweepouts on \\(S^2\\)\nTo illustrate the basic idea, we consider two different sweepouts of \\(S^2\\) into \\(1\\)-cycles. In both cases, we see an explosion of (+), (-) charges emerging from the equator.\nIn the first case, the (+), (-) charges accumulate at the north, south poles, respectively, and then travelling in a group annihilate each other along some longitudinal line.\nIn the second case, the (+), (-) charges emerge from the equator and pass through the north, south poles, respectively, and without stopping continue through the poles eventually annihilating again along the equator lines.\nWhich of these two sweepouts is canonical?\nIn our view, the second sweepout, wherein momentum is preserved, is the more canonical. But it’s another interesting question."
  },
  {
    "objectID": "posts/2023-03-09-DoldThom_Part1/index.html",
    "href": "posts/2023-03-09-DoldThom_Part1/index.html",
    "title": "Dold Thom Theorem. Configuration Spaces and Homology. (1/3)",
    "section": "",
    "text": "Dold-Thom theorem provides an important representation result for constructing singular homology. The essence is to replace singular homology on \\(X\\) with spherical maps on a configuration space \\(A(X)\\). One of the challenges in formulating DT is discovering the proper configuration space based on \\(X\\).\n\nConfiguration Spaces\nVarious configuration spaces are defined in the literature. The configurations typically have a physical interpretation based on hard disks, water droplets, electric charges, etc.. The basic models are:\n\n\nHard disks (where collisions of point particles is prohibited);\n\n\nHard disks with basepoint (where collisions are prohibited except with an annihilating basepoint);\n\n\nWater droplets (c.f. Gromov’s category of finite probability spaces [ref], where collisions are additive);\n\n\nElectrostatic droplets (e.g. signed Borel-Radon measures on \\(X\\), possibly with indivisible quanta, e.g. Millikan’s electron).\n\n\nIn example (1) the configuration space forms a monoid without identity, and the disks are not allowed to collide or intersect or join, just as hard disks cannot be joined together.\nIn example (2), a basepoint \\(pt\\) can be chosen on \\(X\\) and this basepoint serves as zero element, thus rendering the monoid into an additive topological group. However, as we commented above, a choice of basepoint is noncanonical when \\(\\pi_1\\neq 0\\).\nThe formal construction of earlier configuration spaces in the literature involves symmetric products with the diagonal removed, e.g. limits \\(\\lim_{N\\to +\\infty} (X^N - \\Delta)/Symm_N\\). Here \\(Symm_N\\) denotes the symmetric group on \\(N\\) letters. Example (3) arises from Gromov’s category of finite probability spaces, where the objects are so-called reductions \\(f: \\mu \\to \\nu\\) between finite probability spaces \\(\\mu, \\nu\\). See (Gromov 2014, 2012).\nThe category of (4) consists of finite electroneutral configurations, where the objects are again reductions \\(f:\\mu \\to \\nu\\). The proper formalization of (4) into a topological abelian group is provided below in the definition of \\(A_0(X)\\).\n\n\nBasepoints\nThe difficulty with the standard configuration spaces is their choice of basepoint. The usual models are based on the category of pointed topological spaces, from which we define the pointed fundamental group. But homology and reduced homology are essentially basepoint-free. This article was motivated by the author’s desiring a basepoint-free version of DT’s theorem for which the long exact sequence of relative homology is naturally isomorphic to the long exact sequence in homotopy (see DT theorem below).\n\n\nDold-Thom’s Topological Group \\(A(X)\\)\nWe introduce the basic definitions:\n\n(Def. 1) Let \\(X\\) be a topological space and \\(G\\) a topological group, e.g. \\(G=\\bf{Z}\\). Let \\(G(X)\\) be the group of of finitely-supported \\(G\\)-valued distributions on \\(X\\), i.e. \\[G(X):=\\{ \\sum n_x .x ~|~\\text{only finitely many nonzero~} n_x \\in G\\}.\\]\nLet \\(\\epsilon_X: G(X) \\to G\\) defined by \\(\\epsilon_X(\\sum n_x x)= \\sum n_x\\) be the canonical augmentation map.\nLet \\(A=A(X)=A_0(X):=\\ker(\\epsilon_X)\\) be the kernel of the augmentation map.\n\nWe make some remarks:\n\nIf \\(G\\) is an abelian group, we define \\(G(X)\\) and \\(A_0(X;G)\\) in the obvious way. However the cases of \\(G=\\bf{Z}\\) and \\(G=\\bf{Z} /2\\bf{Z}\\) appear to be cases of most interest, and henceforth we suppress the “\\(G\\)” from notation.\nMorever because \\(\\bf{Z}\\) is discrete the point charges (+), (-) are not infinitely divisible. This fails to be true if we replace \\(\\bf{Z}\\)-valued distributions with \\(\\bf{R}\\)-valued distributions, i.e. there exists unit charge quanta if and only \\(G\\) is discrete.\nWe observe that \\(G(X)\\) is a topological abelian group, with zero element \\(\\emptyset:=\\sum 0 x=0\\) corresponding to the zero distribution, and whose zero element is essentially the zero element \\(0\\) of \\(G\\). One might consider \\(\\emptyset\\) the “vacuum state on \\(X\\)”. It follows that \\(A_0(X)\\) is a topological subgroup, consisting of all distributions with zero net charge.\nOne naturally sees \\(A_0\\) as a discrete version of all signed Borel-Radon measures \\(\\mu\\) on \\(X\\) which integrate to zero \\(\\int_X 1. d\\mu(x)=0\\), i.e. which are orthogonal to all constant functions on \\(X\\).\nThe topology on \\(A_0(X)\\) implies recombinations of \\(\\emptyset\\) satisfy local conservation of charge. Thus point charges (+) and (-) must be born from the same spatial position on \\(X\\). Therefore the recombinations require the point charges to continuously move on \\(X\\), and without “teleportation”. This is analogous to the distinction between the standard \\(L^1\\) and \\(L^2\\) optimal transport.\n\nThe essence of DT theorems is to represent homology groups in terms of homotopy groups on configuration spaces. In otherwords to identify the homology functors as homotopy functors. The basic idea of this article is that the vacuum state \\(\\emptyset\\) serves as a type of “canonical basepoint on \\(X\\)”, and this enables a natural equivalence between the long exact sequences of relative homology and homotopy.\n\n\nRelative Dold-Thom Group\nIt’s important to establish the relative version of DT. Let \\(Y\\) be a closed subset of \\(X\\). Our goal is to define a relative configuration space \\(A_0(X/Y)\\), and the key identification will be the canonical isomorphism \\[A_0(X/Y)=A_0(X)/A_0(Y).\\] In terms of net zero charged particle configurations, the idea is to view \\(Y\\) as a where excess charges can “ground out”. That is, spheres in the relative DT group \\(A_0(X/Y)\\) basepointed at \\(\\emptyset\\) are recombinations which either neutralize away from \\(Y\\), or neutralize at \\(Y\\).\nHere is the formal definition. If \\(Y\\) is a closed subset of \\(X\\), then \\(G(Y)\\) and \\(A_0(Y)\\) is a closed subgroup of \\(G(X)\\) and there is a canonical quotient \\(G(X) \\to G(X)/A_0(Y).\\) Morever the augmentation map \\(\\epsilon_X\\) canonically descends to the quotient as a type of augmentation map \\[\\epsilon_{ X / Y}: G(X)/A_0(Y) \\to G, \\] and we identify \\[A_0(X/Y):=\\ker(\\epsilon_{X/Y}).\\]\n\n\nDold-Thom’s Theorem\nWe follow the original approach of (Dold and Thom 1958), which theorems are stated in terms of natural quasifibrations between the configuration spaces.\nRecall the definition of quasifibration:\n\n(Def. 2) A continuous map \\(f: X\\to Y\\) between topological spaces \\(X, Y\\) is a quasifibration if the canonical inclusion of fibres \\(f^{-1}(y)\\) into the homotopy fibre of \\(f\\) is a weak homotopy equivalence for every \\(y\\in Y\\).\n\nHere is the main statement of the Relative Dold-Thom Theorem:\n\n(Dold Thom Theorem) Let \\(Y\\) be closed subspace of \\(X\\). Then the short exact sequence of topological abelian groups \\[0\\to A_0(Y) \\to A_0(X) \\to A_0(X /Y) \\to 0,\\] is a quasifibration inducing a long exact sequence of \\(\\emptyset\\)-pointed homotopy groups \\[\\cdots \\to \\pi_{*+1}(A_0(Y), \\emptyset) \\to \\pi_{*+1}(A_0(X), \\emptyset) \\to \\pi_{*+1}(A_0(X/Y), \\emptyset) \\to \\pi_{*}(A_0(Y), \\emptyset) \\to \\cdots\\] which is naturally equivalent to the long-exact sequence of relative homology groups \\[\\cdots \\to \\tilde{H}_{*+1}(Y) \\to \\tilde{H}_{*+1}(X) \\to H_{*+1}(X,Y) \\to \\tilde{H}_*(Y) \\to \\cdots .\\]\n\n(Sketch of proof:) Following the original argument of Dold-Thom, the theorem reduces to verifying that the functors \\(X\\mapsto \\pi_* A_0(X)\\) satisfy the Eilenberg–Steenrod axioms. By a standard argument it follows that the functors \\(\\pi_* A_0\\) and \\(\\tilde{H}_*\\) are naturally equivalent.\nSome remarks.\n\nIn terms of category theory, DT says that if \\(G\\) is an abelian group, then the functor \\(X\\mapsto \\pi_*(A_0(X;G), \\emptyset)\\) is naturally equivalent to the reduced singular homology functor \\(X\\mapsto \\tilde{H}_*(X;G)\\) in the category \\(TOP\\) of basepoint-free topological spaces. Again, we emphasize that Dold-Thom Theorem is basepoint independant, with the vacuum state \\(\\emptyset\\) serving as “canonical basepoint” on \\(X\\).\nFormally the homotopy groups \\(\\pi_q(A_0(X), \\emptyset)\\) consist of homotopy classes of pointed continuous maps \\(f: (S^q, pt)\\to (A_0(X), \\emptyset)\\). Identifying \\(pt\\) with the point at-infinity, we can thus model homotopy classes as compactly supported \\(q\\)-parameter family of distributions on \\({\\bf{R}}^q\\), where “compactly supported” means the distribution is equal to vacuum state outside a compact subset.\nOne immediate consequence of DT is that we find new topological models for the \\(K(G,n)\\) spaces. For example, we find \\(K(\\bf{Z}, 2)\\) is modelled by \\(A_0(S^2)\\), as opposed to the usual \\({\\bf{C}}P^\\infty\\) model. Identifying \\(S^2\\) with the Riemann sphere \\({\\bf{C}} P^1\\), we can identify \\(A_0(S^2)\\) with the set of rational functions on \\({\\bf{C}} P^1\\), i.e. the space of meromorphic functions on the Riemann sphere. The correspondance from meromorphic functions to \\(A_0(S^2)\\) is given by assigning to every meromorphic function \\(f\\) it’s divisors, i.e. poles and zeros of \\(f\\). More generally we find \\(K(G,n)\\) modelled by \\(A_0(S^n;G)\\) for every integer \\(n\\geq 1\\) and abelian group \\(G\\).\n\n\n\n\nNontrivial \\(1\\)-cycle on \\(S^1\\) represented as a loop in \\(A(X)\\)\n\n\n\n\nNaive Electrical Interpretation of Relative Dold-Thom\nAs a consequence of the Dold-Thom Theorem we have the following lemma:\n\n(Lemma) If \\(Y\\) is closed subset of \\(X\\), then we have canonical isomorphism \\[H_*(X,Y) / image(H_*(X)) = \\ker(H_{*-1}(Y) \\to H_{*-1}(X)),\\] where \\(image(H_*(X))\\) is the image of \\(H_*(X)\\) in the relative homology group \\(H_*(X,Y)\\), and the morphism \\(H_{*-1}(Y) \\to H_{*-1}(X)\\) is induced by the inclusion \\(Y\\hookrightarrow X\\).\n\nProof: Long exactness in DT’s theorem implies \\[image(H_*(X))=\\ker(\\delta)\\] and \\[image(\\delta)=\\ker(H_{*-1}(Y) \\to H_{*-1}(X)).\\] But we have canonical isomorphism \\[H_*(X,Y)/\\ker(\\delta)=image(\\delta),\\] and the result follows.\nWhen the source space \\(X\\) is contractible, then we find the isomorphism \\(H_*(X,Y) \\approx H_{*-1}(Y)\\) is canonical between homology groups. However the morphism is noncanonical on the singular chain groups. The construction of such a morphism requires a well-defined “filling” operation by which the cycles on \\(Y\\) can be filled to relative cycles on \\(X\\) mod \\(Y\\).\nHere is our “electrical” interpretation of the above lemma. We see that relative cycles on \\(X\\) (mod \\(Y\\)) are represented either by:\n\n\nchains \\(w\\) which are already cycles \\(\\del w=0\\) on \\(X\\), or\n\n\nchains \\(w\\) whose chain boundaries \\(\\del w\\) are nontrivial on \\(Y\\) but null homologous in \\(X\\).\n\n\nIn terms of DT, this says that if \\(Y\\) is a ground reservoir for charges on \\(X\\), then recombinations of \\(\\emptyset\\) will have parts which are recombinations in \\(X\\) separated from \\(Y\\) and other parts representing charges grounding out at \\(Y\\). For example, any excess charge on a conductive plate will either ground out at the boundary, or recombine and neutralize in the interior. For the relative homology groups, we can identify a component of \\(A_0(X/Y)\\) which is represented by “spontaneous” transports from \\(\\emptyset\\) to the reservoir \\(Y\\). These relative cycles one the one-dimensional disk with boundary are represented in the figure below.\n\n\n\nNontrivial relative 1-cycle on the interval\n\n\n\n\nConclusion\nThis is the first article of a three-part series on Dold-Thom. Our purpose here was to present the statement of the theorem, which allows an interesting spherical representation of homology cycles. There are many connections and ideas to develop, and which we will describe in future posts.\n-JHM.\n\n\n\n\n\nReferences\n\nDold, A., and R. Thom. 1958. “Quasifaserungen Und Unendliche Symmetrische Produkte.” Annals of Mathematics 67 (2): 239–81. http://www.jstor.org/stable/1970005.\n\n\nGromov, M. 2012. “In a Search for a Structure, Part 1: On Entropy.” http://www.ihes.fr/~gromov/PDF/structre-serch-entropy-july5-2012.pdf.\n\n\n———. 2014. “Six Lectures on Probability, Symmetry, Linearity.” http://www.ihes.fr/~gromov/PDF/probability-huge-Lecture-Nov-2014.pdf."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "jhm labs",
    "section": "",
    "text": "What is Proper Time in Special Relativity?\n\n\n\n\n\n\n\neinstein\n\n\ntime\n\n\n\n\nSequel to our post on What is Time? Here we briefly discuss proper time as defined in special relativity. We argue that proper time \\(d au\\) evidently does not have units of time as typically understood, since Lorentz lengths \\(ds'\\) (squareroot difference of squared lengths) do not have units of Riemannian length (squareroot sum of squared lengths). This might seem pedantic but it’s absolutely necessary to observe.\n\n\n\n\n\n\nJan 24, 2024\n\n\nJHM\n\n\n\n\n\n\n  \n\n\n\n\n2025 Year of World Magnetic Model\n\n\n\n\n\n\n\nampere\n\n\ngauss\n\n\nmagnetism\n\n\n\n\nWorld Magnetic Model 2025. We begin with surprising review of Gauss’ 1836 memoir on General Theory of Terrestrial Magnetism which is amazing for it’s references to Amperes molecular current hypothesis and the aurora. Conclusion is that Gauss is correct assuming his classical magnetic fluid hypothesis, but Ampere’s electrodynamic cylinder and Poisson’s magnetic dipole surfaces are not complete. We identify our starting point for an Amperian Birkeland update to the standard Gauss spherical harmonic model, and this is subject of further work.\n\n\n\n\n\n\nJan 22, 2024\n\n\nJHM\n\n\n\n\n\n\n  \n\n\n\n\nWhat is Time?\n\n\n\n\n\n\n\ntime\n\n\n\n\nQuestion: What is time? Answer: Time is matter in motion. A brief post on a fundamental question.\n\n\n\n\n\n\nJan 20, 2024\n\n\nJHM\n\n\n\n\n\n\n  \n\n\n\n\nVintage Selling and Generative Matchmaking (Part 1)\n\n\n\n\n\n\n\nvintage\n\n\ngenerative\n\n\nmatchmaking\n\n\n\n\nWe have an idea on generative matchmaking in vintage clothing. This means generating items and complete outfits with confidence to customers. Easier said than done. Here we present the hypothesis that colour pattern preferences are the dominant factor when buying clothes. This means that a person’s outfits consists of combinations of colour patterns. This is necessary before any so-called machine learning methods can be applied.\n\n\n\n\n\n\nJan 16, 2024\n\n\nJHM\n\n\n\n\n\n\n  \n\n\n\n\nRemark on the Inherent Double Risk of Premature Quantum Adoption\n\n\n\n\n\n\n\nclassical\n\n\nquantum\n\n\nsecurity\n\n\n\n\nWe maintain that \\(P\\) is not equal to \\(NP\\) (following C.A. Feinstein) and that classical encryption is as secure as ever, quantum or no quantum computer. In this brief note we observe a compound risk inherent in premature quantum adoption. Quantum algorithms simultaneously claim to break the classical encryption protocols, while also claiming to offer quantum secure schemes. This double pressure is prompting authorities to a risky proposition. But why replace a system that remains secure with a replacement where everything is in beta stage.\n\n\n\n\n\n\nJan 15, 2024\n\n\nJHM\n\n\n\n\n\n\n  \n\n\n\n\nAmpere, Weber, and Magnetism the Disposable Hypothesis\n\n\n\n\n\n\n\nweber\n\n\nampere\n\n\nmagnetism\n\n\n\n\nMagnetism interacts in every fundamental experiment in physics, yet magnetism is not so well understood. Here we review Andre Marie Ampere’s fundamantal principle of the equivalence of magneto and electrical forces, and it’s consequences. For example we recall Ampere’s observation that the trivial calculus identity \\(div(curl)=0\\) essentially implies the nonexistence of magnetic monopoles and one of Maxwell’s equations, namely \\(div(B)=0\\). This again is fundamental insight of Ampere.\n\n\n\n\n\n\nJan 10, 2024\n\n\nJHM\n\n\n\n\n\n\n  \n\n\n\n\nSansbury and Bohr. Who is More Energetically Probable?\n\n\n\n\n\n\n\natom\n\n\n\n\nIs Bohr’s planetary model of the hydrogen atom \\([+1, -1]\\) really more probable than Weber-Sansbury’s model of \\([[+2, -1], [+1, -2]]\\)? It’s a billion dollar question isn’t it?\n\n\n\n\n\n\nJan 6, 2024\n\n\nJHM\n\n\n\n\n\n\n  \n\n\n\n\nEconomics of Moving and Delivery.\n\n\n\n\n\n\n\nmoving\n\n\n\n\nHow to save money on moving and delivery? Basically prepare and pack as much as possible into as many closed cardboard boxes as necessary, and order a large truck for transport. We also discuss the basic questions which are most useful for estimating the total cost of moving a household. This manages the client’s expectations, but also allows the moving manager to maximize their profit and schedule themselves accordingly\n\n\n\n\n\n\nJan 4, 2024\n\n\nJHM\n\n\n\n\n\n\n  \n\n\n\n\nComputational Complexity of Fibonacci Sequences.\n\n\n\n\n\n\n\ncomplexity\n\n\n\n\nWe review the complexity of Fibonacci’s sequence \\(1, 1, 2, 3, 5, 8, 13,\\) etc., and its relation to S. Wolfram’s informal definition of computational irreducibility. We consider whether topological irreducibility has analogy to computation, and this is somewhat speculative, as we are looking for strategies to prove that \\(O(log_2~N)\\) is the minimal complexity of computing the \\(N\\)th Fibonacci element \\(f(N)\\).\n\n\n\n\n\n\nJan 4, 2024\n\n\nJHM\n\n\n\n\n\n\n  \n\n\n\n\nWeber Hamiltonian Gibbs Liouville Measure on State Space.\n\n\n\n\n\n\n\nweber\n\n\natom\n\n\n\n\nWe examine the Hamilton equations for Weber’s potential in an isolated two-body system. We construct an invariant measure on the xv state space which is distinct from the conventional Gibbs Liouville measure. The invariant measure is necessary background measure from which we can define entropy of distributions.\n\n\n\n\n\n\nJan 3, 2024\n\n\nJHM\n\n\n\n\n\n\n  \n\n\n\n\nOn Yao’s Millionaire Problem. Part 1.\n\n\n\n\n\n\n\nsecret\n\n\n\n\nWe begin the study of Yao’s Millionaire Problem, approaching via convex analysis. Two players have secret points, and the first player to guess an affine function separating the secrets wins. The question is whether the optimal strategy is uniform on the domain, or whether there is some variation in the density. This is elaborated below using both python and some elementary convex analysis\n\n\n\n\n\n\nJan 3, 2024\n\n\nJHM\n\n\n\n\n\n\n  \n\n\n\n\nBookstores and Image to ISBN Problem\n\n\n\n\n\n\n\nbookstore\n\n\nimage_to_isbn\n\n\n\n\nPeople don’t buy what they don’t see. Take pictures of bookshelves, get an ordered index of ISBN numbers. Get books seen and sold.\n\n\n\n\n\n\nJan 2, 2024\n\n\nJHM\n\n\n\n\n\n\n  \n\n\n\n\nMathematics of Vintage Selling\n\n\n\n\n\n\n\nvintage\n\n\nrisk\n\n\n\n\nThis is our introductory pitch on math and vintage selling.\n\n\n\n\n\n\nJan 2, 2024\n\n\nJHM\n\n\n\n\n\n\n  \n\n\n\n\nMathematical Review of Faraday Cages. A Gap In the Literature.\n\n\n\n\n\n\n\nFaraday\n\n\nharmonic functions\n\n\n\n\nPreliminary article. We review the basic properties of Faraday cages and a curious absence of any mathematical proof of their supposed properties from the Maxwell field equations. This leads to some basic questions.\n\n\n\n\n\n\nJan 2, 2024\n\n\nJHM\n\n\n\n\n\n\n  \n\n\n\n\nMeasuring Tapes and Physics\n\n\n\n\n\n\n\nmeasuring tape\n\n\n\n\nWe present our preliminary thoughts on a new digital measuring tape. This is our answer to the question of dude would you hold my tape.\n\n\n\n\n\n\nJan 2, 2024\n\n\nJHM\n\n\n\n\n\n\n  \n\n\n\n\nWeber, Work, Energy\n\n\n\n\n\n\n\nWeber\n\n\nenergy\n\n\nwork\n\n\n\n\n\n\n\n\n\n\n\nJan 2, 2024\n\n\nJHM\n\n\n\n\n\n\n  \n\n\n\n\nDold Thom Theorem. Configuration Spaces and Homology. (3/3)\n\n\n\n\n\n\n\nDold Thom\n\n\nhomology\n\n\nsweepouts\n\n\nOT\n\n\n\n\n\n\n\n\n\n\n\nMar 13, 2023\n\n\nJHM\n\n\n\n\n\n\n  \n\n\n\n\nDold Thom Theorem. Configuration Spaces and Homology. (1/3)\n\n\n\n\n\n\n\nDold Thom\n\n\nhomology\n\n\n\n\nWe begin our discussion of the greatest theorem you’ve never heard of, namely Dold-Thom’s Theorem. We will present a general relative version of Dold-Thom which leads for example, to the canonical isomorphism between the long exact sequence in homotopy with the long exact sequence in relative homology. The ideas behind DT lead to many interesting questions in optimal transport, which this margin is too small to contain.\n\n\n\n\n\n\nMar 10, 2023\n\n\nJHM\n\n\n\n\n\n\n  \n\n\n\n\nDold Thom Theorem. Configuration Spaces and Homology. (2/3)\n\n\n\n\n\n\n\nDold Thom\n\n\nhomology\n\n\n\n\nWe continue our discussion of Dold-Thom’s Theorem (DT). In this post we look to develop the connections between DT and optimal transportation (OT).\n\n\n\n\n\n\nMar 10, 2023\n\n\nJHM\n\n\n\n\n\n\n  \n\n\n\n\nModified Fizeau Sansbury Spinning Wheel Experiment\n\n\n\n\n\n\n\nSR\n\n\nc\n\n\nlight\n\n\nfizeau\n\n\nsansbury\n\n\n\n\nWe propose a modified Fizeau-Sansbury spinning sawtooth experiment to answer the outstanding question: is light even something that travels in vacuum at all?\n\n\n\n\n\n\nMar 9, 2023\n\n\nJHM\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "JHM is a mathematician, writer, teacher, musician, etc.. Dr. JHM investigates Weber electrodynamics, singularities in optimal transport, algorithms, mapping class groups, closing steinberg symbols, etc..."
  },
  {
    "objectID": "about.html#selected-writings",
    "href": "about.html#selected-writings",
    "title": "About",
    "section": "Selected Writings",
    "text": "Selected Writings\n\nFoundations of Special Relativity and Light Propagation in Vacuum: A Critical Review\nTopology of Singularities of Optimal Semicouplings"
  },
  {
    "objectID": "about.html#personal",
    "href": "about.html#personal",
    "title": "About",
    "section": "Personal",
    "text": "Personal\n\nCanadian Citizen\nBorn 1988\nEnglish-French bilingual\nClean Driving Abstract"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nPhD. Mathematics - University of Toronto [Sept. 2013 - Nov. 2019]\n\nThesis: “Applications of Optimal Transport to Algebraic Topology: How to Build Spines from Singularity” Supervised by Prof. R. J. McCann, FRS.\n\nMSc. Mathematics - University of British Columbia [Sept. 2011 - May 2013]\nBSc. Specialization in Mathematics - University of Ottawa [Sept. 2007 – May 2011]"
  },
  {
    "objectID": "about.html#recent-work-experience",
    "href": "about.html#recent-work-experience",
    "title": "About",
    "section": "Recent Work Experience",
    "text": "Recent Work Experience\nLabourer - (Private Contract), Carleton Place, ON [May 2023 – Dec. 2023]\nLabourer - Atlas Apex Commercial Flat Roofing, Ottawa, ON. [March 2023 -May 2023]\nLabourer - GLOW Productions, Calgary, AB. [Nov. 2022 - Feb. 2023]\nFreelance Analyst - (Private Contracts), Ottawa, ON. and Calgary, AB. [Feb. 2022 – Sept. 2022]\nLabourer - Valley Barn Boards, Luskville, QC. [May 2021 - Dec. 2021]\nLabourer - Ottawa Moving and Delivery, Ottawa, ON. [July 2020 – May 2021]"
  },
  {
    "objectID": "about.html#teaching-experience",
    "href": "about.html#teaching-experience",
    "title": "About",
    "section": "Teaching Experience",
    "text": "Teaching Experience\nPrivate Tutor for various students [Jan 2020 - Dec 2022]\nCourse Instructor for MAT135F (Calculus I) at University of Toronto. [Sept 2019 - Dec. 2019]\nCourse Instructor for MAT235Y (Vector Calculus) at University of Toronto. [May 2019–July 2019]\nCourse Instructor for MAT235Y (Vector Calculus) at University of Toronto. [Sept. 2018 - April 2019]\nTeaching Assistant for PUMP II (Course Coordinator: Dr. Lindsey Shorser). [July 2018 - Sept. 2018]\nTeaching Assistant for MAT244 Ordinary Differential Equations (Course Coordinator: Dr. Tomas Kojar). [June 2018 - Aug. 2018]\nCourse Instructor for MAT235 at University of Toronto. (Course coordinator: Dr. Faisal Al-Faisal). [June 2017 - Sept. 2017]"
  },
  {
    "objectID": "posts/2023-03-08-FizeauSansbury/index.html",
    "href": "posts/2023-03-08-FizeauSansbury/index.html",
    "title": "Modified Fizeau Sansbury Spinning Wheel Experiment",
    "section": "",
    "text": "In this post we discuss our modified Fizeau-Sansbury spinning sawtooth experiment. To begin we review Fizeau’s original 1849 experiment and introduce our modification which allows for testing Ralph Sansbury’s apparatus as outlined in (Sansbury, n.d., 2012). While Fizeau studies a saw tooth wheel in uniform motion on the order of twenty rotations per second, Sansbury’s setup can be interpreted as either the same wheel in nonuniform motion, or a wheel with variable saw tooth length in uniform motion. We argue that Sansbury’s predictions could be tested if Fizeau’s wheel could increase it’s angular velocity from 20 revolutions per second, to 80 revolutions or 160 revolutions per second. This is not easily realized.\nAs described in previous posts, our critical essay on foundations in SR is basically stable and readable. Recently this document has been rewritten into four posts, of which this post is the fifth post, which is centred on Sansbury’s experiment.\nSansbury’s experiment is this. We take \\(c\\) the speed of light as equal to \\(1\\) foot per nanosecond \\([\\mu s]=10^{-8}\\) seconds.\n\nSansbury’s Experiment\nWe quote from (Sansbury, n.d.) and his book (Sansbury 2012).\n(Case 1) A \\(15\\) nanosecond light pulse from a laser was sent to a light detector, \\(30\\) feet away. When the light pulse was blocked at the photodiode during the time of emission, but unblocked at the expected time of arrival, \\(31.2\\) nanoseconds after the beginning of the time of emission, for \\(15\\) nanosecond duration, little light was received. (A little more than the \\(4mV\\) noise on the oscilloscope). This process was repeated thousands of times per second.\n(Case 2) When the light was unblocked at the photodiode during the time of emission (\\(15\\) nanoseconds) but blocked after the beginning of the time of emission, during the expected time of arrival for \\(15\\) nanoseconds, twice as much light was received (\\(8mV\\)). This process was repeated thousands of times per second.\nSansbury’s conclusion? That this indicated that light is not a moving wave or photon, but rather the cumulative effect of instantaneous forces at a distance. That is, undetectable oscillations of charge can occur in the atomic nuclei of the photodiode that spill over as detectable oscillations of electrons after a delay.\nSansbury found the equipment necessary for the experiment too expensive to rent for an extended period of time, and he was possibly not a sufficient expert in calibrating the equipment. So Sansbury’s experiment appears to have not been sufficiently investigated, and we would argue that the experiment has neither been reproduced nor properly reviewed. Thus we turn to the classical Fizeau experiment, and consider its similarities to Sansbury’s setup.\n\n\nFizeau’s Saw Tooth Experiment (1849)\nNow Sansbury’s apparatus has some similarity with Fizeau’s sawtooth apparatus, as used circa 1849 to prove some of the first sensible measurements of luminal velocity.\nHere is a blog which inlcudes a transcription of Fizeau’s original 1849 paper in Comptes Rendus.\nHere is an interesting youtube video en francais sur la mesure de Fizeau. Around the 4-5 minute mark is the most interesting. While the speed of the wheel is increased, the received light signal becomes increasingly erratic and intermittent, until a sufficiently high speed of rotation is achieved and the received light signal becomes eventually null and no light is received.\nHere is a useful physics stackexchange answer.\n\n\nSansbury vs. Fizeau\nNow Fizeau’s original setup is a type of Sansbury test where the wheel is in uniform motion. Sansbury’s setup involves either a wheel in nonuniform motion, or equivalently a wheel in uniform motion with a nonuniform sawtooth distribution.\nWe remark that Fizeau was historically looking to estimate the luminal velocity \\(c\\). Sansbury’s experiment assumes \\(c\\) as given, estimated at \\(1\\) foot per nanosecond. Sansbury’s goal is to distinguish light propagation from particle model, and his setup is meant to test whether light is even something that travels at all.\nLet’s review the basic math of Fizeau’s apparatus, it’s very simple. No matter how we setup the mirrors, we suppose light has some total travel time. In Fizeau’s original setup, light travelled a total path of \\(\\approx 16 km\\). With an expected speed of light \\(c=3\\times 10^8 km\\), then the expected travel time is\n\\[\\begin{equation}\n\\frac{16 \\times 10^3 [m]}{3 \\times 10^8 [m]/[sec]}=5.333\\ldots \\times 10^{-5} [sec].\\end{equation}\\]\nNow consider the wheel with angular velocity \\(\\omega\\) having units of \\([degrees]/[sec]\\) and having a toothlength equal to \\(1/720\\) degrees. The time required to turn one toothlength is therefore \\(\\frac{1/720}{\\omega}=\\frac{1}{720 \\omega} [sec]\\). Thus we find that the expected travel time is equal to the time to rotate one toothlength if the following equality holds \\[\\frac{16 \\times 10^3 }{3 \\times 10^8} = \\frac{1}{720 \\omega}, \\] which implies \\[\\omega \\approx 26 ~~~\\frac{[rotations]}{[sec]}.\\]\nSansbury’s (Case 1) could be realized if the wheel was allowed to rotate nonuniformly, i.e. if the wheel could be accelerated in “impulses” something closer to the actual discrete motions of a clock. For example, if the wheel is initially opened at the time of emission, then immediately rotated one saw tooth length (to the closed position) during the expected time of travel, and just prior to the expected time of arrival is rotated another tooth length (to the open position), then Fizeau would predict that the receiver would observe a strong light source. However Sansbury predicts that the receiver would observe rather a very weak signal. Notice here we require the wheel to move twice as fast as Fizeau’s angular velocity. In otherwords the wheel must rotate two complete tooth lengths before the estimated arrival time.\nSansbury’s (Case 2) could be realized if the wheel was rotating nonuniformly. For example, if we keep the wheel fixed during the expected time of flight of the light particle, and turn the wheel one complete toothlength at the expected time of arrival, then Sansbury would predict a relatively strong signal would be received. Fizeau and the particle model would however predict no light would be received, since in the model it would be blocked by the sawtooth at the expected time of arrival.\nN.B. Fizeau’s conception of the sawtooth wheel is classical. But what happens if we retrospectively apply the SR methodology to the experiment, what results are obtained? It appears that SR has a null effect on the entire experiment, i.e. returns the same results as the classical case. While the sawteeth lengths are contracting in SR, this effects the circumference of the wheel but not the angular velocity. Thus Fizeau experiment appears insensitive to any Lorentz SR effects and an experiment which cannot prove SR in contrast to the classical mechanics.\nWe do not require mathematics at this stage, but rather to perform an experiment. However there is an intersting math aspect to the question, “How to keep a nonuniform wheel in uniform motion?”.\nFizeau’s original wheel was materially balanced: the distribution of teeth was equidistant and regular. The centre of mass corresponded with the axis of rotation.\nBut if we begin to study nonuniform wheels, then the behaviour becomes more difficult depending on, say, whether the centre of mass coincides with the axis of rotation.\nSome comparisons between Fizeau and Sansbury:\n\nFizeau’s involves several reflecting mirrors (beam splitters). Therefore there is more interaction involved in Fizeau’s setup than with Sansbury’s. For Fizeau’s is a type of two-way trip of light, where the source and receiver are space-coincident. But Sansbury’s is a one-way trip, requiring some electronics at the receiver namely a photodiode, to measure the amount of electrons released by the light emission.\nIf the phase of Fizeau’s wheel could be controlled, then we could compare the behaviour of the experiment when the wheels differ by one saw tooth length. The trouble in Fizeau is that, because the source and receiver coincide, it’s evident that no light is emitted when the phase is shifted one tooth length. Sansbury’s experiment however does emphatically require the apparatus to be open at the moment of emission.\n\n\n\nFaster Fizeau Wheels\nWe could test some of Sansbury’s ideas if we could increase the angular velocity of the Fizeau wheel by factor of \\(4\\), i.e. we need a wheel of roughly \\(100\\) revolutions per second instead of \\(20\\) revolutions per second.\nGiven such a revolution speed, then we could change the sawtooth pattern of the wheels, having some that are \\(1/4\\) closed, \\(1/2\\) closed, and \\(3/4\\) closed wheels. For example, we could have the alternating sawtooth \\[\\ldots 0101010101 \\ldots\\] or we could have \\[\\ldots 001100110011 \\ldots\\] both of which are \\(1/2\\) closed but having different patterns. And these patterns would have different predictions depending on the photon model or Sansbury’s cumulative action-at-a-distance. Likewise it would be interesting to compare the predictions given a wheel having a \\(1/4\\)-closed sawtooth pattern \\[\\ldots 0001000100010001 \\ldots\\] versus a \\(3/4\\)-closed pattern \\[ \\ldots 0111011101110111\\ldots.\\]\nIf we could get the Fizeau wheel to spin \\(200\\) revolutions per second, then we could test the theories according to \\(8\\)-periodic patterns, i.e. with sawtooth patterns being \\(1/8, 2/8\\), \\(\\ldots\\), \\(7/8\\)ths closed.\nIf we could build a larger wheel with more teeth, say, \\(1440\\) teeth, then \\(2880\\) teeth, then basic gear ratio would increase the speed of the initial pinion wheel by factor of \\(\\times 2\\), \\(\\times 4\\), etc..\nThe heuristics by which we can determine reasonable revolutions per second depends probably on some energy estimates and would require smaller and smaller radii.\nTo be continued. -JHM\n\n\n\n\n\nReferences\n\nSansbury, Ralph. 2012. The Speed of Light: Cumulative Instantaneous Forces at a Distance.\n\n\n———. n.d. “Light Speed Measurements from Roemer and Bradley to the GPS System.” http://www.naturalphilosophy.org/pdf/abstracts/abstracts_5955.pdf."
  },
  {
    "objectID": "posts/2023-03-10-DoldThom_Part2/index.html",
    "href": "posts/2023-03-10-DoldThom_Part2/index.html",
    "title": "Dold Thom Theorem. Configuration Spaces and Homology. (2/3)",
    "section": "",
    "text": "\\(\\newcommand{\\sub}{\\partial^c \\psi(y)}\\) \\(\\newcommand{\\del}{\\partial}\\)\nIn post we introduced the Dold-Thom group \\(A(X)\\) as the kernel of the augmentation map \\(\\epsilon\\), and the relative Dold-Thom theorems. Ultimately we look to construct topological sweepouts via OT.\nFor convenience we recall the basic definition. - (Definition Dold Thom Group \\(A(X)\\)) If \\(X\\) is topological space, and \\(G\\) is finitely-generated abelian group, then the Dold-Thom group is the kernel \\(A(X;G):=\\ker(\\epsilon)\\) of the augmentation map \\(\\epsilon: G(X) \\to G,\\) defined by \\(\\epsilon(\\sum' g_x .x)=\\sum' g_x\\).\nTherefore \\(A(X;G)\\) consists of all finitely-supported \\(G\\)-valued distributions on \\(X\\) with zero net sum. Here \\(\\emptyset\\) represents the constant \\(0\\)-valued distribution on \\(X\\). The vacuum state \\(\\emptyset\\) serves as canonical basepoint on \\(A(X)\\).\n\n(Dold Thom Theorem) The singular reduced homology functor \\(X\\mapsto \\tilde{H}_*(X;G)\\) is naturally equivalent to the functor of \\(\\emptyset\\)-pointed homotopy groups \\(X\\mapsto \\pi_*(A(X;G), \\emptyset)\\).\n\nFor applications we choose \\(G=\\mathbb{Z}\\) or \\(\\mathbb{Z}/2\\), and will suppress ‘\\(G\\)’ for the remainder of this article. As basic corollary we find:\n\n(Corollary) If \\(Y\\) is a Moore space, e.g. \\(Y=\\mathbb{S}^q\\) is a \\(q\\)-sphere, then \\(A(Y;G)\\) is a model of an Eilenberg-Maclane classifying space \\(K(G,q)\\).\n\n\nHow to Represent Homology Cycles: Dold-Thom and Steenrod’s Problem\nThus DT provides an apparently new source of classifying spaces for abelian coefficient groups \\(G\\). According to the obstruction methods of Steenrod, Eilenberg, Maclane, it follows that there is a natural equivalence between free homotopy classes \\[[X, A_0(Y;G)]\\] and singular cohomology groups \\[H^q(X;G)\\]. Thus Dold-Thom allows the construction of cohomology cycles via the construction of topologically nontrivial maps \\(X\\to A_0(Y;G)\\) whenever \\(H^q(X;G)\\neq 0\\). This leads us to a general topological problem:\n\n(Problem 1) Construct and classify homotopically nontrivial continuous maps \\[f: X \\to A_0(Y)\\] for given topological space \\(X\\) and \\(q\\)-sphere \\(Y=\\mathbb{S}^q\\) whenever \\(H^q(X)\\neq 0\\).\nE.g., If \\(X\\) is compact oriented without boundary, then \\(H^n(X)\\) is nontrivial cyclic group when \\(n=\\dim(X)\\). Can we construct explicit maps \\(f: X \\to A_0(S^n)\\) realizing the orientation class in \\(H^n\\)?\nIf \\(X=S_g\\) is a hyperbolic Riemann surface, can we construct maps \\(f: S_g \\to A_0(\\mathbb{S}^1)\\)?\n\nWe remark that maps \\(f\\) solving Problem 1 generate homological cycles on \\(X\\). The argument is described in R. Kirby’s book (Kirby 2006). If \\(f\\) is a continuous map solving Problem 1, then regular fibres \\(f^{-1}(pt)\\), where \\(pt\\) represents a distribution on \\(Y\\), are cycles in \\(X\\). These cycles are nontrivial whenever \\(f\\) is homotopically nontrivial, and even Poincare dual to the cocycle generated by \\(f\\) when \\(X\\) is compact oriented and \\(Y\\) is a Moore space). A similar idea is found in (Thom 1954).\nBut what are the applications and corollaries of DT?\nWe consider DT as providing a constructive method for building homology cycles, e.g. Steenrod’s problem [ref].\nLet \\((X,d,\\sigma)\\) be a source space, say an oriented complete compact Riemannian manifold with \\(\\sigma=vol_X\\). The topologist faces several questions:\n\nHow to generate nontrivial homological \\(k\\) cycles in \\(X\\)?\nHow to generate nontrivial \\(k\\) dimensional sweepouts of \\(X\\)?\n\nOur goal is to generate smooth maps from the regularity theory of optimal transport.\n\\(\\newcommand{\\sub}{\\partial^c \\psi(y)}\\) \\(\\newcommand{\\del}{\\partial}\\)\nIn post we introduced the Dold-Thom group \\(A(X)\\) as the kernel of the augmentation map \\(\\epsilon\\), and the relative Dold-Thom theorems. Ultimately we look to construct topological sweepouts via OT.\nFor convenience we recall the basic definition. - (Definition Dold Thom Group \\(A(X)\\)) If \\(X\\) is topological space, and \\(G\\) is finitely-generated abelian group, then the Dold-Thom group is the kernel \\(A(X;G):=\\ker(\\epsilon)\\) of the augmentation map \\(\\epsilon: G(X) \\to G,\\) defined by \\(\\epsilon(\\sum' g_x .x)=\\sum' g_x\\).\nTherefore \\(A(X;G)\\) consists of all finitely-supported \\(G\\)-valued distributions on \\(X\\) with zero net sum. Here \\(\\emptyset\\) represents the constant \\(0\\)-valued distribution on \\(X\\). The vacuum state \\(\\emptyset\\) serves as canonical basepoint on \\(A(X)\\).\n\n(Dold Thom Theorem) The singular reduced homology functor \\(X\\mapsto \\tilde{H}_*(X;G)\\) is naturally equivalent to the functor of \\(\\emptyset\\)-pointed homotopy groups \\(X\\mapsto \\pi_*(A(X;G), \\emptyset)\\).\n\nFor applications we choose \\(G=\\mathbb{Z}\\) or \\(\\mathbb{Z}/2\\), and will suppress ‘\\(G\\)’ for the remainder of this article. As basic corollary we find:\n\n(Corollary) If \\(Y\\) is a Moore space, e.g. \\(Y=\\mathbb{S}^q\\) is a \\(q\\)-sphere, then \\(A(Y;G)\\) is a model of an Eilenberg-Maclane classifying space \\(K(G,q)\\).\n\n\n\nHow to Represent Homology Cycles: Dold-Thom and Steenrod’s Problem\nThus DT provides an apparently new source of classifying spaces for abelian coefficient groups \\(G\\). According to the obstruction methods of Steenrod, Eilenberg, Maclane, it follows that there is a natural equivalence between free homotopy classes \\[[X, A_0(Y;G)]\\] and singular cohomology groups \\[H^q(X;G)\\]. Thus Dold-Thom allows the construction of cohomology cycles via the construction of topologically nontrivial maps \\(X\\to A_0(Y;G)\\) whenever \\(H^q(X;G)\\neq 0\\). This leads us to a general topological problem:\n\n(Problem 1) Construct and classify homotopically nontrivial continuous maps \\[f: X \\to A_0(Y)\\] for given topological space \\(X\\) and \\(q\\)-sphere \\(Y=\\mathbb{S}^q\\) whenever \\(H^q(X)\\neq 0\\).\nE.g., If \\(X\\) is compact oriented without boundary, then \\(H^n(X)\\) is nontrivial cyclic group when \\(n=\\dim(X)\\). Can we construct explicit maps \\(f: X \\to A_0(S^n)\\) realizing the orientation class in \\(H^n\\)?\nIf \\(X=S_g\\) is a hyperbolic Riemann surface, can we construct maps \\(f: S_g \\to A_0(\\mathbb{S}^1)\\)?\n\nWe remark that maps \\(f\\) solving Problem 1 generate homological cycles on \\(X\\). The argument is described in R. Kirby’s book (Kirby 2006). If \\(f\\) is a continuous map solving Problem 1, then regular fibres \\(f^{-1}(pt)\\), where \\(pt\\) represents a distribution on \\(Y\\), are cycles in \\(X\\). These cycles are nontrivial whenever \\(f\\) is homotopically nontrivial, and even Poincare dual to the cocycle generated by \\(f\\) when \\(X\\) is compact oriented and \\(Y\\) is a Moore space). A similar idea is found in (Thom 1954).\nBut what are the applications and corollaries of DT?\nWe consider DT as providing a constructive method for building homology cycles, e.g. Steenrod’s problem [ref].\nLet \\((X,d,\\sigma)\\) be a source space, say an oriented complete compact Riemannian manifold with \\(\\sigma=vol_X\\). The topologist faces several questions:\n\nHow to generate nontrivial homological \\(k\\) cycles in \\(X\\)?\nHow to generate nontrivial \\(k\\) dimensional sweepouts of \\(X\\)?\n\nOur goal is to generate smooth maps from the regularity theory of optimal transport.\n\n\nDold-Thom and Optimal Transport\nIn (Martel) and (Martel 2022) we introduced a method for applying optimal transportation to algebraic topology, where the main idea was that singularities of optimal transports are well-defined geometric subvarieties whose geometry is described by the differential geometry of the cost function \\(c\\). Formally given a source \\((X, \\sigma)\\), we introduce target spaces \\((Y, \\tau)\\) and costs \\(c: X\\times Y \\to \\bf{R}\\).\nWhen the cost \\(c\\) satisfies certain assumptions (A0123) we find optimal transport solutions and the solutions to Kantorovich’s dual program are unique and have sufficiently regular topology. In the dual program, the key definition is the \\(c\\)-subdifferential \\(\\sub \\hookrightarrow X\\) defined for \\(y\\in Y\\). Using the \\(c\\)-subdifferential we define Kantorovich’s contravariant functor \\[Z: 2^Y \\to 2^X\\] by \\[Z(Y_I)=\\cap_{y\\in Y_I}\\sub\\] for every closed subset \\(Y_I \\hookrightarrow Y\\).\nWe view \\(Z\\) as defining a \\((Y, \\tau)\\)-parameter family of cells \\(\\sub\\) in \\(X\\). The cells need not be disjoint. There are standard assumptions on the cost \\(c\\) such that almost every cell \\(Z(y)=\\sub\\) is an \\((n-k)\\)-dimensional submanifold of \\(X\\), and otherwise singular subvarieties. Our main idea is that every pair of target \\((Y,\\tau)\\) and cost \\(c\\) determines a unique \\(Y\\)-parameter family of subsets \\(Z(y)=\\sub\\) of \\(X\\).\nThese definitions lead to the following problem:\n\n(Problem 1) Find general conditions on \\(Y, \\tau, c\\) such that \\(c\\)-optimal transports \\(\\pi\\) generate continuous maps \\(f=f_\\pi: X \\to A_0(Y)\\), and such that \\(c\\)-optimal transports \\(\\pi\\) general nontrivial sweepouts of \\(X\\).\n(Problem 2) Find general conditions such that \\(y\\mapsto Z(y)\\) is continuous in the Almgren flat-chain topology.\n\nIn otherwords \\(Z(y)=\\sub\\) defines a \\(Y\\)-parameter family of subvarieties of \\(X\\). Regularity of the optimal transport corresponding to \\(Z=\\sub\\) controls the regularity of the sweepout. Problem 2 appears to be satisfied by the basic cost assumptions (A0)–(A4) in the notation used below.\nFor the purposes of generating sweepouts, the geometric homology spheres \\(S^d\\) form a natural class of target spaces. But given a source \\(X\\), can we readily generate costs \\(c: X\\times S^d \\to \\bf{R}\\)? Already the case of constructing interesting costs \\(S \\times S^1 \\to \\bf{R}\\), or better yet \\(S \\times A_0(S^1) \\to \\bf{R}\\), appears nontrivial problem.\n\\(\\newcommand{\\del}{\\partial}\\)\n\n\nRegularity and Sweepouts\nThe regularity of optimal semicouplings is studied in [McCann-Pass] and [McCann-Pass-Chiappori]. We do not enter into the details in this post. We look to construct interesting topological sweepouts of \\(X\\). The idea is that optimal semicouplings from \\(X\\) to \\(k\\)-dimensional target spaces \\((Y, \\tau)\\) with \\(k &lt; &lt; \\dim(X)\\) provide interesting sweepouts if the cost \\(c: X \\times Y \\to \\bf{R}\\) satisfies various conditions, to be labelled (A0) – (A3) below.\nThe key assumptions require the cost \\(c\\) to be \\(C^2\\) on \\(X\\times Y\\), proper, and satisfying a (Twist) condition. The assumptions imply every \\(c\\)-concave function \\(\\psi: Y\\to {\\bf{R}}\\cup\\{+\\infty\\}\\) is Lipschitz and semiconcave, and differentiable \\(\\tau\\)-almost everywhere on \\(Y\\). Therefore \\(dom(\\nabla_x \\phi)\\) is a full measure subset of the activated domain \\(A \\hookrightarrow\\). The active domain \\(A\\) is naturally defined as \\(dom(\\phi)\\) and with a closure equal to \\(dom(D\\phi)\\). Furthermore, \\(D^2\\psi\\) is defined \\(\\tau\\)-almost everywhere on \\(dom(D\\psi)\\).\nIf \\(\\psi\\) is a \\(c\\)-concave function on the target \\(Y\\) which solves the dual Kantorovich program for the \\(c\\)-optimal semicoupling from source to target, then we obtain the Kantorovich functor \\(Z: 2^Y \\to 2^X\\) as defined previously. For our study of sweepouts, it’s convenient to study semicoupling programs where \\(Z\\) is supported on the singletons \\(Y \\hookrightarrow 2^Y\\). This allows us to represent \\(Z\\) as a function \\(Z: Y \\to 2^X\\), with \\(y\\mapsto Z(y)=\\del^c \\psi(y)\\). Thus our study of sweepouts is complementary to our study of singularities. The regularity of the above map \\(y\\mapsto Z(y)\\) is an important question. Under mild assumptions (given below) we easily find \\(Z(y)\\) are closed subsets of the source for every \\(y\\in Y\\). Moreover an application of Sard’s theorem implies \\(Z(y)\\) are smooth submanifolds for almost every \\(y\\in Y\\). For application to sweepouts, we do not expect all \\(Z(y)\\) to be smooth, for indeed there must exist some singular values. However we do seek sweepouts where \\(Z(y)\\) varies continuously with respect to the \\(y\\) parameter. In the language of sweepouts [Guth], we find it necessary to establish the following elementary result.\n\n(Almgren Continuity) If \\(y_0, y_1\\) are sufficiently close in \\(Y\\), then the \\((n-k)\\)-cycles \\(Z(y_0)\\) and \\(Z(y_1)\\) bound a Lipschitz \\((n-k+1)\\)-chain of small area, i.e. there exists a chain \\(C\\) such that \\(\\del C = Z(y_1)-Z(y_0)\\) and \\(C\\) has arbitrarily small area.\n\nWe assume that \\(Z\\) is Almgren continuous when the cost satisfies assumptions (A0123). We reason that the functions are proper Lipschitz and the gradients, and therefore the unit normal areas are bounded from above, implying that the bounding surface is not too large. [Insert details]\n\n\nTechnical Assumptions on the Cost\nTo begin the study of optimal transport from source \\((X,\\sigma)\\) to targets \\((Y,\\tau)\\), we require a choice of cost function \\(c\\) which satisfies some geometric assumptions.\n\n[(A0)] The cost \\(c\\) is twice-continuously differentiable throughout its domain, jointly in the source and target variables \\((x,y)\\), and nonnegative, and proper. Thus \\(c(x,y)\\geq 0\\) and all proper closed sublevels are compact.\n[(A1)] For every \\(y\\in Y\\), we assume \\(x\\mapsto c(x,y)\\) is nonconstant on every open subset of \\(X\\).\n[(A2)] The cost satisfies (Twist) condition with respect to the source variable throughout \\(dom(c)\\): for every \\(x'\\in X\\) the rule \\(y\\mapsto \\nabla_x c(x',y)\\) defines an injective mapping \\(dom(c_{x'}) \\to T_{x'} X\\).\n\nIt is extremely useful to allow the cost \\(c\\) to have poles, for example \\(c(x,y)=+\\infty\\) whenever \\(x\\in Y\\subset X\\).\nThe (Twist) condition (A2) is properly understood through Kantorovich duality, as we will discuss below. It essentially guarantees the uniqueness a.e. of \\(c\\)-optimal transports. It moreover gives important equation describing the fibres of the optimal transport mapping.\nNow suppose we have a source \\(\\sigma\\), target \\(\\tau\\), and cost \\(c\\) satisfying the assumptions (A012). If the \\(c\\)-optimal transport is regular and continuously single-valued, then the functor \\(Z\\) reduces to a map \\(Y\\to 2^X\\) defined by \\(Z(y)=\\sub\\). As we vary \\(y\\) over the target \\((Y, \\tau)\\) we obtain \\(Y\\)-parameter family of closed subsets \\(Z(y)\\) on \\(X\\). Now we need assumptions on the cost to guarantee the cells \\(Z(y)\\) are closed \\((n-k)\\)-dimensional Lipschitz subvarieties of \\(X\\), and even smooth \\((n-k)\\)-submanifolds for a.e. every \\(y\\in Y\\). The Assumptions (A012) do not imply the cells \\(Z(y)\\) have empty boundary. This requires a further hypothesis depending on the \\(c\\)-convex potential, namely:\n\n[(A3)] If \\(\\phi\\) is a \\(c\\)-convex potential on \\(X\\), then for every \\(y\\in Y\\), we require the gradient \\(\\nabla_x (c(x,y)+\\phi(x))\\) be uniformly bounded away from zero for all \\(x\\in \\sub\\).\n\nAgain these are technical conditions which we do not expect the reader to appreciate at first glance. However they arise naturally from the analysis. For example, (A3) is simply a strong form of Clarke’s nonsmooth implicit function theorem.\nSo where do we go from here?\nTo summarize:\n\nWe want to construct sweepouts from optimal transport, and specifically from optimal semicouplings. Given a source \\((X, \\sigma)\\), we require targets \\((Y, \\tau)\\) and costs \\(c\\) on \\(X\\times Y\\) under various conditions. Generally \\(\\dim(Y) &lt;&lt; \\dim(X)\\). Here the assumptions (A0)–(A2) are relevant.\nRegularity of the sweepouts is attained via regularity of the \\(c\\)-subdifferentials \\(Z(y)=\\sub\\) where \\(\\psi=\\psi^{cc}\\) is the \\(c\\)-concave potential solving Kantorovich’s dual max program. Here the assumption (A3) enters, which is dependant on the gradient of \\(\\psi\\).\n\nThere are various problems which one might consider.\n\n\nPreliminary Remarks on Gromov Guth Width Inequality\nWe conclude this post with a question relating to Gromov-Guth’s width inequalities, and a certain line of enquiry which we could not complete. The previous section introduced the possibility of constructing topological sweepouts via solutions of OT programs. Now we study the possibility of representing minimal sweepouts by such solutions motivated by Guth-Gromov’s width inequality (Guth 2007) and (Guth 2009).\n\nIf \\(X=(X,g)\\) is a closed \\(n\\)-dimensional Riemannian manifold, then there exists a universal constant \\(C(n)\\) depending only on the dimension such that \\(width_k(X,g)^{1/k}\\leq C(n) vol(X,g)^{1/n}.\\)\n\nRecall that the \\(k\\)-width of \\((X,g)\\) is defined by a min-max problem, namely \\[width_k(X,g):=\\min_{\\{z_t\\}} \\max_{t} vol_k(z_t),\\] where the minimum ranges over all \\(k\\)-parameter sweepouts \\(z\\) of \\(X\\), and the volume is with respect to the metric \\(g\\). Estimates on \\(width_k\\) imply every \\(k\\)-sweepout of \\(X\\) contains at least one cycle of large volume.\nOur goal is to interpret \\(width_k\\) in terms of optimal transportation. As we described above, the \\(c\\)-optimal transport from source \\(X\\) to target \\(Y\\) yields an \\((n-k)\\)-dimensional sweepout \\(y\\mapsto Z(y)\\). This assuming the cost \\(c\\) satisfies Assumptions (A0123). A priori it is possible, even likely, that not every \\((n-k)\\)-sweepout \\(\\{z_t\\}\\) is representable by a \\(c\\)-optimal transport \\(Z(y)\\). All the same, the optimal transport datum yields much more geometric information on the sweepout. Let \\(\\sigma, \\tau, c\\) be as above, with \\(Z(y):=\\sub\\). The assumptions (A0123) imply the existence of a measurable map \\(T: X\\to Y\\) defined \\(\\sigma\\)-a.e. satisfying \\(T\\# \\sigma = \\tau\\), and such that \\[g(y)=\\int_{T^{-1}(y)} \\frac{1}{|JT|} f(x) d\\mathscr{H}^{n-k}(x) \\] for \\(\\tau\\)-a.e. \\(y\\in Y\\). Here \\(|JT|\\) denotes Jacobian of \\(T\\) defined by \\(|JT|=\\sqrt{DT . {}^t DT}\\). Recall the Cauchy-Binet formula which expresses \\(JF\\) as the euclidean norm of all \\(k\\times k\\)-square subdeterminants of \\(DF\\).\nTo compare \\(g(y)\\) with the volume of the fibres, we write \\[vol_{n-k}[T^{-1}(y)]=vol_{n-k} [\\sub]=\\int_{T^{-1}(y)} 1 . f(x) d\\mathscr{H}^{n-k}(x).\\] If the Jacobian \\(JT\\) has constant magnitude along the fibre \\(T^{-1}(y)\\), then we can immediately compare the density \\(g(y)=\\frac{d\\tau}{d\\mathscr{H}^{k}}(y)\\) with the \\((n-k)\\)-volume of the fibre \\(T^{-1}(y)\\), namely \\(g(y) . |JT| = vol[T^{-1}(y)]\\). But in general, we cannot expect \\(|JT|\\) to be constant along fibres. Furthermore the width of the sweepout depends on the fibre of maximal volume. So the question arises: how can we use OT to identify the maximal fibres and their respective volumes? How can we generate the extremal width volume inequalities using OT?\n\n(Problem:) Under the above hypotheses, determine method such that the volume of the fibres \\(T^{-1}(y)=\\sub\\) can be estimated and bounded by \\(g(y)\\). That is, approximate the \\((n-k)\\)-width of the sweepout \\(y\\mapsto Z(y)\\) in terms of the optimal transport data \\(\\sigma\\), \\(\\tau\\), \\(c\\), \\(\\psi^{cc}=\\psi\\).\n\n\n\nConclusion\nThis post has more questions than answers. But we trying to indicate something of how OT applies to geometric topology. Naturally we continue studying the Kantorovich functor \\(Z\\). Here we seek basic regularity in \\(Z(y)=\\sub\\) that we might obtain regular sweepouts.\n-JHM.\n\n\n\n\n\nReferences\n\nGuth, Larry. 2007. “The Width-Volume Inequality.” Geometric and Functional Analysis 17 (4): 1139–79.\n\n\n———. 2009. “Minimax Problems Related to Cup Powers and Steenrod Squares.” Geometric And Functional Analysis 18 (6): 1917–87.\n\n\nKirby, Robion C. 2006. The Topology of 4-Manifolds. Vol. 1374. Springer.\n\n\nMartel, J. H. “Applications of Optimal Transport to Algebraic Topology: How to Build Spines from Singularity.” PhD thesis, University of Toronto. https://github.com/jhmartel/Thesis2019.\n\n\n———. 2022. “Topology of Singularities of Optimal Semicouplings.” arXiv Preprint arXiv:2201.12817.\n\n\nThom, René. 1954. “Quelques Propriétés Globales Des Variétés Différentiables.” Commentarii Mathematici Helvetici 28 (1): 17–86."
  },
  {
    "objectID": "posts/2024-01-02-Bookstore/Bookstores and Image to ISBN Problem [JHM Labs].html",
    "href": "posts/2024-01-02-Bookstore/Bookstores and Image to ISBN Problem [JHM Labs].html",
    "title": "Bookstores and Image to ISBN Problem",
    "section": "",
    "text": "Random Bookshelf\n\n\n\nBookstore Problem.\n“People don’t buy what they don’t see”. Or equivalently “People only buy what they see”.\nWe walk into Book Bazaar on Bank Street, and we see shelves on shelves filled with books. And great titles. But who is buying? This is the same problem as the vintage clothing shops. The owners are still running the old fashioned brick and mortar model where the owner waits until:\n\nclients walk into the store;\nclients browse the shelves;\nclients maybe identify an item they want;\nclients maybe buy the item.\n\nWe ask the owner of BB: “Do you have a searchable index of these books?” I.e. do you even know what your inventory is? The owner says “No.” In many cases the owner indeed knows alot of titles in his/her head. But the problem is the owner remains the only person with a virtual approximate index in their head, and who can search it?\nSo what’s the problem, and what’s the solution? We need a basic function to take images of bookshelves, and output lists of books (i.e. authors, titles, etc..). This is like image_to_ISBN_list routine.\n\nInput: digital images of book stacked in bookshelves.\nOutput: Searchable index of ISBNs and titles.\n\nWe see the algorithm factored into three steps: 1) individualization, 2) image-to-text, 3) text-to-isbn.\n\nStep 1: (Individualization) Isolate rectangles of the individual books in the image.\n\nConcretely this means partitioning the digital image into rectangles where each rectangle contains the “spine” of the book and the relevant individual text. The text is unstructured and fragmented.\nRemark. If the books are randomly distributed on the book shelf, then with large probability the books are different colours and different shapes and there is alot of contrast between adjacent books. Therefore the individual rectangles/squares/individualization should perform best on random bookshelves.\nBy contrast the BB has a shelf of “penguin classics” and they are paperbacks which look identical in size and colour, and the only difference becomes the small faint text. However we don’t think this setting is relevant to the BB example, since these books are already “worthless”.\n\nStep 2: (Image-to-text) We compose the individualized rectangles into a basic image_to_text function. This provides some unordered text data. I.e., just words or letters, etc, and whatever symbols are available on the spine image.\n\nThere is limited text on the spine therefore we need extract all the words, as much as possible. But the text is unstructured, i.e. not consisting of sentences and sometimes images, i.e. publisher’s logo.\n\nStep 3: (Text to ISBN) Finally we compose the text output from Step 2 with a text_to_isbn function.\n\nAgain the text extracted from Step 2 is partly unstructured since the spine contains limited publisher information. In Step 3 we take a “best guess” of the book title (ISBN) using the text extracted from Step 2.\nClaim: The composition of steps \\(3 \\circ 2 \\circ 1\\) gives an image_to_isbn mapping. This is the basic tool which we offer to the owners of the bookstores.\nBookstore owners login to a gmail account, input images to a python input which we provide, and an google spreadsheet is automatically being updated with the inventory. Therefore owners have searchable index of ISBN objects.\nRemark. There is important sequel to this idea which is the eventual shipping of the books, and their specific locations when an online sale is made. For practical application to the book store, another key problem is updating the index based on daily or weekly in-flows and out-flows of books. Most stores keep a written list of sales outgoing which can be photographed at end of week and then broadcast/merged into the index."
  },
  {
    "objectID": "posts/2024-01-02-FaradayCage/Mathematical Review of Faraday Cages - A Gap In the Literature.html",
    "href": "posts/2024-01-02-FaradayCage/Mathematical Review of Faraday Cages - A Gap In the Literature.html",
    "title": "Mathematical Review of Faraday Cages. A Gap In the Literature.",
    "section": "",
    "text": "In this article we review the basic properties of Faraday cages.\nWe were brought to this subject reviewing Professor A.K.T. Assis’ book with J.A. Hernandes “The Electric Force of a Current”. Assis remarks that Faraday cages have surprising properties which are elementary, but not well known. Moreover the authors of [ “Mathematics of the Faraday cage.” Siam Review 57.3 (2015): 398-417] were surprised to find no readily available references or mathematical explanations of Faraday’s cage. This was the beginning of our investigation.\n\nHollow Conductors\nIn this article \\(C\\) denotes a conductive body in \\(R^3\\). This means \\(C\\) has a boundary surface \\(S = \\partial C\\), and has a complement \\(R^3 - C\\). We say \\(C\\) is hollow if \\(C\\) divides \\(R^3\\) into “inside” and “outside” volumes, i.e. if \\(R^3-C\\) consists of two connected components. In our examples \\(C\\) will be a hollow spherical shell. The width of the shell is thin. By contrast we might otherwise have a solid spherical ball, and in this case the conductor is solid.\nTo say the body \\(C\\) is a conductor means the shell consists of metal or aluminum, something with abundance of free mobile electrons. By contrast an insulator is made of glass, or ceramic porcelain, or rubber, or oil and plastics.\nIn the setting of Faraday cages, we have a hollow conducting body \\(C\\). Topologically this means \\(C\\) divides the ambient space \\({\\bf{R}}^3\\) into two volumes \\({\\bf{R}}^3 - C = V_i \\coprod V_o\\). We consider \\(V_i, V_o\\) as the inner and outer volumes bounded by \\(C\\). The coproduct symbol “\\(\\coprod\\)” emphasizes that the volumes \\(V_i\\), \\(V_o\\) are disjoint. For a hollow conductor \\(C\\), the boundary intersections \\(\\partial V_i\\) and \\(\\partial V_o\\) are disjoint and also partition the topological boundary \\(S=\\partial C\\) of the conductor. For example a hollow sphere has two boundary components, namely the inner and outer surfaces, which are disjoint and bound the inner and outer volumes.\nWith Faraday cages we consider a hollow conductor \\(C\\) and study the presence or absence of electrical forces in the interior volume. The basic question motivating Faraday cages is this:\n\n“If \\(C\\) is a hollow conductor and \\(F_{ext}\\) is an external electric force originating from the exterior volume, then what is the net electric force on the interior volume \\(V_i\\)?”\n\nActually we will see many Faraday cages are bird cages, being not hollow and having no well-defined topological interior or exterior, i.e. the bird cage does not divide \\(R^3\\) into two volumes! In these cases the Faraday shielding effect is informally(!) phrased as:\n\n“If \\(C\\) is a bird cage conductor (not hollow) and \\(F_{ext}\\) is an electric force originating from an external source, then what is the net electric force inside the bird cage?”\n\nWe should remark that bird cages have no interor or exterior, unless you’re a bird who can’t fit through the grates. From the bird’s point of view, the cage divides space into two volumes which are disjoint and inaccesible.\n\nTherefore we do not deny the apparent shielding effects. But we think there is yet no satisfactory derivation. Historically there are two opposing viewpoints. First we have the standard Faraday-Maxwell viewpoint that conductors have a “shielding effect” and the electric field vanishes within the interior of \\(V_i\\) in the above setting. Second we have the nonstandard viewpoint of Weber-Kirchoff that the electric field is nonzero on the interior of \\(V_i\\). Both claims cannot be correct.\n\n\nPoisson’s Theorem and Surface Charge Distributions:\nWe begin with Poisson’s fundamental theorem on electrostatics (1812).\n\n“When arbitrary electric forces act upon a conductor of arbitrary form from the outside, a distribution of free electricity on the surface of the conductor is always possible – but only one of them – for which the electric forces that originate from that distribution of free electricity will likewise be in equilibrium with the electric forces at all points of the interior of the conductor that act from the outside.” [Quoted from Weber, s 13.29].\n\nNotice the topological expressions e.g. “from the outside”, “on the surface”, “of the interior”. In practice the “inside of \\(C\\)” and the “interior volume bounded by \\(C\\)” are frequently confused, especially in the case when the conductor \\(C\\) is a thin volume, e.g. thin hollow spherical shell \\(\"O\"\\).\nHere’s the formal statement of Poisson’s theorem:\n\n“Given a conducting body \\(C\\), and a net external force \\(F_{ext}\\) acting on \\(C\\), then there exists a unique surface charge distribution \\(\\sigma\\) on the boundary surface \\(\\partial C\\) such that \\[F_{ext}(x)+F_\\sigma(x) =0~~\\text{for every}~~x\\in C.\"\\]\n\nThe key point is that the force throughout the volume of the body is balanced by a charge distribution supported only on the boundary surface \\(S=\\partial C\\).\n[Comment about surface charge distributions and normal derivative] The surface charge \\(\\sigma\\) is obtained via the normal derivative of the potential \\(\\phi\\) along the boundary, namely \\(\\frac{\\partial \\phi}{\\partial n}\\), which is by definition equal to \\(\\nabla \\phi \\cdot {\\bf{n}}~ dS\\) where \\(\\bf{n}\\) is the “outer” normal of the surface.\nPoisson claims the net force \\(F_{net}(x)=0\\) vanishes for \\(x\\in C\\) everywhere inside the conductor. However the Faraday-Maxwell shielding effect asserts that \\(F_{net}(x)=0\\) everywhere in the inner volume \\(x\\in V_i\\). We argue with Weber. et al, that indeed \\(F_{net}\\) is nonzero in \\(V_i\\)!\nProof of Poisson’s Theorem: [Incomplete] [Green-Thompson Energy Argument?]\n\n\nMethod of Images \nWhat are we saying, that Faraday cages don’t work? That you shouldn’t put your face next to the microwave? No, well maybe.\n[Method of images: Review!] Here is the basic example: suppose the conductor \\(C\\) is the lower halfspace \\(\\{(x,y)~|~y\\leq 0\\}\\). Suppose an electric point charge \\(+q\\) is placed external to \\(C\\), i.e. somewhere on the vertical half line \\(y&gt;0\\). Then Poisson’s theorem says there exists a surface distibution on \\(C\\) which balances the net force of the charge \\(+q\\). Here we argue by the method of images. The test charge \\(+q\\) induces a charge in the conductor equivalent to the field generated by an image charge \\(-q_i\\). The net electric potential is therefore the sum of the potentials generated by these point sources \\(+q\\) and \\(-q_i\\), and the electric field is the sum of the electric fields generated by the sources \\(+q, -q_i\\). In this example we see the boundary surface \\(\\partial C\\) is an equipotential surface.\nRemark. The argument is usually erroneously made via Gauss’ law. The error is supposing that a vanishing charge density implies a vanishing electric field. This is false! Gauss’ law relates the divergence of the electric field to the charge density \\(\\rho\\). But vanishing divergence \\(div(\\nabla \\phi)\\)=0$ of course does not imply a vanishing gradient \\(\\nabla \\phi\\). [See links below].\n\n\n“But What About Experiments?”\nBut what about all the experiments? A google search gives twenty links on how to make your own Faraday cage in five minutes for five dollars. We argue that there is an apparent shielding effect, yet this shielding effect is unexplained hitherto.\nFor example it’s recorded that Faraday built a room in 1836 coated with metal foil (specific metal unknown) and could not detect any electrical forces inside the room neither near it’s walls. But what is the explanation?\nIt’s interesting that many applications of Faraday cages involve bird cage designs, where the conductor \\(C\\) actually does not topologically divide the space into inner and outer volumes. In applications the following heuristic is introduced: that the cage only shields radiation which is too large to impinge and pass through the “grates”. This does not really follow from the usual arguments and involves new implicit hypotheses.\n\n\n\nScreenshot from “Prelude to Power”\n\n\n\n\n\nibid\n\n\n\n\n\nibid\n\n\n\nKatie Loves Physics on history of Faraday Cage.\n“How to derive Faraday cage properties from Maxwell equations?”.\n“Why is charge distribution on outer surface zero?”\n“How is electric field inside hollow conducting sphere zero?”\n“What is electric field inside hollow conducting sphere?’\n“What is electric field inside Faraday cage?”\n“What is electric field inside hollow conducting body?”\nEMP Misconceptions.\n\nThere’s alot of different answers, but always ending with a nonexplanation, simply the claim that the resultant electric fields cancel to zero. Similarly there is repeated erroneous argument based on Gauss’ law. But again we must emphasize that the vanishing charge density does not imply a vanishing electric field! This is elementary mistake repeated again and again.\n\n\nProf. Lewin’s Experiment\nProf. Lewin’s experiment does not seem convincing. He has a conductive ping pong. Are we expected to believe that if his pingpong touches the positively charged surface, then this positive charge will be imparted to the pingpong rod, and eventually ? This is only if the positive charges are mobile and attracted to the rod.\nHe touches one side of the box and it’s negatively charged. He touches the other side of the box, and he sees that it’s positively charged. (He’s collecting these charges using the condutive ping pong, then discharging at an electrometer to see the polarity, positive or negative charge). But then Prof. Lewin places the pingpong in the volume bounded by the conductor can, and he moves the ping pong around and seems to indicate that he collects no free electric charges. The absence of charges is shown by the electrometer registering no charge. But what does this show? Not that there is zero net force in the interior, but rather that there are no free electrons!\nIs this fair critique? There are no mobile positive ions which are collected by the pingpong and then deposited. Rather there’s only mobile electrons. On the negative side, there is an excess of electrons which are possibly attracted to the pingpong. On the positive side there is an absence of electrons. When the pingpong conductor is placed in physical contact with the box, there is a flow of electrons from the pingpong conductor to the box."
  },
  {
    "objectID": "posts/2024-01-02-WeberWorkEnergyEmuc2/index.html",
    "href": "posts/2024-01-02-WeberWorkEnergyEmuc2/index.html",
    "title": "Weber, Work, Energy",
    "section": "",
    "text": "“Work, Weber’s Critical Radius, Energy”\nEinstein’s \\(E=mc^2\\) is the most famous formula in history, but what does it mean? Here we emphasize that Einstein derived the formula from mathematics in 1905, but Wilhelm Weber derived the formula from his fundamental electodynamic force law. Weber’s derivation leads to the amazing prediction of 2 equal to 1 plus 1 as a stable electronic system.\nConsider the problem of work and moving particles “upstream” of the potential. Recall the Coulomb-type expression that “opposite charges attract” and “like charges repel”. If we arranged some charged particles on a plate, then in a short time these particles would either be repelled outwards to the boundary of the plate or the opposite charges would cancel on the interior. Again this is with respect to the Coulomb model \\(V(r)=1/r\\).\nBut Weber’s model has the following amazing prediction: “opposite charges attract except at a small critical distance \\(r=r_c\\) where the charges acquires a negative inertial mass and the Weber force becomes repulsive”. Furthermore, “like charges repel except at a small critical distance where the Weber force becomes attractive”. This latter prediction leads to Weber’s *planetary model of the atom”.\nIn the (cgs) units the critical radius \\(r_c\\) is computed as \\(r_c=\\frac{1}{m c^2}\\) where \\(m\\) is the relative inertial mass of one of the particles. The proof of this formula is an application of Newton’s second law, that \\(m a = \\bf{F}\\) where \\(\\bf{F}\\) is Weber’s force.\nNow the question arises: if we have two identical but opposite electrically charged particles, say \\((-1)e\\) and \\((+1)e\\), where \\(e\\) is a small mass, then how much work is required to drive \\((-1)e\\) and \\((+1)e\\) to within Weber’s critical radius \\(r_c\\)?\nWeber comments in his final memoir [ref] that an infinite amount of work would be required, however no technical details are provided. In fact this author thinks its evident that a large finite amount of work is required, and the computation of this work required is the subject of this post.\nHow to compute Work: We imagine a particle moving through a potential \\(U\\) along a trajectory \\(\\gamma.\\) Our goal is to determine how much work is required to breach (“pass through”) the critical radius \\(r_c=\\frac{1}{mc^2}\\) in units where \\(4 \\pi\\epsilon_0 = 1\\).\nWe begin assuming the particle’s trajectory is rectilinear. This means the tangent \\(\\gamma'\\) and \\(\\gamma\\) are parallel for all values of the parameter. Moreover we can use the Riemannian idea of parameterizing the curve by arclength. Then we ask how much work is required to move the particle through \\(U\\) at a constant rate of speed, namely \\(||\\gamma'||=1\\).\nWeber’s force is conservative, so there is some path independance, however the terminal conditions are not fixed a priori. It might happen that breaching the Weber radius is easier if the particles velocity is nearly parallel to the “virtual” surface of the Weber’s critical radius. That is to say, the curvature term \\(r r''/c^2\\) in Weber’s force law might sometimes reduce the work needed, depending on the sign.\nIn terms of the relational variables recall the useful formula/definition \\[ r' = \\frac{\\bf{r} \\cdot \\bf{r'}}{r}.\\] In our setting we find \\[r'=\\frac{\\gamma \\cdot \\gamma'}{||\\gamma||}.\\] Applying the Cauchy-Schwartz inequality, we find \\[r'=1.\\] I.e. equality is obtained in Cauchy-Schwartz because \\(\\gamma, \\gamma'\\) are parallel by hypothesis and \\(||\\gamma'||=1\\).\nMoreover the rectilinear motion implies \\(r''=0\\), i.e. the trajectory has zero curvature, since the direction of the particle does not change. Therefore Weber’s force leads to work being computed by the integral \\[W=(1-\\frac{1}{2c^2})\\int_{+\\infty}^{r_c} \\frac{1}{r^2} dr     = (1-\\frac{1}{2c^2})mc^2=mc^2-m/2.\\] So what is the total energy of the above system? The total energy is the work required \\(mc^2 - m/2\\) plus the initial kinetic energy \\(T_i=m|\\gamma'|^2/2=m/2\\). Therefore the total energy required to breach the Weber critical radius is \\(E=W+T_i=mc^2\\).\nN.B. All the while our particles are “travelling upstream”. So the above computation indicates that if a particle is given sufficient energy (i.e. sufficient kinetic energy, then the particle could breach the Weber critical radius, and arriving at the Weber radius with almost zero kinetic energy).\nN.B. The integral representing \\(W\\) is parameterization independant. Therefore the work required by the unit-parameterized path is not overly specialized, but represents the general computation.\nThe above discussion was restricted to rectilinear trajectories. But the possibility remains that curvilinear (“spiralling”) trajectories require less work to breach the Weber radius. Thus while it appears that breaching the Weber radius via rectilinear paths requires large energy \\(\\approx mc^2\\), perhaps the spiralling paths – where the curvature term maintains a definite sign – are the more interesting.\nProblem: Determine the minimum energy required for an isolated two-body system to breach the Weber critical radius.\nAnswer: The minimum energy required to breach the critical radius is \\(E=mc^2\\).\nThis is consequence of the fact that \\(\\bf{F}\\) is a conservative force, therefore dependant only on the initial and terminal states, and not the path taken. Moreover the work done by a particle traversing a path \\(\\gamma\\) depends only on the difference in potential energies. This implies that the above evaluation in the rectilinear case is essentially the same for all paths from some initial point to to within the critical radius.\n\n\nEinstein’s NonPhysical Derivation\nk we apparently all know that \\(E=mc^2\\) is one of the great achievements of Albert Einstein. He first published the idea in 1905 in a paper called “Does the inertia of a body depend upon its energy content?”\nBut what does the formula mean?\nSupposedly it’s one of the greatest formulas in history, yet what is the content of this supposed formula?\nWe give the reader a few minutes to consider what they understand about this formula. For example, what can you say about it?\nThe standard explanations are not so useful. They say that a body of mass possesses some intrinsic energy which is independant of the observer frame. And this intrinsic energy is computed – according to Einstein’s suggestion – as \\(m_0c^2\\), where \\(m=m_0\\) is the inertial rest mass of the object.\nNow here enters another idea, that the inertial mass of an object increases with its velocity, i.e. we have \\(E=mc^2 = \\beta m_0 c^2\\) where \\(\\beta\\) is one of the Lorentz-type beta factor arising so frequently in special relativity, that is \\(\\beta = 1 /\\sqrt{1-v^2/c^2}\\).\nNow look, for most students in university, or even adults, this is the end of the story. You can take physics in undergraduate college or university, and I think this is the standard treatment.\nYou can ask “Why is the formula true?”, and the teacher might say “You won’t need to prove it on the exam, don’t worry about it!”, or “Well, I don’t know, nobody ever asked me, but I guess you can read Einstein’s work”, or “I don’t know, but look at wikipedia.”\nAnd reading Einstein’s original paper is a good idea, although it won’t likely be satisfying. Another good reference is Levi-Civita’s “Absolute Differential Calculus” textbook which has very detailed mathematical review of Einstein’s “physical principles”.\nIn Levi-Civita’s textbook, the formula is derived via an argument using Hamilton’s principle. It’s something like this: we start with the usual Lagrangian \\(L\\), and find the equations of motion are \\(\\delta \\int L = 0\\), where \\(\\delta \\int\\) is the variational derivative of the functional \\(\\alpha \\mapsto \\int_\\alpha L\\). (Here \\(\\alpha\\) denotes a path, not the earlier Lorentz gamma factor.)\nActually in Einstein and Levi-Civita’s approach, which is based on Hamilton principle. The starting point is a Lagrangian \\(L\\) defined on the statespace, and then with Hamiltonian \\(H=L^*\\). The Hamiltonian principle says \\[\\delta \\int L=0.\\] But Einstein introduces the variational equation \\[\\delta \\int c^2 = 0.\\] And he says this equation is trivial like \\[\\delta \\int dt =0 .\\]\nSo Einstein introduces the Lagrangian \\(c^2 - L\\) and the Hamilton principle becomes \\[\\delta \\int (c^2 - L) = 0.\\]\nThis apparently trivial modification of the Lagrangian is what introduces (imports) the constant \\(c^2\\) energy term in the Hamiltonian.\nWe omit the details from Levi-Civita, but the main formula obtained is \\[H^*=c^2 - L = c^2 \\sqrt{ 1-\\beta -2U/c } = c^2 +  v^2/2 - U,\\] since \\(L = v^2/2 + U\\). Einstein remarks that if \\(v=0\\) and \\(U=0\\), i.e. if the potential energy and kinetic energy vanish, then there still remains an intrinsic energy represented by \\(c^2\\), i.e. the Hamiltonian \\(H^*\\) does not vanish.\nIn the above argument, we remark that it’s not always clear that \\(U=0\\) is meaningful or nonarbitrary. As well known, it’s the gradient of the potential \\(\\nabla U\\) which enters into the dynamical equations, and not the scalar values of \\(U\\). So on this point we are not entirely persuaded that \\(v=0, U=0\\) represents an intrinsic energy.\nSo in summary, what’s the story behind this amazing formula \\(E=mc^2\\) ? In the above argument, there is no story except trivial mathematics. There is no physics!\nLevi-Civita makes this same remark in his textbook, but points a posteriori to radioactive substances.\n\n\nWeber’s Physical Interpretation of E=mu.c^2 ?\nWilhelm Weber (1804 – 1891) was a great physicist mathematician who succeeded C.F. Gauss as directory of the Gottingen Observatory. Weber was the first physicist to define \\(c\\), with Kirchoff using Weber’s force equations, as the velocity of electrical impulses in a material wire.\nIn his works on particle electrodynamics, he discovered in 1860s an amazing physical model of \\(E=\\mu c^2\\) where \\(\\mu\\) is the reduced mass of the system of electric particles. We have written on this amazing fact in a previous post “Weber’s Critical Radius, Work, and E=mc2 Formulas”\nWe repeat the fundamental physical idea (and this is what is totally absent in Einstein’s derivation).\nConsider two equal and identical unit electric charges \\(q_1\\) and \\(q_2\\) which are separated by some relative distance \\(r&gt;0\\). The Couloumbian motto is that “like charges repel, and opposite charges attract”. So a force is required to push the charges \\(q_1\\) and \\(q_2\\) closer together. In otherwords, there is work that needs be done to push the identical charges \\(q_1, q_2\\) together.\nWhat Weber discovererd in 1860s (maybe earlier) was that there is a critical distance, where if a sufficient amount of work is performed, and the charges \\(q_1, q_2\\) become sufficiently close $r&lt; r_c $ (passing through Weber’s critical radius \\(r_c\\)) then there is a sign in Weber’s force law which predicts that the equal charged particles become attractive within the critical radius \\(r&lt; r_c\\). In otherwords it’s possible for two-body system of net charge \\(-2\\) \\((=-1 -1)\\) to be a stable system with large potential energy! There is a large amount of potential energy within this system because alot of work was performed to push the particles through the critical radius.\nMore specifically, Weber’s force predicts that the amount of work required to push the particles through the critical radius is \\(E= \\mu c^2\\), where \\(\\mu\\) is the reduced mass of the system.\nThis is amazing fact! This is a physical explanation of what the formula means. The formula represents an amount of work which has been invested into the system. Moreover, Weber’s formula predicts that this potential energy is stored in surprisingly stable many body systems, e.g. two-body systems of the form \\(-2=-1-1\\).\nI don’t know if the reader can appreciate how EPIC this idea of Weber’s is. It’s the beginning of something incredible, basically returning physics to classical paradigm, and appreciating Wilhelm Weber’s tremendous contributions. Here I have been much influenced by AKT Assis’ recent translation into English (and Spanish, Portugese) of Wilhelm Weber’s collected works (in four volumes)."
  },
  {
    "objectID": "posts/2024-01-04-EconomicsMovingAndDelivery/index.html",
    "href": "posts/2024-01-04-EconomicsMovingAndDelivery/index.html",
    "title": "Economics of Moving and Delivery.",
    "section": "",
    "text": "[Originally written April 2022 -JHM]\nIt’s commonly reported that “moving” or “la demenagement” is among the most stressful events for consumers, c.f. “Americans find moving more stressful than divorce”.\nWhat are the essential difficulties involved in moving? Here we assume we are moving a residential home.\n\nMoving is labour intensive, involving hands-on moving of numerous boxes, furniture (tables, couches, bedsets, dressers), and fragile items (TVs, mirrors, lamps).\nMoving items is highly constrained, often involving heavy items being securely extracted from dwellings, and all this with zero damage to walls or floors.\nAnother difficulty, perhaps unappreciated by the clients, is the necessity of packing the objects into the moving truck.\n\nThis is a type of entropy problem, since the compressed volume of the moving truck restricts the possible range of motions of the objects. Moving requires alot of work and foresight to efficiently pack all the items securely in a truck.\nAnother view of the entropy difficulty is this: a house has many rooms, with the objects distributed sparsely throughout the space. However in a moving truck, all the objects need to be compressed into a single room (namely the box of the truck).\n\nCan the clients estimate the volume of all their objects?\nIs it possible for them to imagine all the objects to be relocated into a single room?\n\nThere is considerable stress involved in the action of, say, extracting heavy expensive “precious” furniture through various stairwells, corners, basements, etc.. The business of last-minute kijiji moving is even more stressful, for the clients are typically totally unprepared. For example, they might be selling a freezer located in the basement of a townhouse, with a very tight spiral staircase, and the client has arranged for its delivery to another basement appartment. The client might be moving their entire household, or only moving this single item. Or the client has received a new treadmill, and require its transport into the basement.\nBuilding codes and standard construction methods make the extraction and deliveries somewhat easier. However extreme furniture pieces often push the movers ingenuity to the extreme, and requires alot of experience to immediately know which precise “furniture ballet” is required. As a rule of thumb : if an object makes three points of contact with the wall/floor/ceiling, then the object cannot be pushed any further without causing damage to the surroudings.\nSo how can the consumer save money when moving?\nThe answer is basically preparation.\nTo save money on moving: order a very large truck to make the loading and offloading easier, and pack as much as possible in regular cardboard boxes. This cannot be overremphasized: as much as possible, all the irregular objects should be packed into boxes, and into as many boxes as necessary.\nWhy?\nBecause it’s expensive for the movers to waste their time in arranging and sometimes rearranging irregular shapes objects into the truck.\nThe client should dissessemble the furniture as much as possible beforehand. Otherwise the movers need to spend working hours on the assembly/disassembly of furniture, and/or the preparation of more fragile objects.\nAnother difficulty is the patience and time required to safely move objects in tight spaces. For example, let us consider IKEA items which are very popular. One of the keys to IKEA’s business model is that their furniture is transported and sold totally disassembled, and neatly packing into boxes. I think it’s evident that tables and dressers dissassembled in boxes is more convenient for transport than fully assembled! In IKEA the consumer is required to read the instructions to assemble their items.\nAnother important constraint in domestic moves is: what is the pathway from the pickup to the truck, then from the truck to the dropoff. These are the environmental factors. Is the client moving in/out of a 20 storey building? Are we moving everything through elevators and long hallways? Is the client moving in/out of a basement? How many stairs? How close can the truck get to the unit?\nNow IKEA and movers are not compatible. For the movers might not be able to move IKEA items in their assembled state. Why? Because the IKEA items are fragile, and not designed to be moved. However IKEA items are also not easily disassembled without causing damage (typically cosmetic) to the items. Therefore movers are typically required to transport IKEA items “as is”, and this is a challenge. For assembled IKEA items have no strength, and are not at all designed for “strongman” transport.\nNow for all the complicated parameters that exist in moving, in this article our goal is to reduce everything to the simplest variables. Basically, if a client calls and wants to move their entire household, and wants to have an estimate (or the moving manager wants an estimate for their own schedule), then we ask the following questions:\n\nWhen is the last time the client has moved their household?\nIf appplicable, ask how long it took and how many “human labour hours” were required for their last move?\nHow many “bedrooms” are now being moved?\n(Basic logistics: pickup and dropoff addresses).\nWhat kind of heavy items? (Fridges, couches, exercises equipment, etc.).\n\nWe make some comments: If the client is only moving a select number of items, then ask client “how did the item get here, how many persons were involved, how long did it take”.\nQuestion 1. gives a lower bound (“a floor”) for the moving manager. Our experience is that people only accumulate items, even more items, after they move. When persons are settled in a location, then they collect more and more diverse items. This always adds to the time required and increases the complexity. Question 2. gives some idea, for example was it a team of four movers or two? Was it a big truck, or a smaller cube truck? Were there any incidents during that last move, particular events or damages to the items?\nQuestion 2. is applicable only if the client can remember the last time the item was moved. However it does help manage the clients expectations.\nQuestion 3. is important, especially for single persons who have recently moved themselves. For every single person, there is required approximately 6 to 10 total labour hours required. I.e. two movers require approximately 3 to 5 hours to move a single person (bachelor). This is large interval, which really determines on the particular circumstances. Heavy objects can take 15 – 30 minutes per item to move per team of two.\nQuestions 4, 5 are rather standard. Some estimate of the travel time and circumstances is necessary. For example, if the clients are located in an appartment building, then there is frequently a large walking distance required, and if there is an elevator involved, then the time can be much longer.\n[End –JHM]"
  },
  {
    "objectID": "posts/2024-01-05-AtomHistory/index.html",
    "href": "posts/2024-01-05-AtomHistory/index.html",
    "title": "Sansbury and Bohr. Who is More Energetically Probable?",
    "section": "",
    "text": "The basic question we want to answer is whether the Weber-Sansbury planetary model of the atoms is any more or less energetically probable than the Bohr quantum model. It’s a billion dollar question. Specifically if we consider hydrogen, then the question becomes:\n\n“Is Bohr’s planetary model of the hydrogen atom \\([+1, -1]\\) more probable than Sansbury-Weber’s model of \\([[+2, -1], [+1, -2]]\\)?\n*“Is Sansbury’s prediction of the structured electron* \\(e^{-}=[+1, -2]\\) correct?”\n\nThe question of which states or models are more probable is effectively an energy and statistical question. It’s generally understood from thermodynamics that more probable states are those with lower potential energies (i.e. with less capacity for work).\nRemark on notation: Here \\([+n]\\) or \\([-n]\\) refers to \\(n\\)-particle system of \\(n\\) equal \\([\\pm 1]\\) charges and having sufficient energy to break through the critical radius. Therefore \\([+n]\\) represents an \\(n\\)-body molecule of identical electric particles.\nFor example the system \\([-1, -1]\\) is naturally repulsive in isolated environment, but becomes the molecule \\([-2]\\) only if sufficient work is done by the environment on the system. This is Weber’s amazing prediction that electrons are attractive within the critical radius and an energy \\(E=\\mu c^2\\) is required, where \\(\\mu\\) is the reduced inertial mass of the system.\nIn a system like \\([[+2, -1], [+1, -2]]\\), it’s possible that the system have zero net radiation (effectively isolated and stable) whilst the subsystems \\([+2, -1]\\) and \\([+1, -2]\\) undergoes a continuous exchange of radiation while maintaining a total energy balance. This is Sansbury’s insight."
  },
  {
    "objectID": "posts/2024-01-05-AtomHistory/index.html#is-weber-sansbury-more-energetically-probable-than-bohr",
    "href": "posts/2024-01-05-AtomHistory/index.html#is-weber-sansbury-more-energetically-probable-than-bohr",
    "title": "Sansbury and Bohr. Who is More Energetically Probable?",
    "section": "",
    "text": "The basic question we want to answer is whether the Weber-Sansbury planetary model of the atoms is any more or less energetically probable than the Bohr quantum model. It’s a billion dollar question. Specifically if we consider hydrogen, then the question becomes:\n\n“Is Bohr’s planetary model of the hydrogen atom \\([+1, -1]\\) more probable than Sansbury-Weber’s model of \\([[+2, -1], [+1, -2]]\\)?\n*“Is Sansbury’s prediction of the structured electron* \\(e^{-}=[+1, -2]\\) correct?”\n\nThe question of which states or models are more probable is effectively an energy and statistical question. It’s generally understood from thermodynamics that more probable states are those with lower potential energies (i.e. with less capacity for work).\nRemark on notation: Here \\([+n]\\) or \\([-n]\\) refers to \\(n\\)-particle system of \\(n\\) equal \\([\\pm 1]\\) charges and having sufficient energy to break through the critical radius. Therefore \\([+n]\\) represents an \\(n\\)-body molecule of identical electric particles.\nFor example the system \\([-1, -1]\\) is naturally repulsive in isolated environment, but becomes the molecule \\([-2]\\) only if sufficient work is done by the environment on the system. This is Weber’s amazing prediction that electrons are attractive within the critical radius and an energy \\(E=\\mu c^2\\) is required, where \\(\\mu\\) is the reduced inertial mass of the system.\nIn a system like \\([[+2, -1], [+1, -2]]\\), it’s possible that the system have zero net radiation (effectively isolated and stable) whilst the subsystems \\([+2, -1]\\) and \\([+1, -2]\\) undergoes a continuous exchange of radiation while maintaining a total energy balance. This is Sansbury’s insight."
  },
  {
    "objectID": "posts/2024-01-05-AtomHistory/index.html#bohrs-invention-of-quantum-hypotheses",
    "href": "posts/2024-01-05-AtomHistory/index.html#bohrs-invention-of-quantum-hypotheses",
    "title": "Sansbury and Bohr. Who is More Energetically Probable?",
    "section": "Bohr’s Invention of Quantum Hypotheses",
    "text": "Bohr’s Invention of Quantum Hypotheses\nWe review the history and development of physics, and we see that individuals with large personalities have a dominating influence in physics. Anybody from anywhere can be called into physics. This we learned from Manjit Kumar’s book on Quantum, and the historical biographic accounts it describes from Planck, Bohr, Einstein, etc..\nIt’s important to recall Niels Bohr’s statement [insert ref] that: “those who are not shocked when they first come across quantum theory cannot possibly have understood it.” This is key to the quantum hypothesis which took this author a long time to appreciate. The point is that Bohr introduced new hypotheses, totally contrary to classical physics, in attempt to “solve” the problem of the apparent stability of the hydrogen atom.\nBy 1894 it seems physicists found a puzzling problem: Maxwell’s electromagnetism equations imply that electric charges in bound orbits must be continuously radiating energy into the environment. The puzzle was a consequence of Maxwell’s field theory, which predicted via the Poynting vector that an accelerating particle would have nonzero energy flux. The accelerating particle would be radiating away energy via the field from the Maxwellian perspective. This implied a short lifetime for any simple electrical systems modelled after Maxwell.\nInitially Rutherford’s model of the atom was static, something like J.J. Thompson’s plum pudding. But eventually the dynamic motion was considered, and this lead to the radiation instability as predicted by Larmor’s calculation. Apparently Rutherford knew this was a problem (1906 Radioactive Transformations).\nThere was further mechanical instability implied by Nagaota (1904 Japanese). This was naive “Saturn” model of rings. The mutual repulsion of electrons in the orbits of the planetary systems could not mutually cooperate in the ringed Saturn models.\nSo Bohr writing to Rutherford (1911-1912) assumes the conclusions of Newton and Maxwell, all forecasting the instability of the “planetary atom”. From here Bohr decided that *“the question of stability must therefore be treated from a different point of view* \\(\\ldots\\).” Bohr determines that a “radical change” is needed. Looks to Planck’s quantum proposal. Bohr claims electrons are restricted to “special orbits” where they do not radiate. Bohr calls these states the so-called “stationary states”. No explanation or mechanism is provided to account for the apparent nonradiation of these stationary states.\nBohr reads John Nicholson, encounters the idea of quantized angular momentum \\(L=mvr\\) when particle is moving in a circular orbit. Nicholson postulated [ref] that \\(L=n h\\) where \\(h\\) is Planck’s constant. From here Bohr calculates \\(E_1 / n^2 = E_n\\), deducing that \\(E_1 = -13.6 eV\\) while \\(E_2 = -13.6/4 = -3.40 eV\\). Bohr also calculated that the radius of the hydrogen atom in ground state is \\(5.3\\) nanometers.\nNext Bohr meets Hans Hansen who asks Bohr to explain the spectra of gases, the so-called absorption and emission lines (February 1913). Bohr conceives that absorption and emission are generated by electrons “jumping” between energy levels, and the radiation \\(E_2 - E_1\\) is equal to the emitted radiation. Bohr uses Planck-Einstein formula \\(E = h \\nu\\) where \\(\\nu\\) is the frequency of the emitted radiation.\nBohr introduces the instantaneous and discontinuous “jumps” or “transitions” of electrons in the orbits, and continues to believe that if the electron travels continuously between orbits, then it must accelerate and radiate continuously. [ref]\nIn March 1913 Bohr tries to convince Rutherford. Rutherford argues that an electron moving in a circle is an oscillating system with a frequency (i.e. the number of revolutions per second). Expect oscillating system to radiate energy at the frequency of its oscillation (Maxwell?). But the two energy levels have two frequencies, and what is the relation between these two frequencies? Moreover if there is no continuous motion between energy levels, the Rutherford asks the usual question: How does the electron “know” where to go? Which energy levels will it jump to?\nBohr eventually publishes [“On the Constitution …”] in April 1913 and released in July.\nThe hypotheses of Bohr’s atomic model include the assumptions that:\n\natomic electrons exist in discrete stable orbits;\nthe angular momenta are discretized following Nicholson’s formula;\nelectrons transition discontinuously between different energy levels, and the electron radiate and absorb photons at that energy level according to Planck-Einstein formula \\(E_f - E_i = h \\nu\\).\n\nEventually the Bohr-Sommerfeld “quantum numbers” would need to be introduced [ref]. These numbers arose from the interpretation of Stern-Gerlach’s experiment, and the hypothesis of “quantum spin” was introduced by W. Pauli [ref]."
  },
  {
    "objectID": "posts/2024-01-05-AtomHistory/index.html#ralph-sansbury-and-classical-physics-2.0",
    "href": "posts/2024-01-05-AtomHistory/index.html#ralph-sansbury-and-classical-physics-2.0",
    "title": "Sansbury and Bohr. Who is More Energetically Probable?",
    "section": "Ralph Sansbury and “Classical Physics 2.0”",
    "text": "Ralph Sansbury and “Classical Physics 2.0”\nHere enters Ralph N. Sansbury (1938 – 2014) and the ideas introduced in his book “Faster Than Light”. Our critical review of special relativity here, and especially our proposed Fizeau-Sansbury experiment is directly influenced by our reading Sansbury’s book.\nFrom our perspective, Sansbury’s ideas coupled with Wilhelm E. Weber’s (1804-1891) work allows for a Classical Physics 2.0 to be developed. And this is our goal.\nAmong Sansbury’s ideas is that “light is not something that travels at all, rather the assumed “travel time” of light (the delay between emission and reception) takes place not in the movement of photons, but in the atomic nuclei of the receiver. [ref] This leads to Sansbury’s idea of cumulative instantaneous action at a distance models.\nOur goal is to develop the Weber-Sansbury planetary atomic models. We assume that atomic molecules are configurations in relatively stable orbits of \\((+1)\\) and \\((-1)\\) electrically charged particles. Our main proposition is that Weber-Sansbury force interactions are stable and substitute – as Sansbury originally proposed – for Bohr’s quantum hypotheses. That is to say, the quantum hypotheses are unnecessary and there is a return to Weber-Newtonian continuity of matter and energy.\nSansbury never explicitly applied Weber’s electrical force laws to his models. His book and papers always assumes Coulomb electrical force laws. The classicaly Coulomb law however does not have the key physical properties of Weber. With Weber force law the Nagaota critique is overturned by the appearance of a new phenomenon, namely the electrical Weber derivation of \\(E=mc^2\\), and the natural precession of orbits.\nWith Weber’s force law, the various electrical orbits inside the atom have more opportunity for “cooperation” and “coordination”. Therefore:\n\nWeber’s force law, especially his derivation of \\(E=\\mu c^2\\), predicts the stability of atomic planetary orbits.\n\nWe have written here on Weber’s prediction of the critical radius \\(r_c\\) where repulsive electric particles obtain a negative effective mass.\nSansbury further proposes that: the apparently discrete orbits of Bohr are actually continuous and synchronized with smaller interior orbits. Thus Sansbury argues that atomic orbits cannot be stably sustained “unless they are multiples of the smaller orbits. This leads to no apparent net radiation because of a balance of forces. The discrete radii whose average frequency between transitions are the observed frequencies and spectrum of wavelengths.” [Paraphrased from Sansbury’s book]\nSansbury observes that there is no measurable difference between average frequencies during light emission and absorption, and difference frequencies as Bohr had proposed.\nFor example, atomic spectra are strips of light produced on a receiver source by a single heated gas source of light. The spectra are produced through a double slit. Sansbury’s explanation is that light from one edge takes slightly longer than the other edge to appear at a point on the screen. The difference in these times causes the observed interference pattern.\n[To be continued – JHM]"
  },
  {
    "objectID": "posts/2024-01-10-AmpereMagnetism/index.html",
    "href": "posts/2024-01-10-AmpereMagnetism/index.html",
    "title": "Ampere, Weber, and Magnetism the Disposable Hypothesis",
    "section": "",
    "text": "Magnetic phenomena plays critical role in nearly all fundamental experiments in physics. Briefly we have:\n\nOersted’s 1820 deflection of compass needle by a current.\nFaraday’s introduction of “electromagnetic” lines of force (1851) to explain his experiments on voltaic and magneto induction (1830s).\nFaraday’s electromagnetic lines of force are formalized by \\(E,B\\) in Maxwell’s equations (~1860s).\nHall’s 1879 experiment deciding the sign of charge carriers in conductors.\nZeeman’s 1897 experiment on the splitting of spectral lines in magnetic fields.\nKaufmann’s 1901 experiment on deflection of electric particles in magnetic fields.\nStern-Gerlach 1922 experiments on interaction of “spin” with magnetic moments.\n\nHowever we insist that magnetism remains poorly understood. Even today efforts are made to “discover” magnetic monopoles, and it’s forgotten that Ampere (Assis and Chaib 2015, 19.2) demonstrated the equivalence of magnetic and electric forces, and Ampere [ampere, 19.1] demonstrated the nonexistence of magnetic monopoles via the elementary observation that \\(div(curl)=0\\). Thus Ampere found the idea of magnetic poles and dipoles as superfluous and disposable hypotheses.\nAnother fundamental critique of magnetic interactions due to Ampere (Assis and Chaib 2015, 19.3) is that these interactions are neither fundamental nor primitive. Indeed Oersted, Faraday, Maxwell, et. al., all postulate interactions between current carrying wires and magnetic poles. They assume the interaction between electric charges in motion and magnetic fields is something fundamental. Yet Ampere argues that elementary forces must act between objects of the same nature. For example, in Coulomb’s electrostatic law, one describes mutual interaction between electric charges. In Newton’s gravitational law, one describes the gravitational force between two masses. Thus Ampere rejects the idea of interactions between objects of different natures as something fundamental. To illustrate Ampere in his own words, we quote from Ampere [as cited above]:\n“1st: Every explanation in the sciences consists in discovering a primitive fact expressed by a general law which, once presented, can be utilized in order to deduce from it all other [facts or laws].”\n“2nd: The primitive fact cannot be here the action between a voltaic conductor and a magnet, because, these two things being heterogeneous, the mutual action between them must necessarily be more complicated than that [mutual action] which takes place between two magnets, or than that [mutual action] which I [Ampere] discovered between two conductors”\n“Is it not evident that one should look for the primitive fact in the action between two things of the same nature, like two conductors, and not in that action between two heterogeneous things, like a conductor and a magnet?”\nAs we describe below, this heterogeneity of the Maxwellian electromagnetic fields \\(E,B\\) is the cause of Einstein’s perceived asymmetry in the Maxwell equations."
  },
  {
    "objectID": "posts/2024-01-10-AmpereMagnetism/index.html#magnetic-effects-in-almost-all-fundamental-experiments",
    "href": "posts/2024-01-10-AmpereMagnetism/index.html#magnetic-effects-in-almost-all-fundamental-experiments",
    "title": "Ampere, Weber, and Magnetism the Disposable Hypothesis",
    "section": "",
    "text": "Magnetic phenomena plays critical role in nearly all fundamental experiments in physics. Briefly we have:\n\nOersted’s 1820 deflection of compass needle by a current.\nFaraday’s introduction of “electromagnetic” lines of force (1851) to explain his experiments on voltaic and magneto induction (1830s).\nFaraday’s electromagnetic lines of force are formalized by \\(E,B\\) in Maxwell’s equations (~1860s).\nHall’s 1879 experiment deciding the sign of charge carriers in conductors.\nZeeman’s 1897 experiment on the splitting of spectral lines in magnetic fields.\nKaufmann’s 1901 experiment on deflection of electric particles in magnetic fields.\nStern-Gerlach 1922 experiments on interaction of “spin” with magnetic moments.\n\nHowever we insist that magnetism remains poorly understood. Even today efforts are made to “discover” magnetic monopoles, and it’s forgotten that Ampere (Assis and Chaib 2015, 19.2) demonstrated the equivalence of magnetic and electric forces, and Ampere [ampere, 19.1] demonstrated the nonexistence of magnetic monopoles via the elementary observation that \\(div(curl)=0\\). Thus Ampere found the idea of magnetic poles and dipoles as superfluous and disposable hypotheses.\nAnother fundamental critique of magnetic interactions due to Ampere (Assis and Chaib 2015, 19.3) is that these interactions are neither fundamental nor primitive. Indeed Oersted, Faraday, Maxwell, et. al., all postulate interactions between current carrying wires and magnetic poles. They assume the interaction between electric charges in motion and magnetic fields is something fundamental. Yet Ampere argues that elementary forces must act between objects of the same nature. For example, in Coulomb’s electrostatic law, one describes mutual interaction between electric charges. In Newton’s gravitational law, one describes the gravitational force between two masses. Thus Ampere rejects the idea of interactions between objects of different natures as something fundamental. To illustrate Ampere in his own words, we quote from Ampere [as cited above]:\n“1st: Every explanation in the sciences consists in discovering a primitive fact expressed by a general law which, once presented, can be utilized in order to deduce from it all other [facts or laws].”\n“2nd: The primitive fact cannot be here the action between a voltaic conductor and a magnet, because, these two things being heterogeneous, the mutual action between them must necessarily be more complicated than that [mutual action] which takes place between two magnets, or than that [mutual action] which I [Ampere] discovered between two conductors”\n“Is it not evident that one should look for the primitive fact in the action between two things of the same nature, like two conductors, and not in that action between two heterogeneous things, like a conductor and a magnet?”\nAs we describe below, this heterogeneity of the Maxwellian electromagnetic fields \\(E,B\\) is the cause of Einstein’s perceived asymmetry in the Maxwell equations."
  },
  {
    "objectID": "posts/2024-01-10-AmpereMagnetism/index.html#faradays-law-of-induction",
    "href": "posts/2024-01-10-AmpereMagnetism/index.html#faradays-law-of-induction",
    "title": "Ampere, Weber, and Magnetism the Disposable Hypothesis",
    "section": "Faraday’s Law of Induction",
    "text": "Faraday’s Law of Induction\nBefore we present Einstein’s view on magnetism and special relativity, we briefly review Faraday’s law of induction. From our perspective, we see Faraday’s results on induction as having two parts:\n\nwe have voltaic induction, where we have interactions of time-varying electric fields inducing secondary electric fields in conductors. This is an induction between homogeneous elements, namely conductors interacting with conductors.\nin application one considers Faraday “magneto” induction, which is the basis behind electrical generators, where mechanical energy is converted into electrical energy.\n\nFor example, standard textbooks (Siskind 1959, 1) say: “generator action can take place when, and only when, there is relative motion between conducting wires (usually copper) and magnetic lines of force.” Examples of generators are steam turbines, gasoline engines, electric motors, or hand-powered cranks, etc.. Rotating electrical generators consist of (i) an even set of electromagnets or permanent magnets and (ii) a laminated steel core containing current carrying copper wires (“armature windings”).\n“In the DC generator, the armature winding is mechanically rotated through the stationary magnetic fields created by the electromagnets or permanent magnets.”\n“In the AC generator, the electromagnets or the permanent magnets and their accompanying magnetic fields are rotated with respect to the stationary armature winding.”\nIf we suppose the magnetic field \\(B\\) is properly defined and if \\(C\\) is a conductor, then Faraday’s law of induction says variations in \\(flux(B)\\) along the conductor generates a reciprocal emf in the conductor according to the formula: \\[\\text{emf}:=\\int_C E\\cdot ds=-\\frac{d}{dt} \\int_S B \\cdot dS=-\\int_S \\frac{dB}{dt} \\cdot dS\\] where \\(S\\) is any surface which bounds the conductor \\(\\partial S=C\\). The induction formula says the circulation of \\(E\\) along the conductor boundary \\(C=\\partial S\\) is equal to the negative flux of \\(\\partial{B}/\\partial t\\) along any bounding surface. We remark that Faraday’s law is invariant whether the magnet is in motion and the conductor at rest, or vice versa, or a combination of both."
  },
  {
    "objectID": "posts/2024-01-10-AmpereMagnetism/index.html#einstein-on-maxwell-equations-and-magnetism-as-relativistic-effect",
    "href": "posts/2024-01-10-AmpereMagnetism/index.html#einstein-on-maxwell-equations-and-magnetism-as-relativistic-effect",
    "title": "Ampere, Weber, and Magnetism the Disposable Hypothesis",
    "section": "Einstein on Maxwell Equations and Magnetism as Relativistic Effect",
    "text": "Einstein on Maxwell Equations and Magnetism as Relativistic Effect\nThere is another important influence of magnetism on physics, and this is Einstein’s interpretation of magnetism as a special relativistic effect. An interesting treatment is provided by Prof. J.D. Norton here\nThere does not appear to be any asymmetry in Faraday’s law of induction. However Einstein’s 1905 paper (Einstein 1905) treatment of magnetism as an effect of special relativity begins from the premise that there is an asymmetry. To paraphrase from the first paragraph: “Maxwell’s equations – as usually understood at the present time – lead to asymmetries which do not appear inherent in the phenomena, for example, in the force interactions of a magnet and conductor in relative motion.”\nWhat is the asymmetry? It’s not easy to precisely locate in the text of Einstein’s paper, but it appears to be this:\n\nEinstein’s Asymmetry: The heterogeneous magnetic-electric structure of solutions to Maxwell’s equations is not Lorentz invariant. Solutions \\((E,B)\\) to Maxwell’s equations in an inertial frame \\(K\\) can satisfy \\(E=0\\) or \\(B=0\\), but in another inertial frame \\(K'\\) the solutions \\((E', B')\\) to Maxwell’s equations can satisfy \\(E'\\neq 0\\) or \\(B' \\neq 0\\). Equivalently, a solution might be strictly magnetic or electric in inertial frame \\(K\\) and a combination of electric and magnetic solutions in \\(K'\\).\n\nTherefore Einstein’s asymmetry consists in the inertial frames \\(K, K'\\) having different “physical” descriptions of the same event (the induced emf in the conductor). The apparent asymmetry is a consequence of the heterogeneity inherent to Maxwell’s equations, namely the interactions of fields \\(E,B\\) with distinct physical natures, namely electric versus magnetic. This noninvariance of heterogeneity follows from Einstein’s application of Lorentz transformations to Maxwell’s equations."
  },
  {
    "objectID": "posts/2024-01-10-AmpereMagnetism/index.html#ampere-weber-explanation-of-faraday-magneto-induction",
    "href": "posts/2024-01-10-AmpereMagnetism/index.html#ampere-weber-explanation-of-faraday-magneto-induction",
    "title": "Ampere, Weber, and Magnetism the Disposable Hypothesis",
    "section": "Ampere-Weber Explanation of Faraday Magneto Induction",
    "text": "Ampere-Weber Explanation of Faraday Magneto Induction\nIn Ampere’s electrodynamics, magnetic forces are induced by electrical currents in secondary circuits. Therefore in Ampere’s system there is only relational electrical force between current elements. Magnetism in Ampere’s model is explained at the atom physical level by bound electron orbits around heavy central atomic protons. A permament magnet has a lattice structure such that the angular momenta of the electrical atoms are stuctured and have an additive coherent net effect. This means the moments are not randomly aligned but highly correlated in some parallel direction. Therefore if a permament magnet is at rest macroscopically, it’s constituent electrical molecules have dynamic orbits with parallel angular momenta. If the conductor is in motion relative to the permanent magnet, then we have a resultant relative motion of the conductor with all the angular momenta of the constituent electrical particles in the magnet.\nThus in Ampere’s setting, we find voltaic induction (relative motion of conductors) and the magneto induction (relative motion of magnet and conductor) become equivalent. Microscopically there is only induction principle, but macroscopically it manifests somewhat different in permanent magnets."
  },
  {
    "objectID": "posts/2024-01-10-AmpereMagnetism/index.html#where-to-from-here",
    "href": "posts/2024-01-10-AmpereMagnetism/index.html#where-to-from-here",
    "title": "Ampere, Weber, and Magnetism the Disposable Hypothesis",
    "section": "Where to from here?",
    "text": "Where to from here?\nAmpere wrote (Assis and Chaib 2015, 29.19):\n“Throughout history, whenever hitherto unrelated phenomena have been reduced to a single principle, a period has followed in which many new facts have been discovered, because a new approach in the conception of causes suggests a multitude of new experiments and explanations. It is thus that Volta’s demonstration of the identity of galvanism and electricity was accompanied by the construction of the electric battery with all the discoveries which have sprung from this admirable device….”\nIn this concluding section, we briefly compare the total Hamiltonian of the heterogeneous standard model of electrons in atoms with the homogeneous Ampere-Weber electrodynamic view. The textbook (Nielsen and Chuang 2010, 7.5.2) briefly presents the total Hamiltonian alleged to describe the electrons in atoms as having the form \\[H_{std}=\\sum \\frac{|p_k|^2}{2m} -\\frac{Ze^2}{r_k} +H_{rel}+H_{ee}+H_{so}+H_{hf}, \\] where:\n\nthe first terms describe the energy balance of kinetic energy with the Coulomb attraction to the positively charged nucleus.\n\\(H_{rel}\\) is a “relativistic correction term”.\n\\(H_{ee}\\) is an electron-electron interaction energy arising from so-called “fermionic” nature of electrons.\n\\(H_{so}\\) is the spin orbit interaction, which is supposed to be the interaction of the electron spin interacting with the magnetic field generated by its motion around the atom.\n\\(H_{hf}\\) is the so=called hyperfine interaction which is supposed to be the electron spin interaction with the magnetic field generated by the nucleus.\n\nThe standard Hamiltonian \\(H_{std}\\) is obviously highly heterogeneous.\nBut the essence of simplicity is Ampere-Weber’s hypothesis that all atoms are electrical molecules consisting of positive and negative point electric charges. But Weber does not simply take the Coulomb force law, but Weber proposes his fundamental law of electrodynamic interaction represented in the Hamiltonian \\[H_{w}=\\sum_{ij} \\frac{\\mu_{ij}}{2} {\\nu_{ij}}^2 + \\frac{q_i q_j}{r_{ij}} (1-\\frac{\\nu_{ij}^2}{2c^2}),\\] where \\(\\nu_{ij}:=r'_{ij} = {\\hat{r}_{ij}} \\cdot v_{ij}\\). In otherwords, Weber’s Hamiltonian depends strictly on the scalar relational variables \\(r_{ij}, \\nu_{ij}\\). Weber’s Hamiltonian is the sum of all pairwise interactions in the electrical system, i.e. there is no reference to three-fold or four-fold, etc., interactions.\nOur ultimate opinion is that Weber’s Hamiltonian \\(H_w\\) is the fundamental homogeneous Hamiltonian, while the standard model’s Hamiltonain \\(H_{std}\\) is heterogeneous. We must imagine that Ampere would insist on Weber’s Hamiltonian being developed as far as possible. This is our goal.\n[To be continued –JHM]"
  },
  {
    "objectID": "posts/2024-01-16-GenerativeVintage/index.html",
    "href": "posts/2024-01-16-GenerativeVintage/index.html",
    "title": "Vintage Selling and Generative Matchmaking (Part 1)",
    "section": "",
    "text": "This post is parallel to our earlier post on vintage selling and represents preliminary thoughts on vintage selling and generative matchmaking. This is key buzzword for 2024."
  },
  {
    "objectID": "posts/2024-01-16-GenerativeVintage/index.html#supply-versus-demand-in-vintage-selling",
    "href": "posts/2024-01-16-GenerativeVintage/index.html#supply-versus-demand-in-vintage-selling",
    "title": "Vintage Selling and Generative Matchmaking (Part 1)",
    "section": "Supply versus Demand in Vintage Selling",
    "text": "Supply versus Demand in Vintage Selling\nOur impression is that the vintage clothing market has a significant gap between supply and demand. This gap is especially pronounced in quality used men’s clothing. By “gap” we mean that customers come to the store looking to spend money, they spend time going through the clothing, see nothing they like, and leave without buying anything. Why? Because they don’t see what they want, i.e. the supply does not overlap with the demand. This is the gap we’re talking about.\nIt appears the total space of customers is the sum of two distinct “mind sets” or “psychological states”. These two mindsets approximately correspond to “feminine” and “masculine” buyers, and are summarized in the idiom “women shop. men buy.” There’s many reference on this subject, for example:\nWharton School of Business Podcast\nWashington Post article from 1999\nMen versus Women shopping statistics\nOur specific purpose here is to describe the strategy for the masculine psychological type. We propose the “masculine mind” is mission oriented and judgemental. They dislike the time required to search through random racks for items that may or may not be there, which may or may not fit, at this or that price, etc.. Other types find this aspect of vintage hunting fun, but our premise is that the “masculine” psychology does not enjoy this at all. This makes for significant barrier between these types of buyers and the vintage stores. Our hypothesis is this: the barrier is removed by proactively generating suggestions to the masculine customer.\nLet’s make this concrete. A customer walks into the shop. Before the clients even look through the racks, can the vintage seller generate a list of ten items which positively interest the customer? I.e. items that the customer wants “Wow I have to have that!”.\nThe good news about the obnoxious loud mouth masculine type is this: they are willing to buy a whole setup now at a good price and leave happy. In and out. It really doesn’t have to be prolonged ordeal for anybody. Nobody has to random search through racks, unless they really want the “thrill”."
  },
  {
    "objectID": "posts/2024-01-16-GenerativeVintage/index.html#problem-statement-generating-probabilities-of-sale-with-confidence",
    "href": "posts/2024-01-16-GenerativeVintage/index.html#problem-statement-generating-probabilities-of-sale-with-confidence",
    "title": "Vintage Selling and Generative Matchmaking (Part 1)",
    "section": "Problem Statement: Generating Probabilities of Sale with Confidence",
    "text": "Problem Statement: Generating Probabilities of Sale with Confidence\nWe begin with a description of the process as we see it. The vintage seller has a supply \\(X\\) of items. This supply is initially opaque and unknown to customers \\(Y\\). A customer \\(y\\) walks into the shop, and they have a null feeling to mostly everything. For example, all the tshirts are hanging “side by side” with their graphics hidden, so the items all “look the same” in the customer’s eye. Maybe something catches their eye immediately, or perchance by random searching through the racks the customer \\(y\\) sees something that strikes them.\nIt’s all about the eye and colour, perception and experience in the customer. Visibility is the primary cause of a sale. An extreme instance of this problem is at shop [X] which is very dark inside, and the racks are overstuffed with items, and there’s no room to move.\nFrom another point of view, the vintage seller has no sense of the client’s preferences. The vintage seller likely doesn’t know the buyer even exists before they physically walk into the store. What’s the minimal amount of information that sellers should know about their average customers? We think there’s benefits to vintage sellers having the items ready for customers before the customers walk through the door! This sounds impossible, but there’s a way. It requires predicting the preference probabilities \\(q(x,y)\\) with confidence. We’re using “\\(q\\)” as placeholder for the probabilities which we are trying to predict, i.e. predicting the event of item \\(x\\) selling to \\(y\\). The questions we want to answer are:\n“What is the probability of item \\(x\\) being sold to \\(y\\) at the sales price today?”\n“When a customer walks into the shop, what are the high- and low- probability items relative to the customer \\(y\\)’s preferences? i.e. for which \\(x\\) is \\(q(x,y)\\) more than fifty percent probable?”\nSo we are asking about probabilities \\(q\\) of items being sold to customers. The probabilities represented in \\(q\\) are themselves uncertain. In otherwords, the customer \\(y\\) doesn’t know what they want. This is where the art of curating a vintage collection enters: the seller sets the fashion and effectively tells the customer what to buy."
  },
  {
    "objectID": "posts/2024-01-16-GenerativeVintage/index.html#knowing-your-audience-fitting-the-vetruvian-man-colour-preferences.",
    "href": "posts/2024-01-16-GenerativeVintage/index.html#knowing-your-audience-fitting-the-vetruvian-man-colour-preferences.",
    "title": "Vintage Selling and Generative Matchmaking (Part 1)",
    "section": "Knowing Your Audience, Fitting the Vetruvian Man, Colour Preferences.",
    "text": "Knowing Your Audience, Fitting the Vetruvian Man, Colour Preferences.\nThe problem is knowing or not knowing your audience. For the machine learning engineers, it raises the question “What are the relevant numerical statistics worth knowing about your audience?” We use \\(x,y\\) as symbols intended to represent real items and real customers. But before we can perform any machine learning method, the symbols \\(x,y\\) need to be correlated to floating point decimals. This is awkward process, and we try to keep it as simple as possible.\nUltimately fashion and clothing is about the human form. So the basis of everything is a person in their body:\n\n\n\nVetruvian Man needs clothes! What shirts, pants, shoes, jackets, etc.., does he prefer?\n\n\nWe imagine every customer is basically their own Vetruvian person, like universal mannequin. What does anybody need to know about Vetruvian man to recommend some clothing items? Obviously we don’t need a longform interview. In fact, it’s better to keep it as simple as possible. We think everything reduces to examples. The only thing the seller needs is a summarized precis of colour pattern combinations. Again we really are trying to keep it as simple as possible.\nDefinition. A colour pattern is an \\(RGB\\) valued measure \\(\\mu\\). An outfit is a triple of colour pattern measures \\(\\mu_1, \\mu_2, \\mu_3\\).\nThe idea is that a choice of outfit is formally represented as a choice of colour patterns \\(\\mu_i\\) on pants, undershirt, and overshirt. The definition suggests that a person’s preferences depends mostly on triplets of colour patterns. What defines a person’s style are their preferences relating to combinations of colours. We emphasize that it’s not strictly preferences in the colours themselves, but preferenecs in the combinations of the colour. The contrast and comparison of these distributions together, i.e. whether they clash or correlate, is how we represent the customer’s style preferences.\nPersonally the author thinks it’s amusing to wear homogeneous colour patterns, i.e. red on red, green on green, blue on blue. We like simplicity and think it’s fun when eveything adds up to “red plus red plus red equals red”. But in other settings its better to be balanced, having at most three primary colours, etc..\nThis makes us wonder about birds and their wonderful plumage. Is there experiment to decide whether birds share common preferences in the colour distributions? It’s curious question…"
  },
  {
    "objectID": "posts/2024-01-16-GenerativeVintage/index.html#summary",
    "href": "posts/2024-01-16-GenerativeVintage/index.html#summary",
    "title": "Vintage Selling and Generative Matchmaking (Part 1)",
    "section": "Summary",
    "text": "Summary\nTo summarize the discussion:\n\nThe theme is generative matchmaking. I.e. if the customers can only see ten items to potentially buy, then generate a list of ten items which have better odds than ten random items. I.e. quickly curate a personal list to the customer. This means predicting the probability \\(q(x,y)\\) of the event of item \\(x\\) being purchased by customer \\(y\\) at the sale price. The purpose is to predict the most probable events, where the probability \\(q\\) is maximized.\nWe hypothesize that customers buy items depending on colour patterns \\(\\mu\\) of the fabric. We assume that customers buy outfits depending on their preference for combination colour patterns \\(\\mu_1\\), \\(\\mu_2\\), \\(\\mu_3\\) of pants, undershirt, and overshirt.\nWe assume the high probability colour distributions of customers can be “learned from examples”.\n\nOur goal here is “setting the stage” for the generative model. We identify the colour pattern distributions as the main eye factor in choosing clothes. Personally we know we only like clothes that have good colours and tones. To actually code, train, deploy the generative model we are suggesting is the next step.\n[To be continued – JHM]"
  },
  {
    "objectID": "posts/2024-01-22-GaussWorldMagnetModel/index.html",
    "href": "posts/2024-01-22-GaussWorldMagnetModel/index.html",
    "title": "2025 Year of World Magnetic Model",
    "section": "",
    "text": "We have recently been made aware that 2025 is the year of the world magnetic model (WMM). As we have discussed earlier, magnetism underlies almost all fundamental experiments. Moreover the year 2025 will also introduce the Artemis II, III lunar space missions. Eyes on the skies. So there is opportunity to engage with the WMM problem now. It’s becoming understood that magnetic fields at the poles are dangerously misunderstood.\nThe recent translation (Assis and Chaib 2015) has been crucial in our understanding of Ampere’s masterpiece and his interactions with the leading scientists of his time. Ampere had no formal education, yet became a professor of mathematics at l’Ecole Polytechnique in Lyons and then Paris. We are specifically interested in Ampere’s hypothesis of electrical molecular currents. A revealing comment included in a footnote quoting from William’s 1981 brief biography on Andre-Marie Ampere:\n[begin quote]“There was no doubt that Ampere took his electrodynamic molecule [that is, the hypothesis of currents of electricity around each molecule] seriously and expected others to do so too. In an answer to a letter from the Dutch physicist van Beck, published in the Journal de Physique in 1821, Ampere argued eloquently for his model, insisting that it could be used to explain not only magnetism but also chemical combination and elective affinity [sic]. In short, it was to be considered the foundation of a new theory of matter. This was one of the reasons why Ampere’s theory of electrodynamics was not immediately and universally accepted. To accept it meant to accept as well a theory of the ultimate structure of matter itself.” [end quote] (Assis and Chaib 2015, footnotes pp.24)\nWe have no qualms about Ampere’s electrodynamic molecules and their applications to “ultimate theories of matter”. In fact we are persuaded by Ampere’s argument for homogeneous electrodynamic theories. Magnetism (in Ampere’s conception) is always to be found in environmental electrical currents (i.e. charged matter in motion). Ampere seeks the source of the earth’s magnetism in electrical currents at both the microscopic level in atomic structures, and global macroscopic currents in and through the earth. We cannot be certain whether Ampere ever looked beyond the earth under his feet to the Sun as a source of geomagnetism.\nSo what about the year 2025 being the year of the world magnetic model?\nThe most difficult aspect of the WMM is that earth’s magnetic field is always changing. We understand magnetic forces as dynamic by definition and susceptible to change. But can we really be confident in our maps being accurate one, two, four years later? Obviously cartographers assume the earth’s land masses are not perceptibly moving. Actually WMM is aware of this variability problem. Hence WMM is revised every five years. We quote from the report “State of the Geomagnetic Field: December 2023”: [begin quote] “The main geomagnetic field is constantly changing due to convective flow of and waves in the Earth’s liquid outer core. As the system is essentially chaotic on longer timescales, this change cannot be entirely predicted, and so the accuracy of the WMM slowly decreases over time, necessitating that it be regularly updated (typically every five years).” [end quote]\nHere we notice the reference to fluid analogies, like the convective flow of waves in earth’s supposed “liquid outer core”. What is the convective flow? Wikipedia would suggest that convection arises from some lower heat layer, and then radiates upwards towards the surface. It’s strange really. Also observe the expression “essentially chaotic on longer timescales.” This is another assumption of the fluid model assumptions. Observe also the reference to the necessity(!) of regular updates which are typically every five years. We do not necessarily share in these standard fluid assumptions. Again referring to Ampere, we insist on homogeneous descriptions, as far as possible. So we do not commit to arbitrary fluid hypotheses at this stage.\nIt is typically presumed that an object is magnetic depending on it’s internal atomic structure. But what is the role of the environment in magnetic effects? For example if a rock on the moon has certain magnetic orientation, and this rock is transported to the earth, will the rock again be magnetic?, and how can we relate the magnetic forces between the two points? Is it possible for a rock to be magnetic when it leaves the moon, and arrives to earth non magnetic, or vice versa? Could it be possible that we take a rock from earth, bring it to the moon, and then return to the earth in the same location, and observe that the magnetic orientation of the rock has changed?\nThese are questions, but we must insist that there is an environmental contribution, depending on an object’s interaction with its environment. This is inherent to Ampere’s electrodynamic explanation of magnetism: the interacting current is external, or internal to the object, but it’s part of the environment. By contrast, the magnetic field \\(B\\) as defined by the Maxwellian classical approach is supposed to be a local object: at the point \\(x\\) one wants to obtain \\(B=B(x)\\) directly. But is this possible? These are admittedly fundamental questions, hence we repeat them."
  },
  {
    "objectID": "posts/2024-01-22-GaussWorldMagnetModel/index.html#introduction-to-world-magnet-model-wmm",
    "href": "posts/2024-01-22-GaussWorldMagnetModel/index.html#introduction-to-world-magnet-model-wmm",
    "title": "2025 Year of World Magnetic Model",
    "section": "",
    "text": "We have recently been made aware that 2025 is the year of the world magnetic model (WMM). As we have discussed earlier, magnetism underlies almost all fundamental experiments. Moreover the year 2025 will also introduce the Artemis II, III lunar space missions. Eyes on the skies. So there is opportunity to engage with the WMM problem now. It’s becoming understood that magnetic fields at the poles are dangerously misunderstood.\nThe recent translation (Assis and Chaib 2015) has been crucial in our understanding of Ampere’s masterpiece and his interactions with the leading scientists of his time. Ampere had no formal education, yet became a professor of mathematics at l’Ecole Polytechnique in Lyons and then Paris. We are specifically interested in Ampere’s hypothesis of electrical molecular currents. A revealing comment included in a footnote quoting from William’s 1981 brief biography on Andre-Marie Ampere:\n[begin quote]“There was no doubt that Ampere took his electrodynamic molecule [that is, the hypothesis of currents of electricity around each molecule] seriously and expected others to do so too. In an answer to a letter from the Dutch physicist van Beck, published in the Journal de Physique in 1821, Ampere argued eloquently for his model, insisting that it could be used to explain not only magnetism but also chemical combination and elective affinity [sic]. In short, it was to be considered the foundation of a new theory of matter. This was one of the reasons why Ampere’s theory of electrodynamics was not immediately and universally accepted. To accept it meant to accept as well a theory of the ultimate structure of matter itself.” [end quote] (Assis and Chaib 2015, footnotes pp.24)\nWe have no qualms about Ampere’s electrodynamic molecules and their applications to “ultimate theories of matter”. In fact we are persuaded by Ampere’s argument for homogeneous electrodynamic theories. Magnetism (in Ampere’s conception) is always to be found in environmental electrical currents (i.e. charged matter in motion). Ampere seeks the source of the earth’s magnetism in electrical currents at both the microscopic level in atomic structures, and global macroscopic currents in and through the earth. We cannot be certain whether Ampere ever looked beyond the earth under his feet to the Sun as a source of geomagnetism.\nSo what about the year 2025 being the year of the world magnetic model?\nThe most difficult aspect of the WMM is that earth’s magnetic field is always changing. We understand magnetic forces as dynamic by definition and susceptible to change. But can we really be confident in our maps being accurate one, two, four years later? Obviously cartographers assume the earth’s land masses are not perceptibly moving. Actually WMM is aware of this variability problem. Hence WMM is revised every five years. We quote from the report “State of the Geomagnetic Field: December 2023”: [begin quote] “The main geomagnetic field is constantly changing due to convective flow of and waves in the Earth’s liquid outer core. As the system is essentially chaotic on longer timescales, this change cannot be entirely predicted, and so the accuracy of the WMM slowly decreases over time, necessitating that it be regularly updated (typically every five years).” [end quote]\nHere we notice the reference to fluid analogies, like the convective flow of waves in earth’s supposed “liquid outer core”. What is the convective flow? Wikipedia would suggest that convection arises from some lower heat layer, and then radiates upwards towards the surface. It’s strange really. Also observe the expression “essentially chaotic on longer timescales.” This is another assumption of the fluid model assumptions. Observe also the reference to the necessity(!) of regular updates which are typically every five years. We do not necessarily share in these standard fluid assumptions. Again referring to Ampere, we insist on homogeneous descriptions, as far as possible. So we do not commit to arbitrary fluid hypotheses at this stage.\nIt is typically presumed that an object is magnetic depending on it’s internal atomic structure. But what is the role of the environment in magnetic effects? For example if a rock on the moon has certain magnetic orientation, and this rock is transported to the earth, will the rock again be magnetic?, and how can we relate the magnetic forces between the two points? Is it possible for a rock to be magnetic when it leaves the moon, and arrives to earth non magnetic, or vice versa? Could it be possible that we take a rock from earth, bring it to the moon, and then return to the earth in the same location, and observe that the magnetic orientation of the rock has changed?\nThese are questions, but we must insist that there is an environmental contribution, depending on an object’s interaction with its environment. This is inherent to Ampere’s electrodynamic explanation of magnetism: the interacting current is external, or internal to the object, but it’s part of the environment. By contrast, the magnetic field \\(B\\) as defined by the Maxwellian classical approach is supposed to be a local object: at the point \\(x\\) one wants to obtain \\(B=B(x)\\) directly. But is this possible? These are admittedly fundamental questions, hence we repeat them."
  },
  {
    "objectID": "posts/2024-01-22-GaussWorldMagnetModel/index.html#c.f.-gauss-and-erdmagnetismus",
    "href": "posts/2024-01-22-GaussWorldMagnetModel/index.html#c.f.-gauss-and-erdmagnetismus",
    "title": "2025 Year of World Magnetic Model",
    "section": "C.F. Gauss and Erdmagnetismus",
    "text": "C.F. Gauss and Erdmagnetismus\n[Missing references. Will be corrected in future post. -JHM]\nThe World Magnet Model is still based principally on Carl Friedrich Gauss’ (1777–1855) fundamental research in the earth’s magnetic field and his memoirs on the subject, e.g. Gauss’ memoir “General Theory of Terrestrial Magnetism”. Gauss is credited with “demonstrating” that the dominant geomagnetic field is generated from the earth’s interior. This is treated in the above memoir. Gauss’ argument claims to be independant of hypotheses as to the origin and cause of earth’s magnetism. We are amazed how Gauss openly discusses Ampere’s molecular current hypothesis, although Gauss himself adopts Poisson’s magnetic fluid approach. It’s surprising to discover that Gauss was so perceptive of Ampere’s brilliance, and ready to admit them insofar as consistent with experiment.\nGauss’ argument is deduced from his assuming a Coulomb-Poisson type force law between the supposed magnet \\(NS\\) dipoles. Gauss assumes that the magnetic potential is integrated by a Coulomb type expression \\(V=\\int d\\mu/r\\) where \\(\\mu\\) is assumed to be the distribution of “free magnetism”. This is a naive concept by today’s standards. Thus Gauss defines the magnetic force as the gradient of a harmonic potential.\nHere we remark that Gauss is correct in reasoning that Coloumb’s formula is consistent with AMCH. Thus Gauss seems to be aware of Ampere’s 1824 results which demonstrated the equivalence of Poisson’s 1823 magnetic force law. But here we think there is a deficiency in both Poisson’s and Ampere’s treatment of the dipole force. Regarding Poisson, we think the molecular magnetic fluid cannot reasonably sustain a dipole surface without an external force. Likewise we criticize Ampere’s electrodynamic solenoid (Assis and Chaib 2015, ch.9–10) as insufficient, since there is by necessity an external force necessary to balance the repulsive force of the parallel closed currents. In both cases we claim they do not satisfactorily address the need for external power supply. Our own interpretation is that Ampere was motivated to demonstrate the equivalence of his force law between currents and the molecular current hypothesis with Poisson’s 1823 results. In otherwords, Ampere’s 1824 papers [Ibid] were reactionary, in a sense, and not conclusive.\nRemark. Gauss considers the “magnetic fluid” as consisting of north and south molecules. But it’s known that there are no material carriers of magnetism, and there are no isolated magnetic poles. Therefore the fundamental premise of the Coulomb-Poisson force seems flawed. Moreover Gauss applies an electrostatic principle of Poisson to represent the distribution of “free magnetic fluid” as supported on the boundary surface of the sphere. This leads to Gauss’ spherical harmonic model of the earth’s magnetic field, and his assumption that the magnetic potential \\(B=\\nabla \\phi\\) is the gradient of a harmonic potential (!)on the surface of the earth and exterior according to Coulomb-Poisson’s proposed magnetic force law.\nThus Gauss presumes that the source of geomagnetism, i.e. the source of the north and south poles, lies in the interior of the earth. We do not necessarily agree with the esteemed Gauss’ analysis. Yet this analysis remains the dominant model today, the coefficients of the potential are fit according to Gauss’ method of least squares.\nHowever Gauss himself reasons beyond his own model, reasoning in Article 36 of his memoir: [begin quote] “Another part of our theory on which there may exist a doubt is, the supposition that the agents of the terrestrial magnetic force are situated exclusively in the interior of the earth. If we seek for their immediate causes, partly or wholly, without the earth, and confine ourselves to known scientific grounds, we can only think of galvanic currents. But the atmosphere is no conductor of such currents, neither is vacant space; thus, in seeking in the upper regions for a vehicle of galvanic currents we go beyond our knowledge. ·But our ignorance gives us no right absolutely to deny the possibility of such currents; we are forbidden to do so by the enigmatical phenomena of the Aurora Borealis, in which there is every appearance that electricity in motion performs a principal part. It will therefore still be interesting to examine what form magnetic action arising from such currents would assume on the surface of the earth.” [end quote].\nThus we are led to further consider the electrodynamic cylinders and the dipole surfaces, and our earlier critique that both models require an external force to maintain the assumed separations. There is an idea here that the earth’s magnetic fields are secondary electric effects caused by the primary Birkeland polar circuits. This is the external force necessary to maintain the charge separations in Ampere’s dynamic solenoid, and likewise would be the force providing the energy required for the dipole magnetic separation assumed in Poisson’s explanation.\nFor us, the key is that Ampere’s idea of perfectly parallel electrical currents defining the cylinder are not physically realistic. Parallel currents repel. Therefore any parallel currents, as we understand them, would require an external power source to maintain the separation. Apparently this idea is rather common in plasma physics, where it’s understood that double layers only exist in nontrivial currents.\nSo the conclusion? That we need to look to the aurora and correct the error common to both Ampere and Poisson in the electrodynamic cylinder and the magnetic dipole layers, respectively.\n[To be continued –JHM]"
  }
]